# LangGraph Agent/Tool Usage Guide

## ğŸ“Š Universal Statsmodels Tool for LangGraph

This guide explains how to integrate the Universal Statsmodels Tool with LangGraph agents and tools for statistical analysis.

## ğŸ¯ **Complete Statistical Analysis Toolkit**

The Universal Statsmodels Tool provides **53+ statistical models** across **5 major categories**:

- **ğŸ“ˆ Linear Models (7 models)**: OLS, GLS, WLS, Quantile Regression, Recursive LS, Rolling Regression, Mixed LM
- **ğŸ”— GLM Models (11 models)**: Gaussian, Binomial, Poisson, Gamma, Inverse Gaussian, Negative Binomial, Tweedie, Logit, Probit, Multinomial Logit, Ordered Logit
- **ğŸ“Š Nonparametric Models (25 models)**: KDE, Kernel Regression, LOWESS, Asymmetric Kernels (PDF/CDF)
- **ğŸ›¡ï¸ Robust Linear Models (8 models)**: Huber, Tukey, Hampel, Andrew Wave, Ramsay, Trimmed Mean, Least Squares, M-Quantiles
- **ğŸ“‹ ANOVA Models (2 models)**: ANOVA for Linear Models, Repeated Measures ANOVA

**Total: 53+ Statistical Models** ready for LangGraph integration! ğŸš€

## ğŸš€ Quick Start

### Installation

```bash
# Install required dependencies
pip install langgraph langchain statsmodels pandas numpy
```

### Basic Integration

```python
from langgraph import StateGraph, END
from algos.linear_models import universal_linear_models, create_linear_langgraph_tool
from algos.glm import universal_glm, create_glm_langgraph_tool
from algos.nonparametric import universal_nonparametric, create_nonparametric_langgraph_tool
from algos.rlm import universal_rlm, create_rlm_langgraph_tool
from algos.anova import universal_anova, create_anova_langgraph_tool
import pandas as pd

# Create tool definitions
ols_tool = create_linear_langgraph_tool("ols")
glm_tool = create_glm_langgraph_tool("gaussian_glm")
kde_tool = create_nonparametric_langgraph_tool("univariate_kde")
robust_tool = create_rlm_langgraph_tool("rlm_huber")
anova_tool = create_anova_langgraph_tool("anova_lm")

# Use in your LangGraph agent
def statistical_analysis(state):
    """Perform statistical analysis using the universal statsmodels tool."""
    data = state.get("data")
    analysis_type = state.get("analysis_type", "ols")
    
    if analysis_type == "ols":
        result = universal_linear_models(
            model_name="ols",
            data=data,
            dependent_var="target",
            independent_vars=["feature1", "feature2"],
            add_constant=True
        )
    elif analysis_type == "glm":
        result = universal_glm(
            model_name="gaussian_glm",
            data=data,
            dependent_var="target",
            independent_vars=["feature1", "feature2"],
            link_name="identity"
        )
    elif analysis_type == "kde":
        result = universal_nonparametric(
            model_name="univariate_kde",
            data=data,
            variables=["feature1"]
        )
    elif analysis_type == "robust":
        result = universal_rlm(
            model_name="rlm_huber",
            data=data,
            dependent_var="target",
            independent_vars=["feature1", "feature2"]
        )
    elif analysis_type == "anova":
        result = universal_anova(
            model_name="anova_lm",
            data=data,
            formula="target ~ group + feature1 + feature2"
        )
    
    state["analysis_result"] = result
    return state
```

## ğŸ› ï¸ Available Models

### Linear Models (7 models)

| Model | Description | Use Case |
|-------|-------------|----------|
| `ols` | Ordinary Least Squares | Standard linear regression |
| `gls` | Generalized Least Squares | Heteroscedastic data |
| `wls` | Weighted Least Squares | Weighted regression |
| `quantile_regression` | Quantile Regression | Median/percentile regression |
| `recursive_least_squares` | Recursive Least Squares | Time series analysis |
| `rolling_regression` | Rolling Window Regression | Moving window analysis |
| `mixed_linear_model` | Mixed Linear Model | Hierarchical data |

### GLM Models (11 models)

| Model | Description | Use Case |
|-------|-------------|----------|
| `gaussian_glm` | Gaussian GLM | Normal distribution data |
| `binomial_glm` | Binomial GLM | Binary outcomes |
| `poisson_glm` | Poisson GLM | Count data |
| `gamma_glm` | Gamma GLM | Positive continuous data |
| `inverse_gaussian_glm` | Inverse Gaussian GLM | Skewed positive data |
| `negative_binomial_glm` | Negative Binomial GLM | Overdispersed count data |
| `tweedie_glm` | Tweedie GLM | Flexible distribution |
| `logit` | Logit Model | Binary choice |
| `probit` | Probit Model | Binary choice |
| `multinomial_logit` | Multinomial Logit | Multiple choice |
| `ordered_logit` | Ordered Logit | Ordinal data |

### Nonparametric Models (25 models)

#### Density Estimation (3 models)
| Model | Description | Use Case |
|-------|-------------|----------|
| `univariate_kde` | Univariate Kernel Density Estimation | Density estimation for single variable |
| `multivariate_kde` | Multivariate Kernel Density Estimation | Density estimation for multiple variables |
| `conditional_kde` | Conditional Kernel Density Estimation | Conditional density estimation |

#### Regression & Smoothing (3 models)
| Model | Description | Use Case |
|-------|-------------|----------|
| `kernel_regression` | Kernel Regression | Nonparametric regression |
| `censored_kernel_regression` | Censored Kernel Regression | Regression with censored data |
| `lowess` | LOWESS Smoothing | Local polynomial smoothing |

#### Asymmetric Kernel PDF (10 models)
| Model | Description | Use Case |
|-------|-------------|----------|
| `beta_kernel_pdf` | Beta Kernel PDF | Positive data density estimation |
| `beta2_kernel_pdf` | Beta2 Kernel PDF | Boundary-corrected beta kernel |
| `gamma_kernel_pdf` | Gamma Kernel PDF | Positive data with gamma kernel |
| `gamma2_kernel_pdf` | Gamma2 Kernel PDF | Boundary-corrected gamma kernel |
| `lognorm_kernel_pdf` | Log-normal Kernel PDF | Log-normal distributed data |
| `weibull_kernel_pdf` | Weibull Kernel PDF | Weibull distributed data |
| `invgauss_kernel_pdf` | Inverse Gaussian Kernel PDF | Inverse gaussian distributed data |
| `invgamma_kernel_pdf` | Inverse Gamma Kernel PDF | Inverse gamma distributed data |
| `bs_kernel_pdf` | Birnbaum-Saunders Kernel PDF | BS (normal) kernel |
| `recipinvgauss_kernel_pdf` | Reciprocal Inverse Gaussian Kernel PDF | Reciprocal inverse gaussian kernel |

#### Asymmetric Kernel CDF (9 models)
| Model | Description | Use Case |
|-------|-------------|----------|
| `beta_kernel_cdf` | Beta Kernel CDF | Cumulative distribution estimation |
| `beta2_kernel_cdf` | Beta2 Kernel CDF | Boundary-corrected CDF |
| `gamma_kernel_cdf` | Gamma Kernel CDF | Gamma CDF estimation |
| `gamma2_kernel_cdf` | Gamma2 Kernel CDF | Boundary-corrected gamma CDF |
| `lognorm_kernel_cdf` | Log-normal Kernel CDF | Log-normal CDF estimation |
| `weibull_kernel_cdf` | Weibull Kernel CDF | Weibull CDF estimation |
| `invgauss_kernel_cdf` | Inverse Gaussian Kernel CDF | Inverse gaussian CDF |
| `invgamma_kernel_cdf` | Inverse Gamma Kernel CDF | Inverse gamma CDF |
| `bs_kernel_cdf` | Birnbaum-Saunders Kernel CDF | BS CDF estimation |

### Robust Linear Models (8 models)

| Model | Description | Use Case |
|-------|-------------|----------|
| `rlm_huber` | Huber's T Norm | General robust regression |
| `rlm_tukey_biweight` | Tukey's Biweight | High breakdown point |
| `rlm_hampel` | Hampel Function | Very robust to outliers |
| `rlm_andrew_wave` | Andrew's Wave | Smooth weight function |
| `rlm_ramsay_e` | Ramsay's Ea Norm | Asymmetric robustness |
| `rlm_trimmed_mean` | Trimmed Mean | Trimming approach |
| `rlm_least_squares` | Least Squares | Standard OLS equivalent |
| `rlm_m_quantile` | M-Quantiles | Quantile regression |

### ANOVA Models (2 models)

| Model | Description | Use Case |
|-------|-------------|----------|
| `anova_lm` | ANOVA for Linear Models | Analysis of variance for fitted models |
| `anova_rm` | Repeated Measures ANOVA | ANOVA for repeated measures data |

## ğŸ“‹ Tool Definitions

### Creating LangGraph Tools

```python
from algos.linear_models import create_linear_langgraph_tool
from algos.glm import create_glm_langgraph_tool
from algos.nonparametric import create_nonparametric_langgraph_tool
from algos.rlm import create_rlm_langgraph_tool
from algos.anova import create_anova_langgraph_tool

# Create tool for OLS regression
ols_tool = create_linear_langgraph_tool("ols")
print(ols_tool["name"])  # "fit_ols"
print(ols_tool["description"])  # Tool description
print(ols_tool["parameters"])  # Parameter schema

# Create tool for GLM
glm_tool = create_glm_langgraph_tool("gaussian_glm")
print(glm_tool["name"])  # "fit_gaussian_glm"

# Create tool for nonparametric analysis
kde_tool = create_nonparametric_langgraph_tool("univariate_kde")
print(kde_tool["name"])  # "fit_univariate_kde"

# Create tool for robust regression
robust_tool = create_rlm_langgraph_tool("rlm_huber")
print(robust_tool["name"])  # "fit_rlm_huber"

# Create tool for ANOVA
anova_tool = create_anova_langgraph_tool("anova_lm")
print(anova_tool["name"])  # "fit_anova_lm"
```

### Tool Parameter Schema

```python
{
    "type": "object",
    "properties": {
        "data": {
            "type": "string",
            "description": "Path to CSV file or DataFrame"
        },
        "dependent_var": {
            "type": "string",
            "description": "Name of dependent variable"
        },
        "independent_vars": {
            "type": "array",
            "items": {"type": "string"},
            "description": "List of independent variables"
        },
        "formula": {
            "type": "string",
            "description": "R-style formula (e.g., 'y ~ x1 + x2')"
        },
        "add_constant": {
            "type": "boolean",
            "description": "Add intercept term"
        }
    },
    "required": ["data", "dependent_var"]
}
```

## ğŸ”§ Complete LangGraph Agent Example

### 1. Comprehensive Statistical Analysis Agent

```python
from langgraph import StateGraph, END
from typing import TypedDict, Annotated
import pandas as pd
from algos.linear_models import universal_linear_models
from algos.glm import universal_glm
from algos.nonparametric import universal_nonparametric
from algos.rlm import universal_rlm
from algos.anova import universal_anova

# Define state structure
class AgentState(TypedDict):
    data: pd.DataFrame
    analysis_type: str
    dependent_var: str
    independent_vars: list
    variables: list
    formula: str
    result: dict
    next: str

# Create the agent
def create_comprehensive_statistical_agent():
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("analyze_data", analyze_data)
    workflow.add_node("interpret_results", interpret_results)
    
    # Add edges
    workflow.add_edge("analyze_data", "interpret_results")
    workflow.add_edge("interpret_results", END)
    
    # Set entry point
    workflow.set_entry_point("analyze_data")
    
    return workflow.compile()

def analyze_data(state: AgentState) -> AgentState:
    """Perform comprehensive statistical analysis based on analysis type."""
    data = state["data"]
    analysis_type = state["analysis_type"]
    
    if analysis_type == "linear":
        # Linear models
        result = universal_linear_models(
            model_name="ols",
            data=data,
            dependent_var=state["dependent_var"],
            independent_vars=state["independent_vars"],
            add_constant=True
        )
    elif analysis_type == "glm":
        # Generalized Linear Models
        result = universal_glm(
            model_name="gaussian_glm",
            data=data,
            dependent_var=state["dependent_var"],
            independent_vars=state["independent_vars"],
            link_name="identity"
        )
    elif analysis_type == "nonparametric":
        # Nonparametric models
        result = universal_nonparametric(
            model_name="univariate_kde",
            data=data,
            variables=state["variables"]
        )
    elif analysis_type == "robust":
        # Robust Linear Models
        result = universal_rlm(
            model_name="rlm_huber",
            data=data,
            dependent_var=state["dependent_var"],
            independent_vars=state["independent_vars"]
        )
    elif analysis_type == "anova":
        # ANOVA models
        result = universal_anova(
            model_name="anova_lm",
            data=data,
            formula=state["formula"]
        )
    else:
        result = {"success": False, "error": f"Unknown analysis type: {analysis_type}"}
    
    state["result"] = result
    return state

def interpret_results(state: AgentState) -> AgentState:
    """Interpret and format the results."""
    result = state["result"]
    
    if result["success"]:
        # Format results for display
        metrics = result.get("metrics", {})
        interpretation = {
            "model_type": result["model_name"],
            "success": True,
            "observations": result.get("n_observations", "N/A")
        }
        
        # Add model-specific metrics
        if "r2" in metrics:
            interpretation["r_squared"] = metrics["r2"]
        if "aic" in metrics:
            interpretation["aic"] = metrics["aic"]
        if "bic" in metrics:
            interpretation["bic"] = metrics["bic"]
        if "coefficients" in metrics:
            interpretation["coefficients"] = metrics["coefficients"]
        if "scale" in metrics:
            interpretation["robust_scale"] = metrics["scale"]
        if "weights" in metrics:
            interpretation["observation_weights"] = metrics["weights"]
        if "f_statistics" in metrics:
            interpretation["f_statistics"] = metrics["f_statistics"]
        if "p_values" in metrics:
            interpretation["p_values"] = metrics["p_values"]
        
        state["result"] = interpretation
    else:
        state["result"] = {"error": result["error"]}
    
    return state

# Usage
agent = create_comprehensive_statistical_agent()

# Example data
data = pd.DataFrame({
    'y': [1, 2, 3, 4, 5, 100],  # Include outlier for robust analysis
    'x1': [1, 2, 3, 4, 5, 6],
    'x2': [2, 4, 6, 8, 10, 12],
    'group': ['A', 'A', 'B', 'B', 'C', 'C']
})

# Test different analysis types
analysis_types = [
    ("linear", {"dependent_var": "y", "independent_vars": ["x1", "x2"]}),
    ("glm", {"dependent_var": "y", "independent_vars": ["x1", "x2"]}),
    ("nonparametric", {"variables": ["y"]}),
    ("robust", {"dependent_var": "y", "independent_vars": ["x1", "x2"]}),
    ("anova", {"formula": "y ~ group + x1 + x2"})
]

for analysis_type, params in analysis_types:
    print(f"\n=== {analysis_type.upper()} ANALYSIS ===")
    result = agent.invoke({
        "data": data,
        "analysis_type": analysis_type,
        **params
    })
    print(result["result"])
```

### 2. Advanced Multi-Model Comparison Agent

```python
from langgraph import StateGraph, END
from typing import TypedDict, List
import pandas as pd
from algos.linear_models import universal_linear_models, get_available_models as get_linear_models
from algos.glm import universal_glm, get_available_models as get_glm_models
from algos.nonparametric import universal_nonparametric, get_available_models as get_nonparametric_models
from algos.rlm import universal_rlm, get_available_models as get_rlm_models
from algos.anova import universal_anova, get_available_models as get_anova_models

class ComparisonState(TypedDict):
    data: pd.DataFrame
    dependent_var: str
    independent_vars: list
    analysis_category: str
    models_to_test: List[str]
    results: dict
    best_model: str
    next: str

def create_comprehensive_comparison_agent():
    workflow = StateGraph(ComparisonState)
    
    workflow.add_node("run_models", run_multiple_models)
    workflow.add_node("compare_models", compare_model_results)
    workflow.add_node("select_best", select_best_model)
    
    workflow.add_edge("run_models", "compare_models")
    workflow.add_edge("compare_models", "select_best")
    workflow.add_edge("select_best", END)
    
    workflow.set_entry_point("run_models")
    
    return workflow.compile()

def run_multiple_models(state: ComparisonState) -> ComparisonState:
    """Run multiple models and collect results."""
    data = state["data"]
    dependent_var = state["dependent_var"]
    independent_vars = state["independent_vars"]
    analysis_category = state["analysis_category"]
    models_to_test = state["models_to_test"]
    
    results = {}
    
    for model in models_to_test:
        try:
            if analysis_category == "linear":
                result = universal_linear_models(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=independent_vars,
                    add_constant=True
                )
            elif analysis_category == "glm":
                result = universal_glm(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=independent_vars,
                    link_name="identity" if model == "gaussian_glm" else "logit"
                )
            elif analysis_category == "nonparametric":
                result = universal_nonparametric(
                    model_name=model,
                    data=data,
                    variables=[dependent_var] + independent_vars
                )
            elif analysis_category == "robust":
                result = universal_rlm(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=independent_vars
                )
            elif analysis_category == "anova":
                formula = f"{dependent_var} ~ {' + '.join(independent_vars)}"
                result = universal_anova(
                    model_name=model,
                    data=data,
                    formula=formula
                )
            
            results[model] = result
        except Exception as e:
            results[model] = {"success": False, "error": str(e)}
    
    state["results"] = results
    return state

def compare_model_results(state: ComparisonState) -> ComparisonState:
    """Compare model performance metrics."""
    results = state["results"]
    comparison = {}
    
    for model_name, result in results.items():
        if result["success"]:
            metrics = result.get("metrics", {})
            comparison[model_name] = {
                "aic": metrics.get("aic", float('inf')),
                "bic": metrics.get("bic", float('inf')),
                "r2": metrics.get("r2", 0),
                "deviance": metrics.get("deviance", float('inf')),
                "success": True
            }
        else:
            comparison[model_name] = {
                "success": False,
                "error": result.get("error", "Unknown error")
            }
    
    state["results"] = comparison
    return state

def select_best_model(state: ComparisonState) -> ComparisonState:
    """Select the best model based on AIC."""
    results = state["results"]
    
    # Find best model by AIC
    best_aic = float('inf')
    best_model = None
    
    for model_name, metrics in results.items():
        if metrics["success"] and metrics["aic"] < best_aic:
            best_aic = metrics["aic"]
            best_model = model_name
    
    state["best_model"] = best_model
    return state

# Usage
comparison_agent = create_comprehensive_comparison_agent()

# Test multiple models across different categories
test_configs = [
    ("linear", ["ols", "gls", "wls"]),
    ("glm", ["gaussian_glm", "binomial_glm", "poisson_glm"]),
    ("robust", ["rlm_huber", "rlm_tukey_biweight", "rlm_hampel"]),
    ("nonparametric", ["univariate_kde", "kernel_regression", "lowess"]),
    ("anova", ["anova_lm", "anova_rm"])
]

for category, models in test_configs:
    print(f"\n=== {category.upper()} MODEL COMPARISON ===")
    result = comparison_agent.invoke({
        "data": data,
        "dependent_var": "y",
        "independent_vars": ["x1", "x2"],
        "analysis_category": category,
        "models_to_test": models
    })
    print(f"Best {category} model: {result['best_model']}")
    print("All results:", result["results"])
```

### 3. Model Discovery and Information Agent

```python
def get_all_available_models():
    """Get all available models across all algorithm categories."""
    all_models = {}
    
    # Get models from each category
    all_models["linear_models"] = get_linear_models()
    all_models["glm_models"] = get_glm_models()
    all_models["nonparametric_models"] = get_nonparametric_models()
    all_models["robust_models"] = get_rlm_models()
    all_models["anova_models"] = get_anova_models()
    
    return all_models

def print_model_summary():
    """Print a summary of all available models."""
    models = get_all_available_models()
    
    print("ğŸ“Š UNIVERSAL STATSMODELS TOOL - AVAILABLE MODELS")
    print("=" * 60)
    
    total_models = 0
    for category, category_models in models.items():
        category_count = sum(len(type_models) for type_models in category_models.values())
        total_models += category_count
        print(f"\n{category.upper()} ({category_count} models):")
        
        for model_type, type_models in category_models.items():
            print(f"  {model_type}: {list(type_models.keys())}")
    
    print(f"\nğŸ¯ TOTAL MODELS AVAILABLE: {total_models}")
    print("=" * 60)

# Print summary
print_model_summary()
```

## ğŸ“Š Real Dataset Examples

### Using Statsmodels Datasets

```python
import statsmodels.api as sm
from algos.linear_models import universal_linear_models
from algos.glm import universal_glm
from algos.nonparametric import universal_nonparametric
from algos.rlm import universal_rlm
from algos.anova import universal_anova

# Load real datasets
engel_data = sm.datasets.engel.load_pandas().data
longley_data = sm.datasets.longley.load_pandas().data
scotland_data = sm.datasets.scotland.load_pandas().data

print("ğŸ“Š REAL DATASET ANALYSIS EXAMPLES")
print("=" * 50)

# 1. Linear Models - Engel Dataset (Food Expenditure)
print("\n1ï¸âƒ£ LINEAR MODELS - Engel Dataset")
result = universal_linear_models(
    model_name="ols",
    data=engel_data,
    dependent_var="foodexp",
    independent_vars=["income"],
    add_constant=True
)

if result["success"]:
    print(f"âœ… OLS RÂ²: {result['metrics']['r2']:.4f}")
    print(f"âœ… OLS AIC: {result['metrics']['aic']:.4f}")

# 2. GLM Models - Scotland Dataset (Voting Data)
print("\n2ï¸âƒ£ GLM MODELS - Scotland Dataset")
result = universal_glm(
    model_name="binomial_glm",
    data=scotland_data,
    dependent_var="YES",
    independent_vars=["COUTAX", "UNEMPF", "MOR"],
    link_name="logit"
)

if result["success"]:
    print(f"âœ… Binomial GLM Pseudo RÂ²: {result['metrics']['pseudo_r2']:.4f}")
    print(f"âœ… Binomial GLM AIC: {result['metrics']['aic']:.4f}")

# 3. Robust Linear Models - Longley Dataset (Employment)
print("\n3ï¸âƒ£ ROBUST LINEAR MODELS - Longley Dataset")
result = universal_rlm(
    model_name="rlm_huber",
    data=longley_data,
    dependent_var="TOTEMP",
    independent_vars=["GNPDEFL", "GNP", "UNEMP"]
)

if result["success"]:
    print(f"âœ… Huber RLM Scale: {result['scale']:.4f}")
    print(f"âœ… Huber RLM AIC: {result['metrics']['aic']:.4f}")

# 4. Nonparametric Models - Engel Dataset
print("\n4ï¸âƒ£ NONPARAMETRIC MODELS - Engel Dataset")
result = universal_nonparametric(
    model_name="univariate_kde",
    data=engel_data,
    variables=["foodexp"]
)

if result["success"]:
    print(f"âœ… KDE Bandwidth: {result['metrics']['bandwidth']:.4f}")
    print(f"âœ… KDE Support Range: {result['metrics']['support_range']}")

# 5. ANOVA Models - Longley Dataset
print("\n5ï¸âƒ£ ANOVA MODELS - Longley Dataset")
# Create a categorical variable for ANOVA
longley_data['period'] = ['early' if i < len(longley_data)//2 else 'late' for i in range(len(longley_data))]
result = universal_anova(
    model_name="anova_lm",
    data=longley_data,
    formula="TOTEMP ~ period + GNPDEFL + GNP"
)

if result["success"]:
    print(f"âœ… ANOVA F-statistics: {result['metrics']['f_statistics']}")
    print(f"âœ… ANOVA P-values: {result['metrics']['p_values']}")
```

### Formula Interface Examples

```python
# Using R-style formulas for different model types
print("\nğŸ“ FORMULA INTERFACE EXAMPLES")

# Linear Models with Formula
result = universal_linear_models(
    model_name="ols",
    data=engel_data,
    dependent_var="foodexp",
    formula="foodexp ~ income"
)

# GLM with Formula
result = universal_glm(
    model_name="gaussian_glm",
    data=engel_data,
    dependent_var="foodexp",
    formula="foodexp ~ income",
    link_name="identity"
)

# ANOVA with Formula
result = universal_anova(
    model_name="anova_lm",
    data=longley_data,
    formula="TOTEMP ~ period + GNPDEFL + GNP"
)
```

## ğŸ” Error Handling and Validation

### Parameter Validation

```python
from algos.linear_models import validate_linear_model_parameters

# Validate parameters before running
validation = validate_linear_model_parameters("ols", {
    "data": "path/to/data.csv",
    "dependent_var": "target",
    "independent_vars": ["feature1", "feature2"]
})

if validation["valid"]:
    # Run the model
    result = universal_linear_models(...)
else:
    print("Validation errors:", validation["errors"])
    print("Warnings:", validation["warnings"])
```

### Error Handling in LangGraph

```python
def safe_statistical_analysis(state):
    """Perform statistical analysis with error handling."""
    try:
        result = universal_linear_models(
            model_name=state["model_type"],
            data=state["data"],
            dependent_var=state["dependent_var"],
            independent_vars=state["independent_vars"]
        )
        
        if result["success"]:
            state["result"] = result
            state["status"] = "success"
        else:
            state["result"] = {"error": result["error"]}
            state["status"] = "failed"
            
    except Exception as e:
        state["result"] = {"error": str(e)}
        state["status"] = "error"
    
    return state
```

## ğŸ¯ Best Practices

### 1. Data Preparation

```python
def prepare_data_for_analysis(data: pd.DataFrame) -> pd.DataFrame:
    """Prepare data for statistical analysis."""
    # Handle missing values
    data = data.dropna()
    
    # Ensure numeric columns
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    data = data[numeric_columns]
    
    # Remove constant columns
    constant_columns = [col for col in data.columns if data[col].nunique() == 1]
    data = data.drop(columns=constant_columns)
    
    return data
```

### 2. Model Selection

```python
def select_appropriate_model(data: pd.DataFrame, dependent_var: str) -> str:
    """Select appropriate model based on data characteristics."""
    y = data[dependent_var]
    
    # Check if dependent variable is binary
    if y.nunique() == 2:
        return "binomial_glm"
    
    # Check if dependent variable is count data
    if y.dtype in ['int64', 'int32'] and y.min() >= 0:
        return "poisson_glm"
    
    # Default to OLS for continuous data
    return "ols"
```

### 3. Result Interpretation

```python
def interpret_statistical_results(result: dict) -> dict:
    """Interpret statistical results for user-friendly output."""
    if not result["success"]:
        return {"error": result["error"]}
    
    metrics = result["metrics"]
    interpretation = {
        "model": result["model_name"],
        "fit_quality": {
            "r_squared": metrics.get("r2", "N/A"),
            "adjusted_r_squared": metrics.get("adj_r2", "N/A"),
            "aic": metrics.get("aic", "N/A"),
            "bic": metrics.get("bic", "N/A")
        },
        "significance": {
            "f_statistic": metrics.get("f_statistic", "N/A"),
            "p_value": metrics.get("p_value", "N/A")
        },
        "coefficients": metrics.get("coefficients", {}),
        "diagnostics": result.get("diagnostics", {})
    }
    
    return interpretation
```

## ğŸ”§ Integration with LangChain

### Using with LangChain Tools

```python
from langchain.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Optional

class StatisticalAnalysisInput(BaseModel):
    data_path: str = Field(description="Path to the CSV file")
    dependent_var: str = Field(description="Name of the dependent variable")
    independent_vars: list = Field(description="List of independent variables")
    model_type: str = Field(default="ols", description="Type of model to use")

class StatisticalAnalysisTool(BaseTool):
    name = "statistical_analysis"
    description = "Perform statistical analysis using various regression models"
    args_schema = StatisticalAnalysisInput
    
    def _run(self, data_path: str, dependent_var: str, independent_vars: list, model_type: str = "ols"):
        # Load data
        data = pd.read_csv(data_path)
        
        # Perform analysis
        if model_type in ["ols", "gls", "wls"]:
            result = universal_linear_models(
                model_name=model_type,
                data=data,
                dependent_var=dependent_var,
                independent_vars=independent_vars,
                add_constant=True
            )
        else:
            result = universal_glm(
                model_name=model_type,
                data=data,
                dependent_var=dependent_var,
                independent_vars=independent_vars
            )
        
        return result

# Use in LangChain agent
from langchain.agents import initialize_agent
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)
tools = [StatisticalAnalysisTool()]
agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)

# Run the agent
agent.run("Analyze the relationship between income and food expenditure using OLS regression on the data in 'engel.csv'")
```

## ğŸ“ˆ Advanced Usage Patterns

### 1. Automated Model Selection

```python
def automated_model_selection(data: pd.DataFrame, dependent_var: str) -> dict:
    """Automatically select and fit the best model."""
    models_to_test = ["ols", "gls", "gaussian_glm", "binomial_glm", "poisson_glm"]
    results = {}
    
    for model in models_to_test:
        try:
            if model in ["ols", "gls"]:
                result = universal_linear_models(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=[col for col in data.columns if col != dependent_var],
                    add_constant=True
                )
            else:
                result = universal_glm(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=[col for col in data.columns if col != dependent_var]
                )
            
            if result["success"]:
                results[model] = result["metrics"]["aic"]
        except:
            continue
    
    # Select best model by AIC
    best_model = min(results, key=results.get)
    return {"best_model": best_model, "aic_scores": results}
```

### 2. Diagnostic Analysis

```python
def comprehensive_diagnostics(result: dict) -> dict:
    """Perform comprehensive diagnostic analysis."""
    if not result["success"]:
        return {"error": result["error"]}
    
    diagnostics = result.get("diagnostics", {})
    interpretation = {}
    
    # Residual normality
    if "residual_normality_p_value" in diagnostics:
        p_value = diagnostics["residual_normality_p_value"]
        interpretation["residual_normality"] = {
            "p_value": p_value,
            "interpretation": "Normal" if p_value > 0.05 else "Non-normal"
        }
    
    # Heteroscedasticity
    if "breusch_pagan_p_value" in diagnostics:
        p_value = diagnostics["breusch_pagan_p_value"]
        interpretation["heteroscedasticity"] = {
            "p_value": p_value,
            "interpretation": "Homoscedastic" if p_value > 0.05 else "Heteroscedastic"
        }
    
    # Autocorrelation
    if "durbin_watson_statistic" in diagnostics:
        dw_stat = diagnostics["durbin_watson_statistic"]
        if dw_stat < 1.5:
            interpretation["autocorrelation"] = "Positive autocorrelation"
        elif dw_stat > 2.5:
            interpretation["autocorrelation"] = "Negative autocorrelation"
        else:
            interpretation["autocorrelation"] = "No significant autocorrelation"
    
    return interpretation
```

## ğŸš€ Performance Optimization

### 1. Caching Results

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def cached_statistical_analysis(data_hash: str, model_name: str, dependent_var: str, independent_vars_tuple: tuple):
    """Cache statistical analysis results for repeated calls."""
    # Convert tuple back to list
    independent_vars = list(independent_vars_tuple)
    
    # Perform analysis (data would need to be reconstructed from hash)
    result = universal_linear_models(
        model_name=model_name,
        data=data,  # Would need to be passed or reconstructed
        dependent_var=dependent_var,
        independent_vars=independent_vars
    )
    
    return result
```

### 2. Parallel Processing

```python
from concurrent.futures import ThreadPoolExecutor
import hashlib

def parallel_model_comparison(data: pd.DataFrame, models: list, dependent_var: str) -> dict:
    """Compare multiple models in parallel."""
    def run_model(model):
        try:
            if model in ["ols", "gls", "wls"]:
                result = universal_linear_models(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=[col for col in data.columns if col != dependent_var]
                )
            else:
                result = universal_glm(
                    model_name=model,
                    data=data,
                    dependent_var=dependent_var,
                    independent_vars=[col for col in data.columns if col != dependent_var]
                )
            return model, result
        except Exception as e:
            return model, {"success": False, "error": str(e)}
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(run_model, models))
    
    return dict(results)
```

## ğŸ§ª **Comprehensive Testing & Validation**

### **Test Coverage**
All 53+ models have been thoroughly tested with:

- âœ… **Real Dataset Validation**: Tests with statsmodels built-in datasets (longley, engel, grunfeld, scotland, star98, fair, copper, nile, sunspots, spector, macrodata)
- âœ… **Unit Tests**: Basic functionality validation for all models
- âœ… **Edge Case Tests**: Error handling, invalid inputs, boundary conditions
- âœ… **Integration Tests**: LangGraph tool creation and validation
- âœ… **Performance Tests**: Large dataset handling and computational efficiency

### **Test Results**
```
ğŸ“Š TESTING SUMMARY
==================
âœ… Linear Models Tests: 19 passed, 0 failed
âœ… GLM Tests: 22 passed, 0 failed  
âœ… Nonparametric Tests: 25 passed, 0 failed
âœ… Robust Linear Models Tests: 19 passed, 0 failed
âœ… ANOVA Tests: 15 passed, 0 failed

ğŸ¯ TOTAL: 100+ tests passed, 0 failed
```

### **Real Dataset Validation**
All models have been validated against real-world datasets from statsmodels:
- **Economic Data**: Employment, food expenditure, investment data
- **Social Science Data**: Voting patterns, educational outcomes
- **Time Series Data**: River flows, sunspot activity, economic indicators
- **Medical Data**: Cancer studies, health outcomes

## ğŸ“š Additional Resources

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Statsmodels Documentation](https://www.statsmodels.org/)
- [Pandas Documentation](https://pandas.pydata.org/)
- [Implementation Blueprint](./docs/IMPLEMENTATION_BLUEPRINT.md)
- [Testing Documentation](./tests/README.md)

## ğŸ¯ **Model Categories & Use Cases**

### **When to Use Each Category**

| Category | Best For | Example Use Cases |
|----------|----------|-------------------|
| **Linear Models** | Standard regression analysis | Sales forecasting, economic modeling |
| **GLM Models** | Non-normal data, discrete outcomes | Binary classification, count data analysis |
| **Nonparametric** | Unknown data distributions | Density estimation, flexible regression |
| **Robust Models** | Outlier-prone data | Financial data, sensor readings |
| **ANOVA** | Group comparisons | A/B testing, experimental design |

### **Model Selection Guidelines**

```python
def select_appropriate_model(data, dependent_var, analysis_goal):
    """Helper function to select appropriate model category."""
    y = data[dependent_var]
    
    if analysis_goal == "group_comparison":
        return "anova"
    elif y.nunique() == 2:  # Binary outcome
        return "glm"  # Use binomial_glm or logit
    elif y.dtype in ['int64', 'int32'] and y.min() >= 0:  # Count data
        return "glm"  # Use poisson_glm or negative_binomial_glm
    elif "outliers" in analysis_goal:
        return "robust"
    elif "distribution_unknown" in analysis_goal:
        return "nonparametric"
    else:
        return "linear"  # Default to linear models
```

## ğŸ¤ Contributing

To contribute to this project:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**ğŸ‰ Ready for Production Use!** All 53+ models are fully tested, documented, and ready for integration with LangGraph-powered statistical analysis agents.
