#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç³»ç»ŸéªŒè¯æµ‹è¯• - å…¨é¢çš„ç«¯åˆ°ç«¯é›†æˆéªŒè¯å’Œè´¨é‡ä¿è¯
"""

import unittest
import tempfile
import os
import sys
import time
import json
import yaml
import threading
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„å’Œsrcè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))


class SystemValidationResult:
    """ç³»ç»ŸéªŒè¯ç»“æœ"""
    
    def __init__(self):
        self.tests_run = 0
        self.tests_passed = 0
        self.tests_failed = 0
        self.tests_skipped = 0
        self.issues_found = []
        self.performance_metrics = {}
        
    def add_result(self, test_name, passed, issue=None, duration=None):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.tests_run += 1
        if passed:
            self.tests_passed += 1
        else:
            self.tests_failed += 1
            if issue:
                self.issues_found.append(f"{test_name}: {issue}")
                
        if duration:
            self.performance_metrics[test_name] = duration
            
    def skip_test(self, test_name, reason):
        """è·³è¿‡æµ‹è¯•"""
        self.tests_run += 1
        self.tests_skipped += 1
        self.issues_found.append(f"{test_name}: SKIPPED - {reason}")
        
    def print_summary(self):
        """æ‰“å°éªŒè¯æ‘˜è¦"""
        print(f"\nğŸ¯ ç³»ç»ŸéªŒè¯ç»“æœæ‘˜è¦")
        print(f"=" * 50)
        print(f"æ€»æµ‹è¯•æ•°: {self.tests_run}")
        print(f"é€šè¿‡: {self.tests_passed} âœ…")
        print(f"å¤±è´¥: {self.tests_failed} âŒ")
        print(f"è·³è¿‡: {self.tests_skipped} âš ï¸")
        
        if self.tests_run > 0:
            success_rate = (self.tests_passed / self.tests_run) * 100
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
            
        if self.issues_found:
            print(f"\nğŸ” å‘ç°çš„é—®é¢˜:")
            for issue in self.issues_found:
                print(f"  - {issue}")
                
        if self.performance_metrics:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            for test, duration in self.performance_metrics.items():
                print(f"  - {test}: {duration:.4f}s")


class TestCompleteSystemValidation(unittest.TestCase):
    """å®Œæ•´ç³»ç»ŸéªŒè¯æµ‹è¯•"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.validation_result = SystemValidationResult()
        self.temp_dir = tempfile.mkdtemp()
        self.test_config = self._create_comprehensive_config()
        
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
        self.validation_result.print_summary()
        
    def _create_comprehensive_config(self):
        """åˆ›å»ºå…¨é¢çš„æµ‹è¯•é…ç½®"""
        config_file = os.path.join(self.temp_dir, 'system_config.yaml')
        
        comprehensive_config = {
            'channels': {
                'dingtalk': {
                    'enabled': True,
                    'webhook': 'https://oapi.dingtalk.com/robot/send?access_token=test',
                    'secret': 'test_secret'
                },
                'email': {
                    'enabled': True,
                    'smtp_host': 'smtp.gmail.com',
                    'smtp_port': 587,
                    'sender': 'test@example.com',
                    'password': 'test_password',
                    'receivers': ['admin@example.com']
                },
                'telegram': {
                    'enabled': False,
                    'token': 'test_token',
                    'chat_id': 'test_chat'
                }
            },
            'events': {
                'builtin': {
                    'sensitive_operation': {
                        'enabled': True,
                        'channels': ['dingtalk', 'email']
                    },
                    'task_completion': {
                        'enabled': True,
                        'channels': ['dingtalk']
                    },
                    'error_occurred': {
                        'enabled': True,
                        'channels': ['email']
                    },
                    'session_start': {
                        'enabled': False
                    }
                },
                'custom': {
                    'git_operation': {
                        'name': 'Gitæ“ä½œæ£€æµ‹',
                        'priority': 'normal',
                        'enabled': True,
                        'channels': ['dingtalk'],
                        'triggers': [{
                            'type': 'pattern',
                            'pattern': r'git\s+(commit|push|merge)',
                            'field': 'tool_input'
                        }]
                    },
                    'production_deploy': {
                        'name': 'ç”Ÿäº§éƒ¨ç½²æ£€æµ‹',
                        'priority': 'critical',
                        'enabled': True,
                        'channels': ['dingtalk', 'email'],
                        'triggers': [{
                            'type': 'condition',
                            'field': 'project',
                            'operator': 'contains',
                            'value': 'prod'
                        }]
                    }
                }
            },
            'notifications': {
                'default_channels': ['dingtalk'],
                'templates': {
                    'default': 'basic',
                    'detailed': 'comprehensive',
                    'alert': 'urgent'
                }
            },
            'intelligent_limiting': {
                'enabled': True,
                'operation_gate': {
                    'enabled': True,
                    'strategies': {
                        'critical_operations': {
                            'type': 'hard_block',
                            'patterns': ['sudo rm -rf', 'DROP TABLE', 'DELETE FROM']
                        }
                    }
                },
                'notification_throttle': {
                    'enabled': True,
                    'limits': {
                        'per_minute': 5,
                        'per_hour': 50
                    }
                }
            },
            'monitoring': {
                'enabled': True,
                'statistics': {
                    'enabled': True,
                    'file_path': os.path.join(self.temp_dir, 'system_stats.json')
                },
                'health_check': {
                    'enabled': True,
                    'check_interval': 30
                }
            },
            'advanced': {
                'logging': {
                    'level': 'INFO',
                    'file': os.path.join(self.temp_dir, 'system_test.log')
                }
            }
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(comprehensive_config, f, default_flow_style=False)
            
        return config_file
        
    def test_complete_notification_workflow(self):
        """æµ‹è¯•å®Œæ•´é€šçŸ¥å·¥ä½œæµç¨‹"""
        test_name = "complete_notification_workflow"
        start_time = time.time()
        
        try:
            # 1. éªŒè¯é…ç½®åŠ è½½
            from config_manager import ConfigManager
            config_manager = ConfigManager(self.test_config)
            config = config_manager.get_config()
            
            self.assertIn('channels', config)
            self.assertIn('events', config)
            
            # 2. éªŒè¯äº‹ä»¶ç®¡ç†å™¨
            from managers.event_manager import EventManager
            event_manager = EventManager(config)
            
            # 3. åˆ›å»ºé€šé“
            from channels.dingtalk import DingtalkChannel
            dingtalk_config = config['channels']['dingtalk']
            dingtalk_channel = DingtalkChannel(dingtalk_config)
            
            # 4. æµ‹è¯•å¤šç§äº‹ä»¶åœºæ™¯
            test_scenarios = [
                {
                    'name': 'sensitive_operation',
                    'context': {
                        'tool_input': 'sudo rm -rf /tmp/test',
                        'project': 'system-validation-test',
                        'user': 'test-user'
                    }
                },
                {
                    'name': 'git_operation',
                    'context': {
                        'tool_input': 'git push origin main',
                        'project': 'validation-project'
                    }
                },
                {
                    'name': 'production_deploy',
                    'context': {
                        'project': 'my-prod-app',
                        'operation': 'deploy'
                    }
                }
            ]
            
            workflow_results = []
            
            for scenario in test_scenarios:
                # å¤„ç†äº‹ä»¶
                triggered_events = event_manager.process_context(scenario['context'])
                
                # è®°å½•ç»“æœ
                workflow_results.append({
                    'scenario': scenario['name'],
                    'events_triggered': len(triggered_events),
                    'events': triggered_events
                })
                
            # éªŒè¯å·¥ä½œæµç¨‹
            total_events = sum(result['events_triggered'] for result in workflow_results)
            
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, total_events > 0, duration=duration)
            
            print(f"\nğŸ”„ å·¥ä½œæµç¨‹æµ‹è¯•ç»“æœ:")
            for result in workflow_results:
                print(f"  - {result['scenario']}: {result['events_triggered']} äº‹ä»¶")
                
        except Exception as e:
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, False, str(e), duration)
            
    def test_multi_channel_integration(self):
        """æµ‹è¯•å¤šæ¸ é“é›†æˆ"""
        test_name = "multi_channel_integration"
        start_time = time.time()
        
        try:
            from config_manager import ConfigManager
            from channels.dingtalk import DingtalkChannel
            from channels.email import EmailChannel
            
            config_manager = ConfigManager(self.test_config)
            config = config_manager.get_config()
            
            # åˆ›å»ºæ‰€æœ‰å¯ç”¨çš„æ¸ é“
            active_channels = {}
            
            for channel_name, channel_config in config['channels'].items():
                if channel_config.get('enabled', False):
                    try:
                        if channel_name == 'dingtalk':
                            channel = DingtalkChannel(channel_config)
                        elif channel_name == 'email':
                            channel = EmailChannel(channel_config)
                        else:
                            continue
                            
                        # éªŒè¯æ¸ é“é…ç½®
                        if channel.validate_config():
                            active_channels[channel_name] = channel
                            
                    except Exception as e:
                        print(f"æ¸ é“ {channel_name} åˆ›å»ºå¤±è´¥: {e}")
                        
            # éªŒè¯å¤šæ¸ é“åˆ›å»ºæˆåŠŸ
            channel_count = len(active_channels)
            
            duration = time.time() - start_time
            self.validation_result.add_result(
                test_name, 
                channel_count > 0, 
                f"åˆ›å»ºäº† {channel_count} ä¸ªæ¸ é“" if channel_count > 0 else "æ²¡æœ‰æ¸ é“åˆ›å»ºæˆåŠŸ",
                duration
            )
            
            print(f"\nğŸ“¡ å¤šæ¸ é“é›†æˆç»“æœ: {channel_count} ä¸ªæ´»è·ƒæ¸ é“")
            for name in active_channels.keys():
                print(f"  - {name}: âœ…")
                
        except Exception as e:
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, False, str(e), duration)
            
    def test_intelligence_systems_integration(self):
        """æµ‹è¯•æ™ºèƒ½ç³»ç»Ÿé›†æˆ"""
        test_name = "intelligence_systems_integration"
        start_time = time.time()
        
        # æ£€æŸ¥æ™ºèƒ½ç»„ä»¶å¯ç”¨æ€§
        intelligence_available = True
        try:
            from claude_notifier.utils.operation_gate import OperationGate
            from claude_notifier.utils.notification_throttle import NotificationThrottle
        except ImportError:
            intelligence_available = False
            
        if not intelligence_available:
            self.validation_result.skip_test(test_name, "æ™ºèƒ½ç»„ä»¶ä¸å¯ç”¨")
            return
            
        try:
            # æµ‹è¯•æ“ä½œé—¨
            from claude_notifier.utils.operation_gate import OperationGate, OperationRequest
            
            gate_config = {
                'enabled': True,
                'strategies': {
                    'critical': {
                        'type': 'hard_block',
                        'patterns': ['sudo rm -rf']
                    }
                }
            }
            
            operation_gate = OperationGate(gate_config)
            
            # æµ‹è¯•é˜»æ­¢æ“ä½œ
            dangerous_request = OperationRequest(
                command='sudo rm -rf /',
                context={'project': 'test'},
                priority='normal'
            )
            
            result, message = operation_gate.should_allow_operation(dangerous_request)
            
            # æµ‹è¯•é€šçŸ¥é™æµ
            from claude_notifier.utils.notification_throttle import NotificationThrottle, NotificationRequest
            
            throttle_config = {
                'enabled': True,
                'limits': {'per_minute': 5}
            }
            
            throttle = NotificationThrottle(throttle_config)
            
            # æµ‹è¯•é™æµé€»è¾‘
            notification_request = NotificationRequest(
                content='test notification',
                channel='test',
                event_type='test'
            )
            
            throttle_result, throttle_message, delay = throttle.should_allow_notification(notification_request)
            
            # éªŒè¯æ™ºèƒ½ç³»ç»ŸåŠŸèƒ½
            intelligence_working = (result is not None) and (throttle_result is not None)
            
            duration = time.time() - start_time
            self.validation_result.add_result(
                test_name,
                intelligence_working,
                "æ™ºèƒ½ç³»ç»Ÿæ­£å¸¸å·¥ä½œ" if intelligence_working else "æ™ºèƒ½ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸",
                duration
            )
            
            print(f"\nğŸ§  æ™ºèƒ½ç³»ç»Ÿé›†æˆç»“æœ:")
            print(f"  - æ“ä½œé—¨: {'âœ…' if result is not None else 'âŒ'}")
            print(f"  - é€šçŸ¥é™æµ: {'âœ…' if throttle_result is not None else 'âŒ'}")
            
        except Exception as e:
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, False, str(e), duration)
            
    def test_monitoring_systems_integration(self):
        """æµ‹è¯•ç›‘æ§ç³»ç»Ÿé›†æˆ"""
        test_name = "monitoring_systems_integration"
        start_time = time.time()
        
        # æ£€æŸ¥ç›‘æ§ç»„ä»¶å¯ç”¨æ€§
        monitoring_available = True
        try:
            from claude_notifier.monitoring.statistics import StatisticsManager
            from claude_notifier.monitoring.health_check import HealthChecker
        except ImportError:
            monitoring_available = False
            
        if not monitoring_available:
            self.validation_result.skip_test(test_name, "ç›‘æ§ç»„ä»¶ä¸å¯ç”¨")
            return
            
        try:
            from claude_notifier.monitoring.statistics import StatisticsManager
            from claude_notifier.monitoring.health_check import HealthChecker
            
            # åˆ›å»ºç»Ÿè®¡ç®¡ç†å™¨
            stats_file = os.path.join(self.temp_dir, 'integration_stats.json')
            stats_manager = StatisticsManager(stats_file)
            
            # åˆ›å»ºå¥åº·æ£€æŸ¥å™¨
            health_config = {'enabled': True}
            health_checker = HealthChecker(health_config)
            
            # æµ‹è¯•ç»Ÿè®¡æ”¶é›†
            stats_manager.record_intelligence_event('operation_gate', 'blocked')
            stats_manager.record_monitoring_event('health_check', 'ok')
            stats_manager.record_cli_operation('status', 0.1, 'success')
            
            # æµ‹è¯•å¥åº·æ£€æŸ¥
            health_result = health_checker.get_system_health()
            
            # éªŒè¯ç›‘æ§åŠŸèƒ½
            monitoring_working = (
                health_result is not None and
                'status' in health_result
            )
            
            duration = time.time() - start_time
            self.validation_result.add_result(
                test_name,
                monitoring_working,
                "ç›‘æ§ç³»ç»Ÿæ­£å¸¸å·¥ä½œ" if monitoring_working else "ç›‘æ§ç³»ç»ŸåŠŸèƒ½å¼‚å¸¸",
                duration
            )
            
            print(f"\nğŸ“Š ç›‘æ§ç³»ç»Ÿé›†æˆç»“æœ:")
            print(f"  - ç»Ÿè®¡ç®¡ç†å™¨: âœ…")
            print(f"  - å¥åº·æ£€æŸ¥: {'âœ…' if monitoring_working else 'âŒ'}")
            
        except Exception as e:
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, False, str(e), duration)
            
    def test_configuration_robustness(self):
        """æµ‹è¯•é…ç½®ç³»ç»Ÿå¥å£®æ€§"""
        test_name = "configuration_robustness"
        start_time = time.time()
        
        try:
            from config_manager import ConfigManager
            
            robustness_results = []
            
            # 1. æµ‹è¯•æ­£å¸¸é…ç½®
            try:
                config_manager = ConfigManager(self.test_config)
                config = config_manager.get_config()
                robustness_results.append(("normal_config", True))
            except Exception as e:
                robustness_results.append(("normal_config", False, str(e)))
                
            # 2. æµ‹è¯•æ— æ•ˆé…ç½®æ–‡ä»¶
            try:
                invalid_config = os.path.join(self.temp_dir, 'invalid.yaml')
                with open(invalid_config, 'w') as f:
                    f.write("invalid: yaml: content:")
                    
                config_manager = ConfigManager(invalid_config)
                robustness_results.append(("invalid_yaml", False, "åº”è¯¥å¤±è´¥ä½†æ²¡æœ‰"))
            except Exception:
                robustness_results.append(("invalid_yaml", True))  # é¢„æœŸå¤±è´¥
                
            # 3. æµ‹è¯•ä¸å­˜åœ¨çš„é…ç½®æ–‡ä»¶
            try:
                nonexistent_config = os.path.join(self.temp_dir, 'nonexistent.yaml')
                config_manager = ConfigManager(nonexistent_config)
                robustness_results.append(("nonexistent_file", False, "åº”è¯¥å¤±è´¥ä½†æ²¡æœ‰"))
            except Exception:
                robustness_results.append(("nonexistent_file", True))  # é¢„æœŸå¤±è´¥
                
            # 4. æµ‹è¯•é…ç½®éªŒè¯
            try:
                config_manager = ConfigManager(self.test_config)
                errors = config_manager.validate_config()
                robustness_results.append(("config_validation", True))
            except Exception as e:
                robustness_results.append(("config_validation", False, str(e)))
                
            # è®¡ç®—å¥å£®æ€§åˆ†æ•°
            passed_tests = sum(1 for result in robustness_results if result[1])
            total_tests = len(robustness_results)
            robustness_score = (passed_tests / total_tests) * 100
            
            duration = time.time() - start_time
            self.validation_result.add_result(
                test_name,
                robustness_score >= 75,  # è‡³å°‘75%é€šè¿‡
                f"å¥å£®æ€§åˆ†æ•°: {robustness_score:.1f}%",
                duration
            )
            
            print(f"\nğŸ›¡ï¸ é…ç½®å¥å£®æ€§æµ‹è¯•:")
            for result in robustness_results:
                status = "âœ…" if result[1] else "âŒ"
                print(f"  - {result[0]}: {status}")
                
        except Exception as e:
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, False, str(e), duration)
            
    def test_system_stress_tolerance(self):
        """æµ‹è¯•ç³»ç»Ÿå‹åŠ›å®¹å¿åº¦"""
        test_name = "system_stress_tolerance"
        start_time = time.time()
        
        try:
            from utils.helpers import is_sensitive_operation
            from utils.statistics import StatisticsManager
            
            # åˆ›å»ºç»Ÿè®¡ç®¡ç†å™¨
            stats_file = os.path.join(self.temp_dir, 'stress_stats.json')
            stats_manager = StatisticsManager(stats_file)
            
            # å‹åŠ›æµ‹è¯•å‚æ•°
            operations_count = 1000
            concurrent_threads = 4
            
            def stress_test_worker(worker_id, operations_per_worker):
                """å‹åŠ›æµ‹è¯•å·¥ä½œå™¨"""
                results = []
                for i in range(operations_per_worker):
                    try:
                        # æ¨¡æ‹Ÿå„ç§æ“ä½œ
                        command = f"test command {worker_id}_{i}"
                        is_sensitive = is_sensitive_operation(command)
                        
                        # è®°å½•ç»Ÿè®¡
                        stats_manager.record_event(f'stress_test_{worker_id}', ['test'])
                        stats_manager.record_notification('test', True, 0.001)
                        
                        results.append(True)
                    except Exception as e:
                        results.append(False)
                        
                return results
                
            # è¿è¡Œå¹¶å‘å‹åŠ›æµ‹è¯•
            operations_per_worker = operations_count // concurrent_threads
            
            with ThreadPoolExecutor(max_workers=concurrent_threads) as executor:
                futures = [
                    executor.submit(stress_test_worker, worker_id, operations_per_worker)
                    for worker_id in range(concurrent_threads)
                ]
                
                stress_results = []
                for future in as_completed(futures):
                    worker_results = future.result()
                    stress_results.extend(worker_results)
                    
            # è®¡ç®—æˆåŠŸç‡
            successful_operations = sum(stress_results)
            total_operations = len(stress_results)
            success_rate = (successful_operations / total_operations) * 100
            
            # ç”Ÿæˆå‹åŠ›æµ‹è¯•æŠ¥å‘Š
            try:
                report = stats_manager.generate_report()
                report_generated = len(report) > 0
            except Exception:
                report_generated = False
                
            stress_tolerance_good = success_rate >= 95 and report_generated
            
            duration = time.time() - start_time
            self.validation_result.add_result(
                test_name,
                stress_tolerance_good,
                f"æˆåŠŸç‡: {success_rate:.1f}%, æŠ¥å‘Š: {'âœ…' if report_generated else 'âŒ'}",
                duration
            )
            
            print(f"\nğŸ’ª ç³»ç»Ÿå‹åŠ›æµ‹è¯•:")
            print(f"  - æ€»æ“ä½œæ•°: {total_operations}")
            print(f"  - æˆåŠŸæ“ä½œ: {successful_operations}")
            print(f"  - æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"  - å¹¶å‘çº¿ç¨‹: {concurrent_threads}")
            print(f"  - æŠ¥å‘Šç”Ÿæˆ: {'âœ…' if report_generated else 'âŒ'}")
            
        except Exception as e:
            duration = time.time() - start_time
            self.validation_result.add_result(test_name, False, str(e), duration)


def run_system_validation():
    """è¿è¡Œç³»ç»ŸéªŒè¯æµ‹è¯•"""
    print("ğŸ¯ å¯åŠ¨ç³»ç»Ÿå…¨é¢éªŒè¯æµ‹è¯•")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"æµ‹è¯•ç¯å¢ƒ: {os.getcwd()}")
    
    # è¿è¡Œæµ‹è¯•
    test_suite = unittest.TestSuite()
    test_suite.addTests(
        unittest.TestLoader().loadTestsFromTestCase(TestCompleteSystemValidation)
    )
    
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    return result.wasSuccessful()


if __name__ == '__main__':
    success = run_system_validation()
    
    if success:
        print(f"\nğŸ‰ ç³»ç»ŸéªŒè¯å®Œæˆï¼æ‰€æœ‰æµ‹è¯•é€šè¿‡")
    else:
        print(f"\nâš ï¸ ç³»ç»ŸéªŒè¯å‘ç°é—®é¢˜ï¼Œéœ€è¦å…³æ³¨")
        
    sys.exit(0 if success else 1)