#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ€§èƒ½åŸºå‡†æµ‹è¯• - æµ‹é‡ç³»ç»Ÿå“åº”æ—¶é—´ã€å†…å­˜ä½¿ç”¨å’Œååé‡
"""

import unittest
import time
import threading
import concurrent.futures
import tempfile
import os
import sys
import psutil
import yaml
from pathlib import Path
from unittest.mock import Mock, patch
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„å’Œsrcè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.start_time = None
        self.end_time = None
        
    def start_timing(self):
        """å¼€å§‹è®¡æ—¶"""
        self.start_time = time.perf_counter()
        
    def end_timing(self, operation_name):
        """ç»“æŸè®¡æ—¶å¹¶è®°å½•"""
        self.end_time = time.perf_counter()
        if self.start_time:
            duration = self.end_time - self.start_time
            self.metrics[operation_name].append(duration)
            return duration
        return None
        
    def record_memory(self, operation_name):
        """è®°å½•å†…å­˜ä½¿ç”¨"""
        process = psutil.Process()
        memory_info = process.memory_info()
        self.metrics[f"{operation_name}_memory_rss"].append(memory_info.rss / 1024 / 1024)  # MB
        self.metrics[f"{operation_name}_memory_vms"].append(memory_info.vms / 1024 / 1024)  # MB
        
    def get_statistics(self, operation_name):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if operation_name not in self.metrics:
            return None
            
        values = self.metrics[operation_name]
        if not values:
            return None
            
        return {
            'count': len(values),
            'min': min(values),
            'max': max(values),
            'avg': sum(values) / len(values),
            'total': sum(values)
        }
        
    def print_report(self):
        """æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        for operation in sorted(self.metrics.keys()):
            if operation.endswith('_memory_rss') or operation.endswith('_memory_vms'):
                continue
                
            stats = self.get_statistics(operation)
            if stats:
                print(f"\nğŸ”¹ {operation}:")
                print(f"   æ‰§è¡Œæ¬¡æ•°: {stats['count']}")
                print(f"   æœ€å°æ—¶é—´: {stats['min']:.4f}s")
                print(f"   æœ€å¤§æ—¶é—´: {stats['max']:.4f}s")
                print(f"   å¹³å‡æ—¶é—´: {stats['avg']:.4f}s")
                print(f"   æ€»æ—¶é—´: {stats['total']:.4f}s")
                
                # å†…å­˜ä¿¡æ¯
                memory_stats = self.get_statistics(f"{operation}_memory_rss")
                if memory_stats:
                    print(f"   å†…å­˜ä½¿ç”¨: {memory_stats['avg']:.2f} MB")


class TestBasicOperationPerformance(unittest.TestCase):
    """æµ‹è¯•åŸºæœ¬æ“ä½œæ€§èƒ½"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.metrics = PerformanceMetrics()
        self.temp_dir = tempfile.mkdtemp()
        
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
        self.metrics.print_report()
        
    def test_helper_function_performance(self):
        """æµ‹è¯•å·¥å…·å‡½æ•°æ€§èƒ½"""
        try:
            from utils.helpers import is_sensitive_operation, truncate_text, escape_markdown
            
            # æµ‹è¯•æ•æ„Ÿæ“ä½œæ£€æµ‹æ€§èƒ½
            test_commands = [
                'sudo rm -rf /',
                'ls -la',
                'git commit -m "test"',
                'npm install',
                'DROP TABLE users'
            ] * 100  # 500æ¬¡æµ‹è¯•
            
            self.metrics.start_timing()
            self.metrics.record_memory('sensitive_operation_detection')
            
            for cmd in test_commands:
                is_sensitive_operation(cmd)
                
            duration = self.metrics.end_timing('sensitive_operation_detection')
            self.metrics.record_memory('sensitive_operation_detection')
            
            # æ€§èƒ½æ–­è¨€
            self.assertLess(duration, 1.0, "æ•æ„Ÿæ“ä½œæ£€æµ‹åº”è¯¥åœ¨1ç§’å†…å®Œæˆ500æ¬¡")
            
            # æµ‹è¯•æ–‡æœ¬æˆªæ–­æ€§èƒ½
            long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬å†…å®¹" * 1000
            
            self.metrics.start_timing()
            
            for _ in range(1000):
                truncate_text(long_text, 100)
                
            duration = self.metrics.end_timing('text_truncation')
            
            self.assertLess(duration, 0.5, "æ–‡æœ¬æˆªæ–­åº”è¯¥åœ¨0.5ç§’å†…å®Œæˆ1000æ¬¡")
            
            # æµ‹è¯•Markdownè½¬ä¹‰æ€§èƒ½
            markdown_text = "è¿™æ˜¯*ç²—ä½“*å’Œ_æ–œä½“_å’Œ`ä»£ç `" * 100
            
            self.metrics.start_timing()
            
            for _ in range(500):
                escape_markdown(markdown_text)
                
            duration = self.metrics.end_timing('markdown_escaping')
            
            self.assertLess(duration, 0.3, "Markdownè½¬ä¹‰åº”è¯¥åœ¨0.3ç§’å†…å®Œæˆ500æ¬¡")
            
        except ImportError as e:
            self.skipTest(f"å·¥å…·å‡½æ•°ä¸å¯ç”¨: {e}")
            
    def test_channel_initialization_performance(self):
        """æµ‹è¯•é€šé“åˆå§‹åŒ–æ€§èƒ½"""
        try:
            from channels.dingtalk import DingtalkChannel
            from channels.email import EmailChannel
            
            configs = {
                'dingtalk': {
                    'enabled': True,
                    'webhook': 'https://test.webhook.com',
                    'secret': 'test'
                },
                'email': {
                    'enabled': True,
                    'smtp_host': 'smtp.test.com',
                    'sender': 'test@test.com',
                    'password': 'test',
                    'receivers': ['user@test.com']
                }
            }
            
            # æµ‹è¯•DingTalkåˆå§‹åŒ–æ€§èƒ½
            self.metrics.start_timing()
            
            channels = []
            for _ in range(100):
                channel = DingtalkChannel(configs['dingtalk'])
                channels.append(channel)
                
            duration = self.metrics.end_timing('dingtalk_initialization')
            self.assertLess(duration, 0.5, "100ä¸ªDingTalké€šé“åˆå§‹åŒ–åº”è¯¥åœ¨0.5ç§’å†…å®Œæˆ")
            
            # æµ‹è¯•Emailåˆå§‹åŒ–æ€§èƒ½
            self.metrics.start_timing()
            
            email_channels = []
            for _ in range(100):
                channel = EmailChannel(configs['email'])
                email_channels.append(channel)
                
            duration = self.metrics.end_timing('email_initialization')
            self.assertLess(duration, 0.5, "100ä¸ªEmailé€šé“åˆå§‹åŒ–åº”è¯¥åœ¨0.5ç§’å†…å®Œæˆ")
            
        except ImportError as e:
            self.skipTest(f"é€šé“æ¨¡å—ä¸å¯ç”¨: {e}")
            
    def test_event_processing_performance(self):
        """æµ‹è¯•äº‹ä»¶å¤„ç†æ€§èƒ½"""
        try:
            from events.builtin import SensitiveOperationEvent, TaskCompletionEvent
            
            # åˆ›å»ºäº‹ä»¶
            sensitive_event = SensitiveOperationEvent()
            completion_event = TaskCompletionEvent()
            
            # å‡†å¤‡æµ‹è¯•ä¸Šä¸‹æ–‡
            contexts = [
                {'tool_input': 'sudo rm -rf /test', 'project': 'test'},
                {'tool_input': 'ls -la', 'project': 'test'},
                {'status': 'completed', 'task_count': 5},
                {'status': 'running', 'task_count': 3}
            ] * 250  # 1000æ¬¡æµ‹è¯•
            
            # æµ‹è¯•æ•æ„Ÿæ“ä½œäº‹ä»¶æ€§èƒ½
            self.metrics.start_timing()
            
            for context in contexts:
                sensitive_event.should_trigger(context)
                
            duration = self.metrics.end_timing('sensitive_event_processing')
            self.assertLess(duration, 1.0, "æ•æ„Ÿäº‹ä»¶å¤„ç†åº”è¯¥åœ¨1ç§’å†…å®Œæˆ1000æ¬¡")
            
            # æµ‹è¯•ä»»åŠ¡å®Œæˆäº‹ä»¶æ€§èƒ½
            self.metrics.start_timing()
            
            for context in contexts:
                completion_event.should_trigger(context)
                
            duration = self.metrics.end_timing('completion_event_processing')
            self.assertLess(duration, 1.0, "å®Œæˆäº‹ä»¶å¤„ç†åº”è¯¥åœ¨1ç§’å†…å®Œæˆ1000æ¬¡")
            
        except ImportError as e:
            self.skipTest(f"äº‹ä»¶æ¨¡å—ä¸å¯ç”¨: {e}")
            
    def test_config_loading_performance(self):
        """æµ‹è¯•é…ç½®åŠ è½½æ€§èƒ½"""
        try:
            from config_manager import ConfigManager
            
            # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
            config_file = os.path.join(self.temp_dir, 'perf_config.yaml')
            
            large_config = {
                'channels': {},
                'events': {'builtin': {}, 'custom': {}},
                'notifications': {'templates': {}}
            }
            
            # åˆ›å»ºè¾ƒå¤§çš„é…ç½®
            for i in range(50):
                large_config['channels'][f'channel_{i}'] = {
                    'enabled': True,
                    'webhook': f'https://test{i}.webhook.com'
                }
                
            for i in range(30):
                large_config['events']['builtin'][f'event_{i}'] = {'enabled': True}
                
            with open(config_file, 'w') as f:
                yaml.dump(large_config, f)
                
            # æµ‹è¯•é…ç½®åŠ è½½æ€§èƒ½
            self.metrics.start_timing()
            self.metrics.record_memory('config_loading')
            
            for _ in range(100):
                config_manager = ConfigManager(config_file)
                config = config_manager.get_config()
                
            duration = self.metrics.end_timing('config_loading')
            self.metrics.record_memory('config_loading')
            
            self.assertLess(duration, 2.0, "100æ¬¡å¤§é…ç½®åŠ è½½åº”è¯¥åœ¨2ç§’å†…å®Œæˆ")
            
        except ImportError as e:
            self.skipTest(f"é…ç½®ç®¡ç†å™¨ä¸å¯ç”¨: {e}")


class TestConcurrencyPerformance(unittest.TestCase):
    """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.metrics = PerformanceMetrics()
        
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.metrics.print_report()
        
    def test_concurrent_event_processing(self):
        """æµ‹è¯•å¹¶å‘äº‹ä»¶å¤„ç†"""
        try:
            from events.builtin import SensitiveOperationEvent
            
            event = SensitiveOperationEvent()
            
            def process_events(context_batch):
                """å¤„ç†ä¸€æ‰¹äº‹ä»¶"""
                results = []
                for context in context_batch:
                    result = event.should_trigger(context)
                    results.append(result)
                return results
                
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            contexts = [
                {'tool_input': f'test command {i}', 'project': 'test'}
                for i in range(1000)
            ]
            
            # åˆ†æ‰¹å¤„ç†
            batch_size = 100
            batches = [contexts[i:i+batch_size] for i in range(0, len(contexts), batch_size)]
            
            # æµ‹è¯•ä¸²è¡Œå¤„ç†
            self.metrics.start_timing()
            
            serial_results = []
            for batch in batches:
                result = process_events(batch)
                serial_results.extend(result)
                
            serial_duration = self.metrics.end_timing('serial_event_processing')
            
            # æµ‹è¯•å¹¶è¡Œå¤„ç†
            self.metrics.start_timing()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [executor.submit(process_events, batch) for batch in batches]
                parallel_results = []
                for future in concurrent.futures.as_completed(futures):
                    parallel_results.extend(future.result())
                    
            parallel_duration = self.metrics.end_timing('parallel_event_processing')
            
            # éªŒè¯ç»“æœä¸€è‡´æ€§
            self.assertEqual(len(serial_results), len(parallel_results))
            
            # æ€§èƒ½æ¯”è¾ƒ
            speedup = serial_duration / parallel_duration
            print(f"\nå¹¶å‘æ€§èƒ½æå‡: {speedup:.2f}x")
            
            # å¹¶è¡Œå¤„ç†åº”è¯¥è‡³å°‘æœ‰ä¸€å®šçš„æ€§èƒ½æå‡
            self.assertGreater(speedup, 0.8, "å¹¶è¡Œå¤„ç†åº”è¯¥æœ‰æ€§èƒ½æå‡æˆ–è‡³å°‘ä¸å˜å·®")
            
        except ImportError as e:
            self.skipTest(f"äº‹ä»¶æ¨¡å—ä¸å¯ç”¨: {e}")
            
    def test_concurrent_channel_operations(self):
        """æµ‹è¯•å¹¶å‘é€šé“æ“ä½œ"""
        try:
            from channels.dingtalk import DingtalkChannel
            
            config = {
                'enabled': True,
                'webhook': 'https://test.webhook.com'
            }
            
            def create_and_validate_channels(count):
                """åˆ›å»ºå’ŒéªŒè¯å¤šä¸ªé€šé“"""
                channels = []
                for _ in range(count):
                    channel = DingtalkChannel(config)
                    validation_result = channel.validate_config()
                    channels.append((channel, validation_result))
                return channels
                
            # æµ‹è¯•ä¸²è¡Œåˆ›å»º
            self.metrics.start_timing()
            serial_channels = create_and_validate_channels(200)
            serial_duration = self.metrics.end_timing('serial_channel_creation')
            
            # æµ‹è¯•å¹¶è¡Œåˆ›å»º
            self.metrics.start_timing()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = [
                    executor.submit(create_and_validate_channels, 50)
                    for _ in range(4)
                ]
                parallel_channels = []
                for future in concurrent.futures.as_completed(futures):
                    parallel_channels.extend(future.result())
                    
            parallel_duration = self.metrics.end_timing('parallel_channel_creation')
            
            # éªŒè¯ç»“æœ
            self.assertEqual(len(serial_channels), len(parallel_channels))
            
            speedup = serial_duration / parallel_duration
            print(f"\né€šé“åˆ›å»ºå¹¶å‘æå‡: {speedup:.2f}x")
            
        except ImportError as e:
            self.skipTest(f"é€šé“æ¨¡å—ä¸å¯ç”¨: {e}")


class TestMemoryPerformance(unittest.TestCase):
    """æµ‹è¯•å†…å­˜æ€§èƒ½"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.metrics = PerformanceMetrics()
        
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.metrics.print_report()
        
    def test_memory_usage_patterns(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æ¨¡å¼"""
        try:
            from utils.statistics import StatisticsManager
            import tempfile
            
            # åˆ›å»ºä¸´æ—¶ç»Ÿè®¡æ–‡ä»¶
            stats_file = tempfile.NamedTemporaryFile(delete=False, suffix='.json')
            stats_file.close()
            
            try:
                # æµ‹è¯•å¤§é‡æ“ä½œçš„å†…å­˜ä½¿ç”¨
                self.metrics.record_memory('memory_baseline')
                
                stats_manager = StatisticsManager(stats_file.name)
                
                # è®°å½•å¤§é‡äº‹ä»¶
                for i in range(1000):
                    stats_manager.record_event(f'test_event_{i % 10}', ['dingtalk'])
                    stats_manager.record_notification('dingtalk', True, 0.1)
                    
                    if i % 100 == 0:
                        self.metrics.record_memory(f'memory_after_{i}_operations')
                        
                # ç”ŸæˆæŠ¥å‘Šï¼ˆå¯èƒ½æ¶ˆè€—æ›´å¤šå†…å­˜ï¼‰
                report = stats_manager.generate_report()
                
                self.metrics.record_memory('memory_after_report')
                
                # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†
                baseline = self.metrics.get_statistics('memory_baseline_memory_rss')
                final = self.metrics.get_statistics('memory_after_report_memory_rss')
                
                if baseline and final:
                    memory_increase = final['avg'] - baseline['avg']
                    print(f"\nå†…å­˜ä½¿ç”¨å¢é•¿: {memory_increase:.2f} MB")
                    
                    # å†…å­˜å¢é•¿åº”è¯¥åˆç†ï¼ˆå°äº100MBï¼‰
                    self.assertLess(memory_increase, 100, "å¤§é‡æ“ä½œåå†…å­˜å¢é•¿åº”è¯¥å°äº100MB")
                    
            finally:
                os.unlink(stats_file.name)
                
        except ImportError as e:
            self.skipTest(f"ç»Ÿè®¡ç®¡ç†å™¨ä¸å¯ç”¨: {e}")
            
    def test_memory_leaks(self):
        """æµ‹è¯•å†…å­˜æ³„æ¼"""
        try:
            from channels.dingtalk import DingtalkChannel
            
            config = {
                'enabled': True,
                'webhook': 'https://test.webhook.com'
            }
            
            initial_memory = None
            memory_samples = []
            
            # å¤šè½®åˆ›å»ºå’Œé”€æ¯å¯¹è±¡
            for round_num in range(10):
                # åˆ›å»ºå¤§é‡å¯¹è±¡
                channels = []
                for _ in range(100):
                    channel = DingtalkChannel(config)
                    channels.append(channel)
                    
                # è®°å½•å†…å­˜
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                memory_samples.append(memory_mb)
                
                if initial_memory is None:
                    initial_memory = memory_mb
                    
                # æ˜¾å¼åˆ é™¤å¯¹è±¡
                del channels
                
            # åˆ†æå†…å­˜è¶‹åŠ¿
            if len(memory_samples) >= 3:
                # æ£€æŸ¥æœ€åå‡ ä¸ªæ ·æœ¬æ˜¯å¦ç¨³å®š
                last_three = memory_samples[-3:]
                memory_variance = max(last_three) - min(last_three)
                
                print(f"\nå†…å­˜ä½¿ç”¨æ ·æœ¬: {[f'{m:.1f}MB' for m in memory_samples]}")
                print(f"å†…å­˜å˜åŒ–èŒƒå›´: {memory_variance:.2f} MB")
                
                # å†…å­˜å˜åŒ–åº”è¯¥ç›¸å¯¹ç¨³å®šï¼ˆå°äº20MBæ³¢åŠ¨ï¼‰
                self.assertLess(memory_variance, 20, "å†…å­˜ä½¿ç”¨åº”è¯¥ç›¸å¯¹ç¨³å®šï¼Œä¸åº”è¯¥æœ‰å¤§çš„æ³„æ¼")
                
        except ImportError as e:
            self.skipTest(f"é€šé“æ¨¡å—ä¸å¯ç”¨: {e}")


class TestScalabilityPerformance(unittest.TestCase):
    """æµ‹è¯•å¯æ‰©å±•æ€§æ€§èƒ½"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.metrics = PerformanceMetrics()
        
    def tearDown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        self.metrics.print_report()
        
    def test_operation_scaling(self):
        """æµ‹è¯•æ“ä½œæ‰©å±•æ€§"""
        try:
            from utils.helpers import is_sensitive_operation
            
            # æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ“ä½œ
            scales = [100, 500, 1000, 2000]
            
            for scale in scales:
                test_commands = [f'test command {i}' for i in range(scale)]
                
                self.metrics.start_timing()
                
                for cmd in test_commands:
                    is_sensitive_operation(cmd)
                    
                duration = self.metrics.end_timing(f'scale_{scale}_operations')
                
                throughput = scale / duration
                print(f"è§„æ¨¡ {scale}: {duration:.4f}s, ååé‡: {throughput:.0f} ops/s")
                
            # åˆ†ææ‰©å±•æ€§
            scales_with_data = []
            for scale in scales:
                stats = self.metrics.get_statistics(f'scale_{scale}_operations')
                if stats:
                    throughput = scale / stats['avg']
                    scales_with_data.append((scale, throughput))
                    
            if len(scales_with_data) >= 2:
                # è®¡ç®—ååé‡å˜åŒ–
                first_throughput = scales_with_data[0][1]
                last_throughput = scales_with_data[-1][1]
                
                throughput_ratio = last_throughput / first_throughput
                
                print(f"\nååé‡æ¯”ç‡ (æœ€å¤§/æœ€å°): {throughput_ratio:.2f}")
                
                # ååé‡ä¸åº”è¯¥æ˜¾è‘—ä¸‹é™ï¼ˆçº¿æ€§æ‰©å±•æ€§ï¼‰
                self.assertGreater(throughput_ratio, 0.5, "ååé‡ä¸åº”è¯¥éšè§„æ¨¡æ˜¾è‘—ä¸‹é™")
                
        except ImportError as e:
            self.skipTest(f"å·¥å…·å‡½æ•°ä¸å¯ç”¨: {e}")


def run_performance_tests():
    """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestBasicOperationPerformance,
        TestConcurrencyPerformance,
        TestMemoryPerformance,
        TestScalabilityPerformance
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)
    
    return result.wasSuccessful()


if __name__ == '__main__':
    print("âš¡ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    # æ£€æŸ¥psutilæ˜¯å¦å¯ç”¨
    try:
        import psutil
        print(f"ç³»ç»Ÿä¿¡æ¯: CPUæ ¸å¿ƒæ•°={psutil.cpu_count()}, å†…å­˜={psutil.virtual_memory().total//1024//1024//1024}GB")
    except ImportError:
        print("âš ï¸ psutilä¸å¯ç”¨ï¼Œéƒ¨åˆ†å†…å­˜æµ‹è¯•å¯èƒ½è¢«è·³è¿‡")
        
    success = run_performance_tests()
    
    if success:
        print("\nâœ… æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ€§èƒ½åŸºå‡†æµ‹è¯•å‘ç°é—®é¢˜")
        
    sys.exit(0 if success else 1)