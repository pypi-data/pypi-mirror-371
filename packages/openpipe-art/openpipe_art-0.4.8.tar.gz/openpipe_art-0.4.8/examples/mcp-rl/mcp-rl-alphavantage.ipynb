{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OpenPipe/ART/blob/art-mcp/examples/mcp-rl/mcp-rl-alphavantage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caZYLROd8xnV"
   },
   "source": [
    "To train a model for your custom task, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
    "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
    "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
    "\n",
    "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
    "\n",
    "</div>\n",
    "\n",
    "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
    "\n",
    "**MCPâ€¢RL: Tool-driven agent training**\n",
    "\n",
    "This notebook shows how to train a Qwen 2.5 7B model to automatically optimize against any MCP server. Simply define the server's tools and resources and the notebook below will:\n",
    "\n",
    "1. Query the server's tools and resources\n",
    "2. Generate diverse input examples for your task\n",
    "3. Train the model using RULER's automatic evaluation\n",
    "4. Test the trained model on new inputs against the server\n",
    "\n",
    "RULER learns what makes a good output purely from the MCP server's tools and resources - no expected outputs required!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OsrwCDQ5cviC"
   },
   "outputs": [],
   "source": [
    "# @title ðŸ’¿ Installation\n",
    "\n",
    "!uv pip install -q openpipe-art==0.3.11.post5 langchain-core tenacity \"mcp>=1.11.0\" \"gql<4\" fastmcp --prerelease allow --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8b8kgQ69ZDM"
   },
   "source": [
    "<a name=\"Configuration\"></a>\n",
    "\n",
    "### ðŸŽ¯ Configuration - Edit These Settings\n",
    "\n",
    "Add an OpenRouter key and customize your training by modifying the values below.\n",
    "\n",
    "By default your model will be trained to retrieve and analyze stock and crypto market data from the Alphavantage MCP server. To teach your model to use another MCP server, configure it to run in the [MCP server](#mcp) cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "so6r1_OG9en3"
   },
   "outputs": [],
   "source": [
    "# Required - Used for generating training inputs and RULER evaluation\n",
    "OPENROUTER_API_KEY = \"\"\n",
    "\n",
    "# Optional - Enables metric logging\n",
    "WANDB_API_KEY = \"\"\n",
    "\n",
    "# Shared key for the demo - DO NOT USE IN PRODUCTION, AND EXPECT RATE LIMITS\n",
    "ALPHAVANTAGE_API_KEY = \"HR32X84C3B4HJ93C\"\n",
    "\n",
    "# Choose the base model to train\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Options: \"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "I_AFDSOv_LrB"
   },
   "outputs": [],
   "source": [
    "# @title Advanced Settings\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"mcp-7b-alphavantage\"  # Name for your trained model\n",
    "PROJECT_NAME = \"mcp-rl\"  # Project name for tracking\n",
    "\n",
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    \"num_training_inputs\": 16,  # Number of training inputs to generate\n",
    "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
    "    \"num_epochs\": 3,  # Number of times through all data\n",
    "    \"rollouts_per_group\": 3,  # Different responses per input (for RULER comparison)\n",
    "    \"learning_rate\": 1e-5,  # Learning rate\n",
    "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
    "}\n",
    "\n",
    "MAX_TURNS = 10  # Maximum number of turns for the model to generate during one rollout\n",
    "\n",
    "NUM_TEST_INPUTS = 8  # Number of test inputs to generate\n",
    "RULER_MODEL = \"openrouter/openai/o4-mini\"  # Model for RULER evaluation\n",
    "INPUT_GENERATION_MODEL = \"openai/o4-mini\"\n",
    "\n",
    "# GPU configuration (for T4 â€”Â keep these as-is unless you have a reason to change them)\n",
    "MAX_SEQ_LENGTH = 4096  # Maximum sequence length\n",
    "GPU_MEMORY_UTILIZATION = 0.7  # GPU memory usage (0.0-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "gxUn4E_IPjq8"
   },
   "outputs": [],
   "source": [
    "# @title MCP server\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "import aiohttp\n",
    "from dotenv import load_dotenv\n",
    "from fastmcp import FastMCP\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Required for Alphavantage demo\n",
    "if ALPHAVANTAGE_API_KEY:\n",
    "    os.environ[\"ALPHAVANTAGE_API_KEY\"] = ALPHAVANTAGE_API_KEY\n",
    "else:\n",
    "    raise ValueError(\"ALPHAVANTAGE_API_KEY is required for the Alphavantage demo.\")\n",
    "\n",
    "\n",
    "class AlphaVantageClient:\n",
    "    \"\"\"Client for interacting with Alpha Vantage API\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "    async def fetch_data(self, function: str, **params) -> Dict[str, Any]:\n",
    "        \"\"\"Fetch data from Alpha Vantage API\"\"\"\n",
    "        query_params = {\n",
    "            \"function\": function,\n",
    "            \"apikey\": self.api_key,\n",
    "            \"datatype\": \"json\",\n",
    "            **params,\n",
    "        }\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(self.base_url, params=query_params) as response:\n",
    "                if response.status != 200:\n",
    "                    raise Exception(f\"API request failed: {response.status}\")\n",
    "\n",
    "                data = await response.json()\n",
    "\n",
    "                if \"Error Message\" in data:\n",
    "                    raise Exception(f\"Alpha Vantage API Error: {data['Error Message']}\")\n",
    "\n",
    "                if (\n",
    "                    \"Thank you for using Alpha Vantage! Please contact premium@alphavantage.co if you are targeting a higher API call volume.\"\n",
    "                    in data\n",
    "                ):\n",
    "                    raise Exception(\n",
    "                        \"Alpha Vantage API Error: Thank you for using Alpha Vantage! Please contact premium@alphavantage.co if you are targeting a higher API call volume.\"\n",
    "                    )\n",
    "\n",
    "                return data\n",
    "\n",
    "\n",
    "def _format_json(data: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format JSON data for display\"\"\"\n",
    "    import json\n",
    "\n",
    "    return json.dumps(data, indent=2)\n",
    "\n",
    "\n",
    "# Initialize FastMCP server\n",
    "mcp = FastMCP(\"mcp-alphavantage\")\n",
    "client = AlphaVantageClient(os.getenv(\"ALPHAVANTAGE_API_KEY\"))\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    retry=retry_if_exception_type(\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
    "    ),\n",
    ")\n",
    "async def get_stock_quote(symbol: str) -> str:\n",
    "    \"\"\"Get real-time stock quote for a symbol\n",
    "\n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., AAPL, MSFT)\n",
    "    \"\"\"\n",
    "    data = await client.fetch_data(\"GLOBAL_QUOTE\", symbol=symbol)\n",
    "    return f\"Stock Quote for {symbol}:\\n{_format_json(data)}\"\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    retry=retry_if_exception_type(\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
    "    ),\n",
    ")\n",
    "async def get_time_series_daily(symbol: str, outputsize: str = \"compact\") -> str:\n",
    "    \"\"\"Get daily time series data for a stock\n",
    "\n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., AAPL, MSFT)\n",
    "        outputsize: Output size: compact (latest 100 data points)\n",
    "    \"\"\"\n",
    "    data = await client.fetch_data(\n",
    "        \"TIME_SERIES_DAILY\",\n",
    "        symbol=symbol,\n",
    "        outputsize=outputsize,\n",
    "    )\n",
    "    return f\"Daily Time Series for {symbol}:\\n{_format_json(data)}\"\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    retry=retry_if_exception_type(\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
    "    ),\n",
    ")\n",
    "async def search_symbol(keywords: str) -> str:\n",
    "    \"\"\"Search for stock symbols by keywords\n",
    "\n",
    "    Args:\n",
    "        keywords: Keywords to search for (e.g., company name)\n",
    "    \"\"\"\n",
    "    data = await client.fetch_data(\"SYMBOL_SEARCH\", keywords=keywords)\n",
    "    return f\"Symbol Search Results for '{keywords}':\\n{_format_json(data)}\"\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    retry=retry_if_exception_type(\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
    "    ),\n",
    ")\n",
    "async def get_company_overview(symbol: str) -> str:\n",
    "    \"\"\"Get fundamental data and company overview\n",
    "\n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., AAPL, MSFT)\n",
    "    \"\"\"\n",
    "    data = await client.fetch_data(\"OVERVIEW\", symbol=symbol)\n",
    "    return f\"Company Overview for {symbol}:\\n{_format_json(data)}\"\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    retry=retry_if_exception_type(\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
    "    ),\n",
    ")\n",
    "async def get_sma(\n",
    "    symbol: str,\n",
    "    interval: str = \"daily\",\n",
    "    time_period: int = 30,\n",
    "    series_type: str = \"close\",\n",
    ") -> str:\n",
    "    \"\"\"Get Simple Moving Average (SMA) technical indicator\n",
    "\n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., AAPL, MSFT)\n",
    "        interval: Time interval (1min, 5min, 15min, 30min, 60min, daily, weekly, monthly)\n",
    "        time_period: Number of data points for SMA calculation\n",
    "        series_type: Price type to use for calculation (close, open, high, low)\n",
    "    \"\"\"\n",
    "    data = await client.fetch_data(\n",
    "        \"SMA\",\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        time_period=time_period,\n",
    "        series_type=series_type,\n",
    "    )\n",
    "    tech_analysis_key = \"Technical Analysis: SMA\"\n",
    "    # Alpha Vantage returns a dict keyed by timestamp; convert to list to slice\n",
    "    data[tech_analysis_key] = dict(list(data[tech_analysis_key].items())[:time_period])\n",
    "    return f\"SMA for {symbol}:\\n{_format_json(data)}\"\n",
    "\n",
    "\n",
    "@mcp.tool\n",
    "@retry(\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=30),\n",
    "    retry=retry_if_exception_type(\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError, Exception)\n",
    "    ),\n",
    ")\n",
    "async def get_rsi(\n",
    "    symbol: str,\n",
    "    interval: str = \"daily\",\n",
    "    time_period: int = 14,\n",
    "    series_type: str = \"close\",\n",
    ") -> str:\n",
    "    \"\"\"Get Relative Strength Index (RSI) technical indicator\n",
    "\n",
    "    Args:\n",
    "        symbol: Stock symbol (e.g., AAPL, MSFT)\n",
    "        interval: Time interval (daily, weekly, monthly)\n",
    "        time_period: Number of data points for RSI calculation\n",
    "        series_type: Price type to use for calculation (close, open, high, low)\n",
    "    \"\"\"\n",
    "    data = await client.fetch_data(\n",
    "        \"RSI\",\n",
    "        symbol=symbol,\n",
    "        interval=interval,\n",
    "        time_period=time_period,\n",
    "        series_type=series_type,\n",
    "    )\n",
    "    tech_analysis_key = \"Technical Analysis: RSI\"\n",
    "    # Alpha Vantage returns a dict keyed by timestamp; convert to list to slice\n",
    "    data[tech_analysis_key] = dict(list(data[tech_analysis_key].items())[:time_period])\n",
    "    return f\"RSI for {symbol}:\\n{_format_json(data)}\"\n",
    "\n",
    "\n",
    "# For in-memory usage, we don't need server_params anymore\n",
    "# The FastMCP server is now available as the 'mcp' variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_y2IwetPjq8"
   },
   "source": [
    "<a name=\"mcp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nEB1JGY6Pjq8"
   },
   "outputs": [],
   "source": [
    "# @title Let's generate our train and validation scenarios!\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from fastmcp import Client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Required\n",
    "if OPENROUTER_API_KEY:\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = OPENROUTER_API_KEY\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def generate_scenarios(\n",
    "    mcp_server: FastMCP,\n",
    "    num_scenarios: int = 24,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    # Connect to MCP server using in-memory transport\n",
    "    async with Client(mcp_server) as client:\n",
    "        # Get available tools\n",
    "        tools_result = await client.list_tools()\n",
    "        tools_info = []\n",
    "        for tool in tools_result:\n",
    "            tool_info = {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema,\n",
    "            }\n",
    "            tools_info.append(tool_info)\n",
    "\n",
    "        # Get available resources\n",
    "        try:\n",
    "            resources_result = await client.list_resources()\n",
    "            resources_info = []\n",
    "            for resource in resources_result.resources:\n",
    "                resource_info = {\n",
    "                    \"uri\": str(resource.uri),\n",
    "                    \"name\": resource.name,\n",
    "                    \"description\": resource.description,\n",
    "                    \"mimeType\": resource.mimeType,\n",
    "                }\n",
    "                resources_info.append(resource_info)\n",
    "        except Exception:\n",
    "            # Some servers might not have resources\n",
    "            resources_info = []\n",
    "\n",
    "    # Prepare the prompt for o3\n",
    "    tools_description = json.dumps(tools_info, indent=2)\n",
    "    resources_description = (\n",
    "        json.dumps(resources_info, indent=2)\n",
    "        if resources_info\n",
    "        else \"No resources available\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are an expert at creating realistic scenarios for testing AI agents that interact with MCP (Model Context Protocol) servers.\n",
    "\n",
    "Given the following available tools and resources from an MCP server, generate {num_scenarios} diverse, realistic scenarios that a user might want to accomplish using these tools.\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "{tools_description}\n",
    "\n",
    "AVAILABLE RESOURCES:\n",
    "{resources_description}\n",
    "\n",
    "Requirements for scenarios:\n",
    "1. Each scenario should be a task that can be accomplished using the available tools\n",
    "2. Scenarios should vary in complexity - some simple (1-2 tool calls), some complex (multiple tool calls)\n",
    "3. Scenarios should cover different use cases and tool combinations (though the task should not specify which tools to use)\n",
    "4. Each scenario should be realistic - something a real user might actually want to do\n",
    "5. Assign a difficulty rating from 1 (easy, single tool call) to 5 (hard, complex multi-step analysis)\n",
    "6. The task should always include generating a summary of the work done and a thorough analysis and report of the results\n",
    "\n",
    "You must respond with a JSON object containing a \"scenarios\" array of exactly {num_scenarios} objects. Each object must have:\n",
    "- \"task\": string describing the scenario\n",
    "- \"difficulty\": integer from 1-5 representing complexity\n",
    "\n",
    "Example:\n",
    "{{\n",
    "  \"scenarios\": [\n",
    "    {{\"task\": \"Get the current stock price for Apple (AAPL)\", \"difficulty\": 1}},\n",
    "    {{\"task\": \"Compare the 30-day SMA with current price for Tesla and determine if it's above or below the moving average and generate a thorough analysis and report\", \"difficulty\": 2}}\n",
    "  ]\n",
    "}}\"\"\"\n",
    "\n",
    "    # Call OpenAI's model with structured JSON output\n",
    "    client_openai = openai.OpenAI(\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    )\n",
    "\n",
    "    # Define the JSON schema for the response\n",
    "    response_schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"scenarios\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"task\": {\"type\": \"string\"},\n",
    "                        \"difficulty\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5},\n",
    "                    },\n",
    "                    \"required\": [\"task\", \"difficulty\"],\n",
    "                    \"additionalProperties\": False,\n",
    "                },\n",
    "                \"minItems\": num_scenarios,\n",
    "                \"maxItems\": num_scenarios,\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"scenarios\"],\n",
    "        \"additionalProperties\": False,\n",
    "    }\n",
    "\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=INPUT_GENERATION_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_completion_tokens=8000,\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\"name\": \"scenario_list\", \"schema\": response_schema},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Parse the JSON response\n",
    "    content = response.choices[0].message.content\n",
    "    try:\n",
    "        result = json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON response:\", e)\n",
    "        print(\"Response content:\", content)\n",
    "        raise e\n",
    "\n",
    "    # Extract scenarios from the response\n",
    "    if \"scenarios\" in result:\n",
    "        scenarios = result[\"scenarios\"]\n",
    "    else:\n",
    "        # If the response is just an array\n",
    "        scenarios = result if isinstance(result, list) else list(result.values())[0]\n",
    "\n",
    "    # Validate we got exactly the right number\n",
    "    if len(scenarios) != num_scenarios:\n",
    "        raise ValueError(f\"Expected {num_scenarios} scenarios, got {len(scenarios)}\")\n",
    "\n",
    "    return scenarios\n",
    "\n",
    "\n",
    "num_scenarios = TRAINING_CONFIG[\"num_training_inputs\"] + NUM_TEST_INPUTS\n",
    "for _ in range(10):\n",
    "    scenarios = await generate_scenarios(\n",
    "        mcp,  # Use the FastMCP server directly\n",
    "        num_scenarios=num_scenarios,\n",
    "    )\n",
    "\n",
    "    if len(scenarios) == num_scenarios:\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"\\nGenerated {len(scenarios)} scenarios:\")\n",
    "for i, scenario in enumerate(scenarios, 1):\n",
    "    print(f\"{i}. Task: {scenario['task']}\")\n",
    "    print(f\"   Difficulty: {scenario['difficulty']}/5\")\n",
    "\n",
    "# Shuffle scenarios randomly\n",
    "random.shuffle(scenarios)\n",
    "\n",
    "raw_train_scenarios = scenarios[: TRAINING_CONFIG[\"num_training_inputs\"]]\n",
    "raw_val_scenarios = scenarios[TRAINING_CONFIG[\"num_training_inputs\"] :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FET-_U0IPjq8"
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to train your model!\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import mcp.types as types\n",
    "import weave\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import art\n",
    "from art.local import LocalBackend\n",
    "from art.rewards import ruler_score_group\n",
    "from art.utils import iterate_dataset\n",
    "\n",
    "# Optional\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    weave.init(PROJECT_NAME)\n",
    "else:\n",
    "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Declare the model\n",
    "model = art.TrainableModel(\n",
    "    name=MODEL_NAME,\n",
    "    project=PROJECT_NAME,\n",
    "    base_model=BASE_MODEL,\n",
    ")\n",
    "\n",
    "# To run on a T4, we need to override some config defaults.\n",
    "model._internal_config = art.dev.InternalModelConfig(\n",
    "    init_args=art.dev.InitArgs(\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "    ),\n",
    "    engine_args=art.dev.EngineArgs(\n",
    "        enforce_eager=True,\n",
    "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Initialize the server\n",
    "backend = LocalBackend(\n",
    "    in_process=True,\n",
    "    path=\"./.art\",\n",
    ")\n",
    "\n",
    "# Register the model with the local Backend\n",
    "await model.register(backend)\n",
    "\n",
    "print(\"Model created!\")\n",
    "print(\"Base model:\", BASE_MODEL)\n",
    "print(\"Model name:\", MODEL_NAME)\n",
    "print(\"Project name:\", PROJECT_NAME)\n",
    "\n",
    "# =============== Rollout function code ===============\n",
    "\n",
    "\n",
    "def get_content_text(result) -> str:\n",
    "    # Extract text content from tool call result\n",
    "    if isinstance(result, str):\n",
    "        return result\n",
    "    elif hasattr(result, \"content\") and result.content:\n",
    "        if isinstance(result.content, list):\n",
    "            # Handle list of content items\n",
    "            content_text = \"\"\n",
    "            for item in result.content:\n",
    "                if isinstance(item, types.TextContent):\n",
    "                    content_text += item.text\n",
    "                else:\n",
    "                    content_text += str(item)\n",
    "        elif isinstance(result.content[0], types.TextContent):\n",
    "            content_text = result.content[0].text\n",
    "        else:\n",
    "            content_text = str(result.content)\n",
    "    else:\n",
    "        content_text = str(result)\n",
    "\n",
    "    return content_text\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class McpScenario:\n",
    "    \"\"\"A scenario for MCP agent evaluation.\"\"\"\n",
    "\n",
    "    task_description: str\n",
    "    mcp_server: FastMCP\n",
    "    max_turns: int = 10\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "async def rollout(\n",
    "    model: art.Model,\n",
    "    scenario: McpScenario,\n",
    "    debug: bool = False,\n",
    ") -> art.Trajectory:\n",
    "    \"\"\"Run an MCP agent rollout with FastMCP server.\n",
    "\n",
    "    Args:\n",
    "        model: The ART model to use for the agent\n",
    "        scenario: The MCP scenario to run (must include mcp_server)\n",
    "\n",
    "    Returns:\n",
    "        Trajectory containing the results of the rollout\n",
    "    \"\"\"\n",
    "    traj = art.Trajectory(\n",
    "        messages_and_choices=[],\n",
    "        reward=0,\n",
    "        metadata={\"task\": scenario.task_description},\n",
    "        metrics={\n",
    "            \"task_completed\": False,\n",
    "            \"success\": False,\n",
    "            \"ran_out_of_turns\": False,\n",
    "        },\n",
    "        scenario=scenario,\n",
    "    )\n",
    "\n",
    "    # Initialize system prompt\n",
    "    system_prompt = f\"\"\"You are an MCP (Model Context Protocol) agent.\\n\\nYou have access to MCP tools through the server. Use them to complete your task.\\n\\nWhen you believe you have completed the task, call the 'complete_task' function with a summary of what you accomplished. You have a total of {scenario.max_turns} turns to complete the task. Only use tool calls, do not write any content. After you have completed the task, call the 'complete_task' function with a summary of what you accomplished. Call complete_task by itself, not in conjunction with any other tools.\"\"\"\n",
    "\n",
    "    # Connect to FastMCP server using in-memory transport\n",
    "    try:\n",
    "        async with Client(scenario.mcp_server) as client:\n",
    "            # Get available tools from the server\n",
    "            tools_result = await client.list_tools()\n",
    "\n",
    "            # Convert to OpenAI format\n",
    "            tool_schemas = []\n",
    "            for tool in tools_result:\n",
    "                tool_schema = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description or f\"MCP tool: {tool.name}\",\n",
    "                        \"parameters\": tool.inputSchema\n",
    "                        or {\"type\": \"object\", \"properties\": {}},\n",
    "                    },\n",
    "                }\n",
    "                tool_schemas.append(tool_schema)\n",
    "\n",
    "            if debug:\n",
    "                available_tools = [tool[\"function\"][\"name\"] for tool in tool_schemas]\n",
    "                print(f\"Available MCP tools: {available_tools}\")\n",
    "\n",
    "            # Add completion tool schema\n",
    "            tool_schemas.append(\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": \"complete_task\",\n",
    "                        \"description\": \"Complete the task with a summary\",\n",
    "                        \"parameters\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"summary\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"Summary of accomplishments\",\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\"summary\"],\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            traj.tools = tool_schemas\n",
    "\n",
    "            # Initialize conversation\n",
    "            traj.messages_and_choices = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Please complete this task: {scenario.task_description}\",\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            if debug:\n",
    "                print(traj.messages())\n",
    "\n",
    "            num_turns = 0\n",
    "            task_completed = False\n",
    "\n",
    "            # Main interaction loop\n",
    "            while num_turns < scenario.max_turns and not task_completed:\n",
    "                num_turns += 1\n",
    "\n",
    "                try:\n",
    "                    # Get LLM response\n",
    "                    async with traj.track_duration(\"llm_completion\"):\n",
    "                        openai_client = AsyncOpenAI(\n",
    "                            api_key=model.inference_api_key,\n",
    "                            base_url=model.inference_base_url,\n",
    "                        )\n",
    "\n",
    "                        response = await openai_client.chat.completions.create(\n",
    "                            model=model.inference_model_name\n",
    "                            if model.inference_model_name\n",
    "                            else model.name,\n",
    "                            messages=traj.messages(),\n",
    "                            tools=tool_schemas,\n",
    "                            max_completion_tokens=8000,\n",
    "                        )\n",
    "\n",
    "                    choice = response.choices[0]\n",
    "\n",
    "                    if debug:\n",
    "                        print(f\"Choice: {choice.message}\")\n",
    "\n",
    "                    traj.messages_and_choices.append(choice)\n",
    "\n",
    "                    # Handle tool calls\n",
    "                    if choice.message.tool_calls:\n",
    "                        for tool_call in choice.message.tool_calls:\n",
    "                            try:\n",
    "                                tool_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                                if tool_call.function.name == \"complete_task\":\n",
    "                                    traj.metrics[\"task_completed\"] = True\n",
    "                                    traj.logs.append(\n",
    "                                        f\"Task completion attempted with summary: {tool_args['summary']}\"\n",
    "                                    )\n",
    "                                else:\n",
    "                                    # Call MCP tool through FastMCP client\n",
    "                                    result = await client.call_tool(\n",
    "                                        tool_call.function.name, tool_args\n",
    "                                    )\n",
    "\n",
    "                                    content_text = get_content_text(result)\n",
    "\n",
    "                                    if len(content_text) > 20000:\n",
    "                                        print(\n",
    "                                            f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
    "                                        )\n",
    "                                        print(f\"Args: {tool_args}\")\n",
    "                                        # print first and last 1000 characters\n",
    "                                        print(content_text[:1000])\n",
    "                                        print(content_text[-1000:])\n",
    "                                        raise Exception(\n",
    "                                            f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
    "                                        )\n",
    "\n",
    "                                    # Add tool response\n",
    "                                    traj.messages_and_choices.append(\n",
    "                                        {\n",
    "                                            \"role\": \"tool\",\n",
    "                                            \"tool_call_id\": tool_call.id,\n",
    "                                            \"content\": content_text,\n",
    "                                        }\n",
    "                                    )\n",
    "\n",
    "                                if debug:\n",
    "                                    print(f\"Tool call result: {content_text}\")\n",
    "\n",
    "                            except Exception as e:\n",
    "                                traj.logs.append(f\"Tool call error: {e}\")\n",
    "\n",
    "                                # Add error response\n",
    "                                traj.messages_and_choices.append(\n",
    "                                    {\n",
    "                                        \"role\": \"tool\",\n",
    "                                        \"tool_call_id\": tool_call.id,\n",
    "                                        \"content\": f\"Error: {str(e)}\",\n",
    "                                    }\n",
    "                                )\n",
    "                    else:\n",
    "                        # No tool calls, just continue conversation\n",
    "                        break\n",
    "\n",
    "                except Exception as e:\n",
    "                    traj.logs.append(f\"Error in turn {num_turns}: {e}\")\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        traj.logs.append(f\"MCP server error: {e}\")\n",
    "    if not task_completed and num_turns == scenario.max_turns:\n",
    "        traj.metrics[\"ran_out_of_turns\"] = True\n",
    "\n",
    "    traj.metrics[\"num_turns\"] = num_turns\n",
    "\n",
    "    if debug:\n",
    "        for message in traj.messages_and_choices:\n",
    "            print(\"\\n\")\n",
    "            print(message)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    return traj.finish()\n",
    "\n",
    "\n",
    "# =============== Training code ===============\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\n",
    "    f\"Using config: max_turns={MAX_TURNS}, rollouts_per_group={TRAINING_CONFIG['rollouts_per_group']}, groups_per_step={TRAINING_CONFIG['groups_per_step']}, num_epochs={TRAINING_CONFIG['num_epochs']}, learning_rate={TRAINING_CONFIG['learning_rate']}\"\n",
    ")\n",
    "\n",
    "await model.register(backend)\n",
    "\n",
    "train_scenarios = [\n",
    "    McpScenario(\n",
    "        task_description=scenario[\"task\"],\n",
    "        mcp_server=mcp,  # Use the FastMCP server directly\n",
    "        max_turns=MAX_TURNS,\n",
    "    )\n",
    "    for scenario in raw_train_scenarios\n",
    "]\n",
    "\n",
    "# Create dataset iterator using raw scenarios (not McpScenario objects)\n",
    "train_iterator = iterate_dataset(\n",
    "    train_scenarios,\n",
    "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
    "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
    "    initial_step=await model.get_step(),  # Resume from checkpoint\n",
    ")\n",
    "\n",
    "# Main training loop using iterate_dataset\n",
    "for batch in train_iterator:\n",
    "    print(\"Gathering trajectory groups with RULER scoring...\")\n",
    "\n",
    "    # Use gather_trajectory_groups with ruler_score_group\n",
    "    groups = await art.gather_trajectory_groups(\n",
    "        (\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(model, scenario, False)\n",
    "                for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
    "            )\n",
    "            for scenario in batch.items\n",
    "        ),\n",
    "        pbar_desc=f\"train gather step {batch.step}\",\n",
    "    )\n",
    "\n",
    "    scored_groups = []\n",
    "    for group in groups:\n",
    "        # Use RULER to assign relative scores to each trajectory\n",
    "        judged_group = await ruler_score_group(\n",
    "            group, judge_model=RULER_MODEL, debug=True, swallow_exceptions=True\n",
    "        )\n",
    "        scored_groups.append(judged_group)\n",
    "\n",
    "    print(\"starting train\")\n",
    "    await model.train(\n",
    "        scored_groups,\n",
    "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "YRO9ndqo5ky4"
   },
   "outputs": [],
   "source": [
    "# @title Test Your Model!\n",
    "\n",
    "# Generate test inputs\n",
    "print(\"Generating test inputs...\")\n",
    "val_scenarios = [\n",
    "    McpScenario(\n",
    "        task_description=scenario[\"task\"],\n",
    "        mcp_server=mcp,  # Use the FastMCP server directly\n",
    "        max_turns=MAX_TURNS,\n",
    "    )\n",
    "    for scenario in raw_val_scenarios\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ§ª Testing the trained model on {len(val_scenarios)} new inputs:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, scenario in enumerate(val_scenarios):\n",
    "    print(f\"\\nTest {i + 1}:\")\n",
    "    print(f\"Input: {scenario.task_description}\")\n",
    "\n",
    "    # Run the model\n",
    "    result_trajectory = await rollout(model, scenario)\n",
    "\n",
    "    # Extract the model's response\n",
    "    messages = result_trajectory.messages()\n",
    "    model_response = messages[-1][\"content\"] if messages else \"No response\"\n",
    "\n",
    "    print(f\"Model output: {model_response}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Testing completed!\")\n",
    "print(\n",
    "    f\"\\nYour model '{MODEL_NAME}' has been trained to effectively use the Alphavantage MCP server.\"\n",
    ")\n",
    "print(\"\\nTo use this model in production:\")\n",
    "print(\"1. The model checkpoint is saved in ./.art/\")\n",
    "print(\"2. You can load it using the vLLM library\")\n",
    "print(\n",
    "    \"3. Or continue training with more examples by adjusting the configuration at the top\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "utI-VYM8s5lo"
   },
   "outputs": [],
   "source": [
    "# @title Upload to Hugging Face ðŸ¤—\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "lora_model_path = (\n",
    "    f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",
    ")\n",
    "\n",
    "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=lora_model_path,\n",
    "    max_seq_length=16384,\n",
    "    dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "if False:  # Change to True to upload finetune\n",
    "    peft_model.push_to_hub_merged(f\"HF_ACCOUNT/{model.name}\", tokenizer, token=\"hf_...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuevYgXT-I1h"
   },
   "source": [
    "### Next Steps\n",
    "\n",
    "Congratulations! You've successfully trained a custom model for your task using only:\n",
    "- A pre-built MCP server\n",
    "- Example inputs (no outputs needed!)\n",
    "- RULER's automatic evaluation\n",
    "\n",
    "Here are some ways to improve results:\n",
    "\n",
    "1. **More diverse inputs**: Generate more varied input examples\n",
    "2. **Longer training**: Increase the number of training steps\n",
    "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
    "4. **MCP server refinement**: Add better tools and resources to the server\n",
    "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
    "\n",
    "Remember: RULER learns what \"good\" means from your MCP server alone - no labeled data required!\n",
    "\n",
    "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
