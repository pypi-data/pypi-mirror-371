{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this agent, click _Runtime_ and press _Run all_. Make sure you've enabled a free Tesla T4 GPU!\n",
    "\n",
    "<div class=\"align-center\">\n",
    "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
    "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
    "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
    "\n",
    "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [GitHub](https://github.com/openpipe/art).\n",
    "\n",
    "</div>\n",
    "\n",
    "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
    "\n",
    "This notebook shows how to train a Qwen 2.5 3B model to play tic tac toe. It will demonstrate how to set up a multi-turn agent, how to train it, and how to evaluate it.\n",
    "\n",
    "Completions and metrics will be logged to Weights & Biases.\n",
    "\n",
    "You will learn how to construct an [agentic environment](#Environment), how to define a [rollout](#Rollout), and how to run a [training loop](#Loop).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install openpipe-art==0.4.7 vllm==0.9.2 \"gql<4\" --prerelease allow --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "Later on in the notebook, we'll be creating a model that can automatically logs metrics and chat completions to Weights & Biases. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optional\n",
    "WANDB_API_KEY = \"\"\n",
    "if WANDB_API_KEY:\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic Environment\n",
    "\n",
    "<a name=\"Environment\"></a>\n",
    "\n",
    "ART allows your agent to learn by interacting with its environment. In this example, we'll create an environment in which the agent can play tic tac toe.\n",
    "\n",
    "Feel free to read as much or as little of this section's code as you'd like. The important thing to understand is that we're defining the rules of this agent's environment. In many cases, this will already be defined by the task you're trying to solve, but if you need to define a custom environment, this is how you do it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Literal, TypedDict\n",
    "\n",
    "\n",
    "class TicTacToeGame(TypedDict):\n",
    "    board: list[list[str]]\n",
    "    agent_symbol: Literal[\"x\", \"o\"]\n",
    "    opponent_symbol: Literal[\"x\", \"o\"]\n",
    "\n",
    "\n",
    "def generate_game(board_length: int = 3) -> TicTacToeGame:\n",
    "    board = [[\"_\" for _ in range(board_length)] for _ in range(board_length)]\n",
    "    agent_symbol = random.choice([\"x\", \"o\"])\n",
    "    opponent_symbol = \"x\" if agent_symbol == \"o\" else \"o\"\n",
    "    return {\n",
    "        \"board\": board,\n",
    "        \"agent_symbol\": agent_symbol,\n",
    "        \"opponent_symbol\": opponent_symbol,\n",
    "    }\n",
    "\n",
    "\n",
    "def render_board(game: TicTacToeGame) -> str:\n",
    "    board = game[\"board\"]\n",
    "    board_length = len(board)\n",
    "    # print something like this:\n",
    "    #    1   2   3\n",
    "    # A  _ | x | x\n",
    "    # B  o | _ | _\n",
    "    # C  _ | o | _\n",
    "    # where _ is an empty cell\n",
    "\n",
    "    board_str = \"   \" + \"   \".join([str(i + 1) for i in range(board_length)]) + \"\\n\"\n",
    "    for i in range(board_length):\n",
    "        board_str += f\"{chr(65 + i)}  {board[i][0]} | {board[i][1]} | {board[i][2]}\\n\"\n",
    "    return board_str\n",
    "\n",
    "\n",
    "def get_opponent_move(game: TicTacToeGame) -> tuple[int, int]:\n",
    "    # get a random empty cell\n",
    "    empty_cells = [\n",
    "        (i, j) for i in range(3) for j in range(3) if game[\"board\"][i][j] == \"_\"\n",
    "    ]\n",
    "    return random.choice(empty_cells)\n",
    "\n",
    "\n",
    "def apply_agent_move(game: TicTacToeGame, move: str) -> None:\n",
    "    board_length = len(game[\"board\"])\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(move)\n",
    "        square = root.text\n",
    "    except Exception:\n",
    "        raise ValueError(\"Invalid xml\")\n",
    "\n",
    "    try:\n",
    "        row_index = ord(square[0]) - 65\n",
    "        col_index = int(square[1]) - 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise ValueError(\"Unable to parse square\")\n",
    "\n",
    "    if (\n",
    "        row_index < 0\n",
    "        or row_index >= board_length\n",
    "        or col_index < 0\n",
    "        or col_index >= board_length\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Invalid move, row or column out of bounds: {row_index}, {col_index}\"\n",
    "        )\n",
    "\n",
    "    # check if the move is valid\n",
    "    if game[\"board\"][row_index][col_index] != \"_\":\n",
    "        raise ValueError(\"Square already occupied\")\n",
    "\n",
    "    game[\"board\"][row_index][col_index] = game[\"agent_symbol\"]\n",
    "\n",
    "\n",
    "def check_winner(board: list[list[str]]) -> Literal[\"x\", \"o\", \"draw\", None]:\n",
    "    board_length = len(board)\n",
    "    # check rows\n",
    "    for row in board:\n",
    "        if row.count(row[0]) == board_length and row[0] != \"_\":\n",
    "            return row[0]\n",
    "    # check columns\n",
    "    for col in range(board_length):\n",
    "        if [board[row][col] for row in range(board_length)].count(\n",
    "            board[0][col]\n",
    "        ) == board_length and board[0][col] != \"_\":\n",
    "            return board[0][col]\n",
    "\n",
    "    # top right to bottom left\n",
    "    upward_diagonal = [board[i][board_length - i - 1] for i in range(board_length)]\n",
    "    if (\n",
    "        upward_diagonal.count(upward_diagonal[0]) == board_length\n",
    "        and upward_diagonal[0] != \"_\"\n",
    "    ):\n",
    "        return upward_diagonal[0]\n",
    "\n",
    "    # top left to bottom right\n",
    "    downward_diagonal = [board[i][i] for i in range(board_length)]\n",
    "    if (\n",
    "        downward_diagonal.count(downward_diagonal[0]) == board_length\n",
    "        and downward_diagonal[0] != \"_\"\n",
    "    ):\n",
    "        return downward_diagonal[0]\n",
    "\n",
    "    # check for draw\n",
    "    if all(cell != \"_\" for row in board for cell in row):\n",
    "        return \"draw\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "Now that we've defined the rules of our environment, we can create a model that will learn to play 2048. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import art\n",
    "from art.local import LocalBackend\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "backend = LocalBackend(path=\"./.art\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model\n",
    "\n",
    "Now that we've defined the rules of our environment, we can create a model that will learn to play tic tac toe. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model = art.TrainableModel(\n",
    "    name=\"001-script\",\n",
    "    project=\"tic-tac-toe\",\n",
    "    base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    ")\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Rollout\n",
    "\n",
    "<a name=\"Rollout\"></a>\n",
    "\n",
    "A rollout is a single episode of an agent performing its task. It generates one or more trajectories, which are lists of messages and choices.\n",
    "\n",
    "In this example, the rollout function generates a game of tic tac toe, and the agent plays it until the game is finished. It then returns a trajectory which contains all the `system` and `user` messages presented to the agent, as well as all the `choices` that the agent made.\n",
    "\n",
    "When the game is finished the `reward` for the agent's performance is calculated based on whether the agent won, lost, drew, or errored, which is then assigned to the trajectory.\n",
    "\n",
    "This rollout function will be called many times in parallel during each step of the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import openai\n",
    "import weave\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import art\n",
    "\n",
    "if os.getenv(\"WANDB_API_KEY\", \"\"):\n",
    "    print(\"initializing weave\")\n",
    "    weave.init(model.project, settings={\"print_call_link\": False})\n",
    "\n",
    "\n",
    "class TicTacToeScenario(BaseModel):\n",
    "    step: int\n",
    "\n",
    "\n",
    "@weave.op\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError,))\n",
    "async def rollout(model: art.Model, scenario: TicTacToeScenario) -> art.Trajectory:\n",
    "    game = generate_game()\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return your move as an XML object with a single property 'move', like so: <move>A1</move>. Optional moves are 'A1', 'B3', 'C2', etc. You are the {game['agent_symbol']} symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        metadata={\n",
    "            \"notebook-id\": \"tic-tac-toe\",\n",
    "            \"step\": scenario.step,\n",
    "        },\n",
    "        reward=0,\n",
    "    )\n",
    "\n",
    "    move_number = 0\n",
    "\n",
    "    if game[\"agent_symbol\"] == \"o\":\n",
    "        starting_opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
    "            \"opponent_symbol\"\n",
    "        ]\n",
    "\n",
    "    while check_winner(game[\"board\"]) is None:\n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        messages = trajectory.messages()\n",
    "\n",
    "        try:\n",
    "            client = AsyncOpenAI(\n",
    "                base_url=model.inference_base_url,\n",
    "                api_key=model.inference_api_key,\n",
    "            )\n",
    "\n",
    "            chat_completion = await client.chat.completions.create(\n",
    "                model=model.get_inference_name(),\n",
    "                messages=messages,\n",
    "                max_completion_tokens=128,\n",
    "            )\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "        except ValueError:\n",
    "            trajectory.reward = -1 + (math.log(move_number + 1) / math.log(100))\n",
    "            break\n",
    "\n",
    "        move_number += 1\n",
    "        if check_winner(game[\"board\"]) is not None:\n",
    "            break\n",
    "\n",
    "        opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
    "\n",
    "    winner = check_winner(game[\"board\"])\n",
    "\n",
    "    if winner == game[\"agent_symbol\"]:\n",
    "        trajectory.reward = 1\n",
    "        trajectory.metrics[\"win\"] = 1\n",
    "    elif winner == game[\"opponent_symbol\"]:\n",
    "        trajectory.reward = 0\n",
    "        trajectory.metrics[\"win\"] = 0\n",
    "    elif winner == \"draw\":\n",
    "        trajectory.reward = 0.5\n",
    "        trajectory.metrics[\"win\"] = 0.5\n",
    "\n",
    "    trajectory.metrics[\"num_moves\"] = move_number\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Loop\"></a>\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "The training loop is where the magic happens. For each of the 100 steps defined below, the rollout function will be called 200 times in parallel. This means that 200 games will be played at once. Each game will produce a trajectory, which will be used to update the model.\n",
    "\n",
    "The `gather` step will wait for all of the trajectories to be generated, then it will delete all but the most recent checkpoint and train the model on the new trajectories.\n",
    "\n",
    "Inference will be blocked until the training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 2\n",
    "ROLLOUTS_PER_STEP = 48\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "for i in range(await model.get_step(), TRAINING_STEPS):\n",
    "    train_groups = await art.gather_trajectory_groups(\n",
    "        (\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(model, TicTacToeScenario(step=i))\n",
    "                for _ in range(ROLLOUTS_PER_STEP)\n",
    "            )\n",
    "            for _ in range(1)\n",
    "        ),\n",
    "        pbar_desc=\"gather\",\n",
    "    )\n",
    "    await model.delete_checkpoints()\n",
    "    await model.train(train_groups, config=art.TrainConfig(learning_rate=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "\n",
    "Just like that, you've trained an agent to play tic tac toe! Now it's time to use your model outside of ART, in the wild! The easiest way to do that is to load it from disk, where it was saved after each training step, and either run inference on it locally or upload it to a central hub like HuggingFace.\n",
    "\n",
    "Check out the code below for small demo of the model you just trained playing tic tac toe!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# example: .art/tic-tac-toe/models/002/checkpoints/0003\u001b[39;00m\n\u001b[1;32m      5\u001b[0m lora_model_path \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.art/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;01mawait\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_step()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trained model found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# example: .art/tic-tac-toe/models/002/checkpoints/0003\n",
    "lora_model_path = (\n",
    "    f\".art/{model.project}/models/{model.name}/checkpoints/{await model.get_step():04d}\"\n",
    ")\n",
    "\n",
    "if Path(lora_model_path).exists():\n",
    "    import torch\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    print(f\"loading model from {lora_model_path}\\n\")\n",
    "\n",
    "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=lora_model_path,\n",
    "        max_seq_length=16384,\n",
    "        dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(peft_model)\n",
    "\n",
    "    game = generate_game()\n",
    "    move_number = 0\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return your move as an XML object with a single property 'move', like so: <move>A1</move>. Optional moves are 'A1', 'B3', 'C2', etc. You are the {game['agent_symbol']} symbol.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    if game[\"agent_symbol\"] == \"o\":\n",
    "        starting_opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
    "            \"opponent_symbol\"\n",
    "        ]\n",
    "\n",
    "    while check_winner(game[\"board\"]) is None:\n",
    "        rendered_board = render_board(game)\n",
    "        messages.append({\"role\": \"user\", \"content\": rendered_board})\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages, return_tensors=\"pt\", add_generation_prompt=True\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        content = \"\"\n",
    "\n",
    "        def get_completion() -> str:\n",
    "            with torch.no_grad():\n",
    "                outputs = peft_model.generate(\n",
    "                    input_ids=inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "                return tokenizer.decode(\n",
    "                    outputs[0][inputs.shape[1] :], skip_special_tokens=True\n",
    "                )\n",
    "\n",
    "        try:\n",
    "            content = get_completion()\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\", e)\n",
    "            raise e\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "            move_number += 1\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid move on move {move_number}: {content}\")\n",
    "            print(f\"Reason: {e}\")\n",
    "            continue\n",
    "\n",
    "        # print the board every move\n",
    "        print(f\"\\nmove {move_number}\")\n",
    "        print(f\"board:\\n{rendered_board}\")\n",
    "        print(f\"agent move: {content}\")\n",
    "        print(f\"updated board:\\n{render_board(game)}\")\n",
    "\n",
    "        if check_winner(game[\"board\"]) is not None:\n",
    "            break\n",
    "        move_number += 1\n",
    "\n",
    "        opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
    "\n",
    "    winner = check_winner(game[\"board\"])\n",
    "\n",
    "    print(f\"game finished in {move_number} moves\")\n",
    "\n",
    "    if winner == game[\"agent_symbol\"]:\n",
    "        print(\"game won! 💪\")\n",
    "    elif winner == game[\"opponent_symbol\"]:\n",
    "        print(\"game lost! 😢\")\n",
    "    elif winner == \"draw\":\n",
    "        print(\"draw! 🤷‍♂️\")\n",
    "\n",
    "    print(f\"final board:\\n\\n{render_board(game)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"align-center\">\n",
    "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
    "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
    "<a href=\"https://openpipe.ai/blog/art-e-mail-agent\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_E_pill.png\" height=\"50\"></a>\n",
    "\n",
    "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
