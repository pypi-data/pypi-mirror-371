{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Pegasus++ Files\n",
    "\n",
    "PegasusTools provides a set of classes that you can use to load Pegasus++ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading NBF Files\n",
    "\n",
    "Passing the path to an NBF file to `pt.PegasusNBFData` will read that file from disk and return a `PegasusNBFData` object which contains member variables with all the header data and a dictionary containing the actual field data from the simulation; this dictionary is indexed via the names of the fields in the NBF file.\n",
    "\n",
    "### Example\n",
    "\n",
    "Before we can load an NBF file we need to have one to load. Normally you would use the files that Pegasus++ output but to facilitate this example I will create one here. The file creation tools used here are intended for use in examples and testing only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pegasustools as pt\n",
    "\n",
    "# ===== SETUP CODE. DISREGARD =====\n",
    "sys.path.append(str(Path.cwd().parent.resolve() / \"tests\"))\n",
    "\n",
    "import test_loading_nbf\n",
    "\n",
    "root_path = Path.cwd() / \"example_data\"\n",
    "root_path.mkdir(exist_ok=True)\n",
    "nbf_file_path = root_path / \"example.nbf\"\n",
    "\n",
    "_ = test_loading_nbf.generate_random_nbf_file(nbf_file_path, seed=42, dims=3)\n",
    "# ===== END OF SETUP CODE ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an NBF file to play with we can load it. It will automatically display some metadata about the loaded NBF file via the PegasusTools logger, `pt.logger`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbf_data = pt.PegasusNBFData(nbf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is fully loaded now and all the data is in numpy arrays. You can access the metadata programmatically with via the following, read only, member variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{nbf_data.time              = }\")\n",
    "print(f\"{nbf_data.big_endian        = }\")\n",
    "print(f\"{nbf_data.num_meshblocks    = }\")\n",
    "print(f\"{nbf_data.list_of_variables = }\")\n",
    "print(f\"{nbf_data.mesh_params       = }\")\n",
    "print(f\"{nbf_data.meshblock_params  = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data that was loaded. Note that the keys are the same as the list of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in nbf_data.data:\n",
    "    print(\n",
    "        f\"key: '{key}',\"\n",
    "        f\"type: {type(nbf_data.data[key])},\"\n",
    "        f\"shape: {nbf_data.data[key].shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have a `PegasusNBFData` that contains all the information in the original NBF file but now in numpy arrays that are easy to manipulate and perform additional postprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Spectra Files\n",
    "\n",
    "Passing the path to an spectra file to `pt.load_file` will read that file from disk and return a `PegasusSpectralData` object which contains member variables with all the metadata and the spectra in the `data` member variables. The `PegasusSpectralData` also contains several methods for working with spectral data.\n",
    "\n",
    "### Example\n",
    "\n",
    "Before we can load a spectra file we need to have one to load. Normally you would use the files that Pegasus++ output but to facilitate this example I will create one here. The file creation tools used here are intended for use in examples and testing only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pegasustools as pt\n",
    "\n",
    "# ===== SETUP CODE. DISREGARD =====\n",
    "sys.path.append(str(Path.cwd().parent.resolve() / \"tests\"))\n",
    "\n",
    "import test_loading_spectra\n",
    "\n",
    "root_path = Path.cwd() / \"example_data\"\n",
    "root_path.mkdir(exist_ok=True)\n",
    "spectra_file_path = root_path / \"example.spec\"\n",
    "\n",
    "_ = test_loading_spectra.generate_random_spec_file(\n",
    "    spectra_file_path, num_meshblocks=10, seed=42\n",
    ")\n",
    "# ===== END OF SETUP CODE. ====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an spectra file to play with we can load it. There are two different versions of spectra files, a newer version that includes `n_prp`, `n_prl`, `max_w_prp`, and `max_w_prl` and an older version that doesn't. If you're reading the newer version then `n_prp`, `n_prl`, `max_w_prp`, and `max_w_prl` will be read from the file. If you're reading the older version then you can optionally pass the values of `n_prp`, `n_prl`, `max_w_prp`, and `max_w_prl` that are set in the peginput file, if not they will be set as the default values of 200, 400, 4.0, and 4.0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_data = pt.PegasusSpectralData(spectra_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is fully loaded now and all the data is in numpy arrays. Let's take a look at the header information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{spectra_data.time      = }\")\n",
    "print(f\"{spectra_data.n_prp     = }\")\n",
    "print(f\"{spectra_data.n_prl     = }\")\n",
    "print(f\"{spectra_data.max_w_prp = }\")\n",
    "print(f\"{spectra_data.max_w_prl = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data that was loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type: {type(spectra_data.data)}\\nshape: {spectra_data.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `x1min`, `x1max`, `x2min`, `x2max`, `x3min`, and `x3max` meshblock location data is also loaded and stored in the `PegasusSpectralData.meshblock_locations` member variable which is a Polars DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectra_data.meshblock_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have a `PegasusSpectralData` that contains all the information in the original spectra file but now in a numpy array that is easy to manipulate and perform additional postprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Track Files (`.track_mpiio_optimized` and `.track.dat` files)\n",
    "\n",
    "Reading the two track file formats is more complex than the reading the spectra or NBF files since we want to significantly change the format to facilitate efficient searching through the track data. The goal of the file reading tools here are to transform the binary or ASCII files into Parquet files, along with a few data transformations. Once this transformation is complete the resulting directory of Parquet files can be quickly searched and manipulated using any of the standard dataframe tools such as Pandas, Polars, PyArrow, PySpark, or Dask. Polars is recommended, that's what PegasusTools uses internally and is what I found to be the best combination of stability and performance, I refer you to the [Polars documentation](https://docs.pola.rs) for details on how to work with it and some additional notes on Polars are in the [Using Polars](#Using-Polars) section\n",
    "\n",
    "### Transformations\n",
    "\n",
    "PegasusTools performs three transformations to raw track data in addition to converting the data to Parquet format:\n",
    "\n",
    "1. Computes new particle IDs. The new IDs are globally unique and not just unique within the meshblock. They are computed using a simple hash function that combines the old particle ID, meshblock ID, and species (if applicable) to uniquely identify a single particle. Particle IDs are not guanteed to be sequential, though they often are.\n",
    "2. Computes the magnetic moment $\\mu$, stored in the column named `mu`\n",
    "3. Computes the absolute change in $\\mu$ from one timestep to the next for a given particle. This column is named `delta_mu_abs` and is defined as $\\Delta\\mu_{abs} = \\mid \\mu_{t} - \\mu_{t-1} \\mid$; the first time step where this operation is not defined is set to [`null`](https://docs.pola.rs/user-guide/expressions/missing-data/).\n",
    "\n",
    "### Suggested Workflow\n",
    "\n",
    "#### Data Prep\n",
    "\n",
    "1. Run the simulation\n",
    "2. Get the path to the directory containing the `.track.dat` or `.track_mpiio_optimized` files. This directory should be on your cluster's scratch file system.\n",
    "3. Once the simulation is complete you should use one of the slurm templates below submit a job that will convert the track files to Parquet files. Details for converting the different file types are provided in their relevant sections. The end result is a directory full of parquet files where each files has a range of particles in it.\n",
    "\n",
    "#### Analyzing the Data\n",
    "\n",
    "1. Load the Parquet files using your preferred dataframe API, using the streaming/lazy evaluation engine if the dataset is larger than memory.\n",
    "2. Perform the query/transformations that you need to do\n",
    "3. Write out the (hopefully) smaller dataset to a new Parquet file for future plotting or additional post processing so that you don't have to reprocess the entire dataset every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing `.track.dat` Files\n",
    "\n",
    "This slurm submission script can be used to convert the ASCII `.track.dat` files to parquet files. Everything inside of angled brackets (`<like this>`) should be replaces with appropriate values for your usage. \n",
    "\n",
    "Converting `.track.dat` files is pretty quick, observed times for a 290GB dataset are about 150s on Stellar and about 20 minutes on Della. You could reasonably run this on a visualization node if you want, but not a header node. If you choose to run this on a vis node make sure to set the environment variable `POLARS_MAX_THREADS` to 1, otherwise it will spawn `num_processes` times number of CPU cores Polars threads and become very slow for all users.\n",
    "\n",
    "The final parquet files contain many particles and have file names of the format `<input file prefix>_particles_<first particle id>_<last particle id>.parquet`. This should substantially reduce the number of files that the file system has to manage since each parquet file contains thousands of particles. The target/maximum size of the parquet files can be tuned with the `max_parquet_size` argument; it defaults to ~2GB. Reducing the target file size below about 500MB significantly reduces performance.\n",
    "\n",
    "\n",
    "#### `.track.dat` Files with Species\n",
    "\n",
    "If the track ASCII files have species information then the `species` column in the parquet files will contain the species ID with `0` corresponding to protons (i.e. `p` files) and the non-proton species index increased by one to make room for the protons. I.e. `m00` becomes `1`, `m01` becomes `2`, etc.\n",
    "\n",
    "If the track ASCII files *do not* have species information then every element in the `species` column is set to 0 since the particles must be protons.\n",
    "\n",
    "***\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#SBATCH --job-name=<job name>\n",
    "#SBATCH --nodes=1                      # Only works with a single node\n",
    "#SBATCH --ntasks=1                     # Only works with a single task\n",
    "#SBATCH --cpus-per-task=32             # Number of CPUs assigned to each task\n",
    "#SBATCH --time=0-00:30:00              # Walltime in dd-hh:mm:ss format\n",
    "#SBATCH --mem-per-cpu=4G               # CPU memory per cpu-core\n",
    "#SBATCH --mail-user=<your email here>  # Send notification email to this address\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "# First we need make sure Polars only uses a single thread since PegasusTools is\n",
    "# handling the parallelization\n",
    "import os\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"1\"\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import pegasustools as pt\n",
    "\n",
    "\n",
    "# Python's Multiprocessing sometimes requires that any code that calls it directly or\n",
    "# indirectly is in a function.\n",
    "def main():\n",
    "    # Get the number of processes to use\n",
    "    num_processes = int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n",
    "\n",
    "    # The path to the .track.dat files. They should be in system scratch\n",
    "    source_directory = pathlib.Path(\"</path/to/directory/with/.track.dat/files>\")\n",
    "\n",
    "    # The path to the directory to write the parquet files to. This should also be in system\n",
    "    # scratch. It will be created if it doesn't exist.\n",
    "    destination_directory = pathlib.Path(\"</path/to/directory/to/write/parquet/files>\")\n",
    "\n",
    "    # Perform the transformation\n",
    "    pt.collate_tracks_from_ascii(\n",
    "        num_processes,\n",
    "        source_directory,\n",
    "        destination_directory,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing `.track_mpiio_optimized` Files\n",
    "\n",
    "This slurm submission script can be used to convert the binary `.track_mpiio_optimized` files to parquet files. Everything inside of angled brackets (`<like this>`) should be replaces with appropriate values for your usage.\n",
    "\n",
    "On Stellar this scales well up to about 16 cores and takes about 2 hours to run on a 2TB dataset. It consumes quite a bit of memory and saturates the I/O bandwidth and so should only be run on a compute node.\n",
    "\n",
    "On Della this conversion exhibits some strange behaviour. For a 2TB dataset it takes about 3 hours, ~90GB of memory per core, and only scales up to about 10 cores. The actual memory usage is only about 15GB per core but Linux caches a lot of the source files and if you use less than ~90GB of memory per core it crashes. This is all true on the x86 nodes on Della that I've tested on. On ARM systems (namely the Grace CPU) it runs fine with only about 11GB of memory per CPU, scales well up to 50 cores, and runs in about 20 minutes. I'm not sure why this discrepancy is happening but it is something to keep in mind when running on other systems or when hardware gets changed out. The code does perform same basic correctness checks at the end and will notify you if they fail. If the checks fail you can try rerunning the final step with more memory by setting the `restart_collect` argument to `True`.\n",
    "\n",
    "The final parquet files contain many particles and have file formats of the format `<input file prefix>_particles_<first particle id>_<last particle id>.parquet`. During the run, and if a restart is required, you may see some parquet files with names that end in `_temp`. These are temporary files created to store intermediate steps and PegasusTools will clean them up and remove them at the end of a successful run.\n",
    "\n",
    "***\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "#SBATCH --job-name=<job name>\n",
    "#SBATCH --nodes=1                      # Only works with a single node\n",
    "#SBATCH --ntasks=1                     # Only works with a single task\n",
    "#SBATCH --cpus-per-task=16             # Number of CPUs assigned to each task\n",
    "#SBATCH --time=0-04:00:00              # Walltime in dd-hh:mm:ss format\n",
    "#SBATCH --mem-per-cpu=16G              # CPU memory per cpu-core\n",
    "#SBATCH --mail-user=<your email here>  # Send notification email to this address\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "# First we need make sure Polars only uses a single thread since PegasusTools is\n",
    "# handling the parallelization\n",
    "import os\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"2\"\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import pegasustools as pt\n",
    "\n",
    "\n",
    "# Python's Multiprocessing sometimes requires that any code that calls it directly or\n",
    "# indirectly is in a function.\n",
    "def main():\n",
    "    # Get the number of processes to use\n",
    "    num_processes = int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n",
    "\n",
    "    # The path to the .track.dat files. They should be in system scratch\n",
    "    source_directory = pathlib.Path(\"</path/to/directory/with/.track.dat/files>\")\n",
    "\n",
    "    # The path to the directory to write the parquet files to. This should also be in\n",
    "    # system scratch. It will be created if it doesn't exist.\n",
    "    destination_directory = pathlib.Path(\"</path/to/directory/to/write/parquet/files>\")\n",
    "\n",
    "    # Perform the transformation. If you need to restart the last step of the job then\n",
    "    # pass the `restart_collect=False` argument.\n",
    "    pt.collate_tracks_from_binary(\n",
    "        num_processes,\n",
    "        source_directory,\n",
    "        destination_directory,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polars\n",
    "\n",
    "When using Polars there are a few things to keep in mind:\n",
    "\n",
    "- Be sure to install the `polars-u64-idx` package not the regular `polars` package. The regular `polars` package uses 32 bit indices internally and that limits it to ~4.3 billion rows which is too few for dealing with Pegasus track data. The `polars-u64-idx` package uses 64 bit indices and as such can support ~18 quintillion rows, more than enough for this application.\n",
    "- As much as possible you should utilize the \n",
    "[lazy API](https://docs.pola.rs/user-guide/lazy/)\n",
    "(see \n",
    "[here](https://medium.com/background-thread/what-is-lazy-evaluation-programming-word-of-the-day-8a6f4410053f) \n",
    "or \n",
    "[here](https://en.wikipedia.org/wiki/Lazy_evaluation) \n",
    "for a general discussion of what lazy evaluation is). \n",
    "This enables you to work on datasets that are larger than the memory of the system you're currently using and lazily evaluated operations are potentially faster and more efficient than eagerly evaluated operations. The caveat to this is that Polars uses a streaming engine under the hood to perform lazy operations and at the time of writing (June 2025) they are rewriting their streaming engine. As such not all operations are currently supported in streaming mode. I believe that most operations that will be needed for track data are already supported and you can check what operations are currently supported on their \n",
    "[Tracking issue for the new streaming engine](https://github.com/pola-rs/polars/issues/20947).\n",
    "If there is an operation that you need that is not yet supported I recommend doing as much filtering/selecting/etc as you can lazily to reduce the size of the dataset so that it fits in memory, then doing whatever unsupported operations you need using the in-memory engine.\n",
    "- There are two functions in Polars for reading parquet files: `pl.read_parquet` and `pl.scan_parquet`. The `read` variant reads the parquet file(s) directly into memory as a Polars DataFrame. The `scan` variant reads the Parquet file(s) as a Polars LazyFrame that you can perform lazily evaluated or streaming operations on. There are also `pl.scan_*` and `pl.read_*` functions for other file formats.\n",
    "- Similarly to `pl.scan_parquet` and `pl.read_parquet` there are two different functions for writing Parquet files: `pl.LazyFrame.sink_parquet` and `pl.DataFrame.write_parquet`. These operate on LazyFrames and DataFrames respectively. `sink_parquet` also acts as an implicit `collect` operation to execute the lazily evaluated tasks.\n",
    "- `pl.scan_parquet`, `pl.read_parquet`, and the other `scan`/`read` functions in Polars accept a single file as an argument, an iterable of files, or a directory (in which case they will read all the files in that directory with the proper file type). When reading multiple files this way Polars treats them as a single large DataFrame/LazyFrame.\n",
    "\n",
    "I refer you to the [Polars User Guide](https://docs.pola.rs) and [Polars Python API Reference](https://docs.pola.rs/api/python/stable/reference/index.html) for details on general usage of Polars and to learn the basics. Below there are a couple of examples of potentially common operations you may need to perform on Pegasus++ Track data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "##### Compute $\\Delta\\mu_{abs}$\n",
    "\n",
    "This is already done when converting from the Pegasus++ output files to the Parquet files so you likely don't need to do this exact operation but it is instructive.\n",
    "\n",
    "***\n",
    "```python\n",
    "import pathlib\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Load the dataset lazily\n",
    "source_directory = pathlib.Path(\"</path/to/directory/with/parquet/files>\")\n",
    "dataset = pl.scan_parquet(source_directory)\n",
    "\n",
    "# Note that this assumes that the data is already sorted by particle ID and then time\n",
    "# Compute delta mu\n",
    "dataset = dataset.with_columns(  # with_columns creates a new column with the name \"delta_mu_abs\"\n",
    "    # pl.when(condition) computes the value in `then()` when condition is True and the\n",
    "    # value in `otherwise()` when condition is False.\n",
    "    delta_mu_abs=pl.when(pl.col(\"particle_id\") == pl.col(\"particle_id\").shift())\n",
    "    # `.shift()` shifts the indicated column up by 1. So a.shift()[i] = a[i-1]\n",
    "    .then((pl.col(\"mu\") - pl.col(\"mu\").shift()).abs())\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Write the dataframe with the new column to a new file. Note that since evaluation is\n",
    "# lazy no compution is done until we call `sink_parquet`\n",
    "destination_path = pathlib.Path(\"</path/to/output/parquet/file>\")\n",
    "dataset.sink_parquet(destination_path)\n",
    "```\n",
    "***\n",
    "\n",
    "##### Get All Particles with $\\Delta\\mu_{abs}$ Above Threshold \n",
    "\n",
    "***\n",
    "```python\n",
    "import pathlib\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Load the dataset lazily\n",
    "source_directory = pathlib.Path(\"</path/to/directory/with/parquet/files>\")\n",
    "dataset = pl.scan_parquet(source_directory)\n",
    "\n",
    "# First we need to figure out which particles have delta mu values above the threshold\n",
    "threshold = 0.02\n",
    "selected_particle_ids = (\n",
    "    # Filter selects the rows that match the condition\n",
    "    dataset.filter(pl.col(\"delta_mu_abs\") > threshold)\n",
    "    # Select selects the column(s) that match arguments.\n",
    "    .select(pl.col(\"particle_id\"))\n",
    "    # Unique gets the unique elements in the dataframe\n",
    "    .unique()\n",
    ").collect(engine=\"streaming\")  # Perform the computation using the streaming engine\n",
    "\n",
    "# Select returns a Dataframe but `is_in` requires an imploded Series so let's do that\n",
    "# conversion now\n",
    "selected_particle_ids = selected_particle_ids.to_series().implode()\n",
    "\n",
    "# Now we know which particles have delta mu values above the threshold we need to get\n",
    "# the full time series data for all those particles\n",
    "selected_particles = dataset.filter(pl.col(\"particle_id\").is_in(selected_particle_ids))\n",
    "\n",
    "# Write the dataframe with the selected particles\n",
    "destination_path = pathlib.Path(\"</path/to/output/parquet/file>\")\n",
    "selected_particles.sink_parquet(destination_path)\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Trace Files (`.trace_mpiio_optimized` files) \n",
    "\n",
    "Reading trace files is done identically to reading binary track files (i.e. `.track_mpiio_optimized` files) with three exceptions:\n",
    "\n",
    "1. PegasusTools only supports trace files with the newer headers that include column information. If you have an older dataset that does not include those headers you should add the headers manually or write a script to do it.\n",
    "2. Unlike the track data no transformations are done. The `block_id` is already a global ID since there's only one trace per meshblock and points in space don't have a $\\mu$ value to compute.\n",
    "3. ASCII trace files are not supported.\n",
    "\n",
    "To convert the binary trace files to parquet files use the same slurm script provided above for binary track files just replace the call to `pt.collate_tracks_from_binary` with `pt.collate_traces`, the rest of the slurm script remains unchanged, including the arguments to the collating function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading `.hst` Files\n",
    "\n",
    "`.hst` files can be read using the `load_hst_file` function which will return a Polars dataframe containing the data from the `.hst` file. This function automatically corrects for any overlap due to restarts by only accepting the newest/latest data.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the .hst file\n",
    "hst_path = Path.cwd() / \"example_data\" / \"example.hst\"\n",
    "\n",
    "# Load the data, `hst_data` is a Polars dataframe\n",
    "hst_data = pt.load_hst_file(hst_path)\n",
    "\n",
    "# Check the number of rows and column names\n",
    "print(f\"Number of Rows: {len(hst_data)}\")\n",
    "print(f\"Column Names: {hst_data.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bobs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
