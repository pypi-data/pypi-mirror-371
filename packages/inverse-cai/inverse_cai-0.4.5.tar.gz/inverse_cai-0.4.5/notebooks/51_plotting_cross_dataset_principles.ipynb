{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for principles across datasets\n",
    "\n",
    "import inverse_cai.app.loader\n",
    "import inverse_cai.app.metrics\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "\n",
    "# Load data from results directory\n",
    "RESULTS_PATH = pathlib.Path(\"../exp/outputs/2024-11-17_19-22-42_large_dataset/results\")\n",
    "votes_df = inverse_cai.app.loader.create_votes_df(RESULTS_PATH)\n",
    "\n",
    "# Compute metrics for all principles\n",
    "metrics = {\n",
    "    \"overall\": inverse_cai.app.metrics.compute_metrics(votes_df),\n",
    "    \"by_dataset\": {},\n",
    "}\n",
    "\n",
    "for dataset in votes_df[\"dataset\"].unique():\n",
    "    metrics[\"by_dataset\"][dataset] = inverse_cai.app.metrics.compute_metrics(votes_df[votes_df[\"dataset\"] == dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_OVERALL = True\n",
    "INCLUDE_BF = False\n",
    "\n",
    "# Create DataFrame with principle metrics\n",
    "datasets_names = votes_df[\"dataset\"].unique()\n",
    "dataset_size = {\n",
    "    dataset_name: len(votes_df[votes_df[\"dataset\"] == dataset_name][\"comparison_id\"].unique())\n",
    "    for dataset_name in datasets_names\n",
    "}\n",
    "data = []\n",
    "for principle in metrics[\"overall\"][\"principles\"]:\n",
    "    row = {\n",
    "        \"Principle\": principle.replace(\"Select the response that \", \"\").replace(\".\", \"\")[:50],\n",
    "        \"Overall Performance\": metrics[\"overall\"][\"metrics\"][\"perf\"][\"by_principle\"][principle],\n",
    "        \"Overall Accuracy\": metrics[\"overall\"][\"metrics\"][\"acc\"][\"by_principle\"][principle],\n",
    "        \"Overall Relevance\": metrics[\"overall\"][\"metrics\"][\"relevance\"][\"by_principle\"][principle],\n",
    "    }\n",
    "\n",
    "    # Add per-dataset metrics\n",
    "    for dataset in metrics[\"by_dataset\"].keys():\n",
    "        row.update({\n",
    "            #f\"{dataset} Perf\": metrics[\"by_dataset\"][dataset][\"metrics\"][\"perf\"][\"by_principle\"][principle],\n",
    "            f\"{dataset} Accuracy\": metrics[\"by_dataset\"][dataset][\"metrics\"][\"acc\"][\"by_principle\"][principle],\n",
    "            f\"{dataset} Relevance\": metrics[\"by_dataset\"][dataset][\"metrics\"][\"relevance\"][\"by_principle\"][principle],\n",
    "        })\n",
    "\n",
    "    data.append(row)\n",
    "\n",
    "principles_df = pd.DataFrame(data)\n",
    "\n",
    "# remove rows that have no relevance of above 0.05 for any dataset (use or to allow for multiple datasets)\n",
    "principles_df = principles_df[\n",
    "    (principles_df[\"alpacaeval Relevance\"] > 0.05) |\n",
    "    (principles_df[\"chatbotarena Relevance\"] > 0.05) |\n",
    "    (principles_df[\"prism Relevance\"] > 0.05)\n",
    "]\n",
    "\n",
    "# Sort by overall performance\n",
    "principles_df = principles_df.sort_values(\"Overall Performance\", ascending=False)\n",
    "\n",
    "# Format metrics as percentages with 2 decimal places\n",
    "metric_columns = [col for col in principles_df.columns if col != \"Principle\"]\n",
    "\n",
    "if EXCLUDE_OVERALL:\n",
    "    metric_columns = [col for col in metric_columns if \"Overall\" not in col]\n",
    "    overall_col_format = \"\"\n",
    "    # remove all columns that start with \"Overall\"\n",
    "    allowed_columns = [col for col in principles_df.columns if not col.startswith(\"Overall\")]\n",
    "    principles_df = principles_df[allowed_columns]\n",
    "\n",
    "else:\n",
    "    overall_col_format = \"rrr\"\n",
    "\n",
    "#Â replace above formatting with per row formatting\n",
    "for i, row in principles_df.iterrows():\n",
    "    for col in metric_columns:\n",
    "        if \"Overall\" in col:\n",
    "            principles_df.at[i, col] = f\"{row[col]*100:.1f}\"\n",
    "        else:\n",
    "            dataset_name, metric_type = col.split(\" \")\n",
    "            max_val = max([row[f\"{name} {metric_type}\"] for name in datasets_names])\n",
    "            relevance_val = row[f\"{dataset_name} Relevance\"]\n",
    "            size_dataset = dataset_size[dataset_name]\n",
    "            principles_df.at[i, col] = f\"\\\\textbf{{{row[col]*100:.1f}}}\" if row[col] == max_val and INCLUDE_BF else f\"{row[col]*100:.1f}\"\n",
    "\n",
    "            if relevance_val * size_dataset < 50:\n",
    "                # make text color grey\n",
    "                principles_df.at[i, col] = f\"\\\\textcolor{{lightgray}}{{{principles_df.at[i, col]}}}\"\n",
    "# Create Styler object for LaTeX\n",
    "styler = principles_df.style.set_caption(\"Performance of principles across datasets\").hide(axis=\"index\")\n",
    "\n",
    "\n",
    "\n",
    "# Generate LaTeX table\n",
    "latex_table = styler.to_latex(\n",
    "    column_format=\"l\" + overall_col_format + \"|rr\" * (len(metrics[\"by_dataset\"])),\n",
    "    caption=\"\\\\textbf{Reconstruction results of principles across three datasets}: \\\\emph{AlpacaEval} ($648$ preferences), \\\\emph{ChatbotArena} ($5,115$), and \\\\emph{PRISM} ($7,490$). Metrics shown are accuracy (\\\\emph{Acc}) and relevance (\\\\emph{Rel}) scores. All principles are generated by ICAI based on a separate training set of $1000$ preferences from \\\\emph{PRISM} and \\\\emph{ChatbotArena}. Sorted by overall performance, where performance combines accuracy and relevance scores. Greyed-out values indicate that the principle was relevant for less than $50$ preferences on the respective dataset.\",\n",
    "    label=\"tab:principles_performance\",\n",
    "    position=\"H\",\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    ")\n",
    "\n",
    "# Create two-row header\n",
    "datasets = list(metrics[\"by_dataset\"].keys())\n",
    "if not EXCLUDE_OVERALL:\n",
    "    datasets = [\"Overall\"] + datasets\n",
    "top_row = [\"\\\\multicolumn{1}{c}{\\\\textbf{Principle}} & \"]  # Empty cell for Principle column\n",
    "bottom_row = [\"\\\\multicolumn{1}{c}{\\emph{Select the response that...}} & \"]\n",
    "\n",
    "DATASET_NAMES = {\n",
    "    \"Overall\": \"Overall\",\n",
    "    \"alpacaeval\": \"AlpacaEv.\",\n",
    "    \"chatbotarena\": \"ChatbotAr.\",\n",
    "    \"prism\": \"PRISM\",\n",
    "}\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    num_cols = 3 if dataset == \"Overall\" else 2\n",
    "    top_row.append(f\"\\\\multicolumn{{{num_cols}}}{{c}}{{\\\\textbf{{{DATASET_NAMES[dataset]}}}}} & \")\n",
    "    if dataset == \"Overall\":\n",
    "        bottom_row.append(\"\\\\textbf{Perf} & \\\\textbf{Acc} & \\\\textbf{Rel} & \")\n",
    "    else:\n",
    "        bottom_row.append(\"\\\\textbf{Acc} & \\\\textbf{Rel} & \")\n",
    "\n",
    "# Remove trailing \" & \" from last entries\n",
    "top_row[-1] = top_row[-1].rstrip(\" & \")\n",
    "bottom_row[-1] = bottom_row[-1].rstrip(\" & \")\n",
    "\n",
    "# Combine rows\n",
    "new_header = (\n",
    "    \"\".join(top_row) + r\" \\\\\" + \"\\n\" +\n",
    "    \"\".join(bottom_row) + r\" \\\\\"\n",
    ")\n",
    "\n",
    "# Insert the new header and remove original header\n",
    "latex_table = latex_table.replace(r\"\\toprule\", r\"\\toprule\" + \"\\n\" + new_header)\n",
    "latex_table_lines = latex_table.split('\\n')\n",
    "latex_table = '\\n'.join([line for line in latex_table_lines if not line.startswith('Principle &')])\n",
    "\n",
    "\n",
    "# Save to file\n",
    "output_path = pathlib.Path(\"appendix/numerical_results/principles_performance.tex\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"Generated LaTeX table:\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
