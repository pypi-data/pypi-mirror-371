{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/raw/lmsys/chatbot_arena_kaggle2024_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we randomly sample datapoints in such a way, that for each of the selected models there is at least 100 samples winning or losing, or if there are less than 100 samples, we take all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all datapoints that where both model_a and model_b are in the list\n",
    "# [gpt-4-0314 (highest ranking gpt-4), gpt-4-1106-preview (highest ranking gpt-4 turbo), llama-2-70b-chat, llama-2-7b-chat, mixtral-8x7b-instruct-v0.1]\n",
    "\n",
    "model_list = ['gpt-4-0314', 'gpt-4-1106-preview', 'llama-2-70b-chat', 'llama-2-13b-chat', 'llama-2-7b-chat', 'mixtral-8x7b-instruct-v0.1']\n",
    "\n",
    "df = df[df['model_a'].isin(model_list) & df['model_b'].isin(model_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# remove ties\n",
    "df = df[df['winner_tie'] == 0]\n",
    "\n",
    "# get number of prompts and responses per model\n",
    "def convert_to_list(list_str):\n",
    "    try:\n",
    "        return ast.literal_eval(list_str)\n",
    "    except:\n",
    "        print(\"Error parsing:\", list_str)\n",
    "        return []\n",
    "\n",
    "for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    df[col] = df[col].apply(convert_to_list)\n",
    "    df[col + \"_length\"] = df[col].apply(len)\n",
    "    df[col] = df[col].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "\n",
    "# filter out row with either prompt/response_a/response_b being len more than 1\n",
    "df = df[(df[\"prompt_length\"] == 1) & (df[\"response_a_length\"] == 1) & (df[\"response_b_length\"] == 1)]\n",
    "\n",
    "# add winner_model and loser_model columns\n",
    "df['winner_model'] = df.apply(lambda x: x['model_a'] if x['winner_model_a'] == 1 else x['model_b'], axis=1)\n",
    "df['loser_model'] = df.apply(lambda x: x['model_a'] if x['winner_model_a'] == 0 else x['model_b'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if each col of prompt, response_a, response_b is encodable in utf-8\n",
    "# drop rows that are not encodable\n",
    "for col in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            row[col].encode(\"utf-8\")\n",
    "        except:\n",
    "            df.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"winner_model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loser_model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from the least frequent model, we select the first 100 wins and 100 losses\n",
    "\n",
    "# get winner list of models sorted by frequency\n",
    "models_winner = df[\"winner_model\"].value_counts().index.tolist()\n",
    "\n",
    "# get loser list of models sorted by frequency\n",
    "models_loser = df[\"loser_model\"].value_counts().index.tolist()\n",
    "\n",
    "num_samples = 10000\n",
    "\n",
    "def generate_dataframe_options(df, models_winner, models_loser):\n",
    "    df_final = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    models_winner = models_winner.copy()\n",
    "    models_loser = models_loser.copy()\n",
    "\n",
    "    for model in models_loser:\n",
    "        if len(df_final) > 0:\n",
    "            # get number of losses for this model already in the dataframe\n",
    "            n_losses = df_final[df_final[\"loser_model\"] == model].shape[0]\n",
    "        else:\n",
    "            n_losses = 0\n",
    "\n",
    "        # get df without the losses already in the final dataframe\n",
    "        df_model = df[df[\"loser_model\"] == model]\n",
    "\n",
    "        if n_losses > 0:\n",
    "            # make sure we only consider new datapoints\n",
    "            df_model = df_model[~df_model[\"id\"].isin(df_final[\"id\"])]\n",
    "\n",
    "        # shuffle df_model\n",
    "        df_model = df_model.sample(frac=1)\n",
    "\n",
    "        # get 100 - n_losses\n",
    "        df_model = df_model.head(max(0,100 - n_losses))\n",
    "\n",
    "        # print(f\"Adding {df_model.shape[0]} losses for {model}\")\n",
    "        df_final = pd.concat([df_final, df_model])\n",
    "    for model in models_winner:\n",
    "        # get number of wins for this model already in the dataframe\n",
    "        n_wins = df_final[df_final[\"winner_model\"] == model].shape[0]\n",
    "\n",
    "        # get df without the wins already in the final dataframe\n",
    "        df_model = df[df[\"winner_model\"] == model]\n",
    "        df_model = df_model[~df_model[\"id\"].isin(df_final[\"id\"])]\n",
    "\n",
    "        # shuffle df_model\n",
    "        df_model = df_model.sample(frac=1)\n",
    "\n",
    "        # get 100 - n_wins\n",
    "        df_model = df_model.head(max(0, 100 - n_wins))\n",
    "\n",
    "        # print(f\"Adding {df_model.shape[0]} wins for {model} (total prev {n_wins})\")\n",
    "        df_final = pd.concat([df_final, df_model])\n",
    "\n",
    "    # drop any duplicates\n",
    "    df_final = df_final.drop_duplicates()\n",
    "\n",
    "    return df_final\n",
    "\n",
    "df_final_options = []\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for i in tqdm.tqdm(range(num_samples)):\n",
    "    df_final_options.append(generate_dataframe_options(df, models_winner, models_loser))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframes by number of rows\n",
    "df_final_options = sorted(df_final_options, key=lambda x: x.shape[0])\n",
    "\n",
    "# get the first 10 dataframes\n",
    "df_final_options = df_final_options[:10]\n",
    "\n",
    "# print lengths\n",
    "for i, df_final in enumerate(df_final_options):\n",
    "    print(f\"Option {i}: {df_final.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final_options[0].copy()\n",
    "print(df_final[\"winner_model\"].value_counts())\n",
    "print(df_final[\"loser_model\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "#df_final = df_final.rename(columns={\"response_a\": \"text_a\", \"response_b\": \"text_b\"})\n",
    "df_final[\"text_a\"] = df_final[\"response_a\"]\n",
    "df_final[\"text_b\"] = df_final[\"response_b\"]\n",
    "df_final[\"preferred_text\"] = df_final.apply(lambda x: \"text_a\" if x[\"winner_model_a\"] == 1 else \"text_b\", axis=1)\n",
    "\n",
    "# format into correct text\n",
    "# prepend instruction column to both text_a and text_b\n",
    "for col in [\"text_a\", \"text_b\"]:\n",
    "    df_final[col] = \"Instruction:\\n\" + df_final[\"prompt\"] + \"\\n\\n\\nAssistant:\\n\" + df_final[col]\n",
    "\n",
    "# shuffle\n",
    "df_final: pd.DataFrame = df_final.sample(frac=1, random_state=42)\n",
    "\n",
    "df_final.to_csv(\"../data/processed/lmsys/chatbot_arena_kaggle2024_train_balanced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how long the prompt lists are and if they are the same for all datapoints\n",
    "\n",
    "df_final[\"prompt_length\"] = df_final[\"prompt\"].apply(lambda x: len(ast.literal_eval(x)))\n",
    "df_final[df_final[\"prompt_length\"] != 1].iloc[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = df[\"model_a\"].value_counts().to_dict()\n",
    "\n",
    "for key, value in val_counts.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[\"model_a\"].value_counts().to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./api_calls.jsonl\"\n",
    "\n",
    "import json\n",
    "\n",
    "# count number of total chars in entire jsonl file\n",
    "\n",
    "total_chars = 0\n",
    "line_dicts = []\n",
    "with open(path, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        total_chars += len(line)\n",
    "        if '\"type\":\"return_value\"' in line:\n",
    "            line_dict = ast.literal_eval(line)\n",
    "            line_dicts.append(line_dict)\n",
    "\n",
    "\n",
    "\n",
    "print((total_chars / 1000000) * 0.150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "\n",
    "for line_dict in line_dicts:\n",
    "    total_prompt_tokens += line_dict[\"message\"][\"token_usage\"][\"prompt_tokens\"]\n",
    "    total_completion_tokens += line_dict[\"message\"][\"token_usage\"][\"completion_tokens\"]\n",
    "\n",
    "\n",
    "total_cost = (total_completion_tokens * 0.6 +  total_prompt_tokens * 0.15) / 1000000\n",
    "\n",
    "print(f\"total cost: {total_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
