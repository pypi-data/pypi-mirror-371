Metadata-Version: 2.4
Name: LLMEvaluationFramework
Version: 0.0.11
Summary: End-to-End LLM Evaluation and Auto-Suggestion Framework
Home-page: https://github.com/isathish/LLMEvaluationFramework
Author: Sathishkumar Nagarajan
Author-email: mail@sathishkumarnagarajan.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python
Dynamic: summary

<h1 align="center">ğŸš€ LLMEvaluationFramework</h1>

<p align="center">
  <img src="https://img.shields.io/badge/LLM-Evaluation-blueviolet?style=for-the-badge&logo=python" alt="Framework Badge">
  <img src="https://img.shields.io/badge/Docs-Online-green?style=for-the-badge&logo=readthedocs" alt="Docs Badge">
</p>

<p align="center">
  <i>Advanced Python framework for evaluating, testing, and benchmarking Large Language Models (LLMs)</i><br>
  <a href="https://isathish.github.io/llmevaluationframework/"><b>ğŸ“š View Full Documentation</b></a>
</p>

<p align="center">
  <a href="https://pypi.org/project/llm-evaluation-framework/"><img src="https://img.shields.io/pypi/v/llm-evaluation-framework?color=blue&label=PyPI&logo=python" alt="PyPI"></a>
  <a href="https://github.com/isathish/LLMEvaluationFramework/actions"><img src="https://img.shields.io/github/actions/workflow/status/isathish/LLMEvaluationFramework/python-app.yml?label=CI&logo=github" alt="Build Status"></a>
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License"></a>
  <a href="https://github.com/isathish/LLMEvaluationFramework/stargazers"><img src="https://img.shields.io/github/stars/isathish/LLMEvaluationFramework?style=social" alt="Stars"></a>
</p>

---

## ğŸŒŸ At a Glance

> **LLMEvaluationFramework** is your **all-in-one** solution for **evaluating, testing, and benchmarking** Large Language Models (LLMs) with style and precision.

<div align="center">
  <img src="https://via.placeholder.com/900x400?text=Framework+Architecture" alt="Framework Architecture Diagram">
</div>

---
## ğŸ“– Overview

**LLMEvaluationFramework** is a **production-grade** toolkit for **evaluating, testing, and benchmarking LLMs**.  
It provides a modular architecture with model inference, automated suggestions, model registry management, and synthetic dataset generation â€” all in one package.

---

## âœ¨ Key Features

<div align="center">

| ğŸš€ Feature | ğŸ’¡ Description |
|------------|----------------|
| âš¡ **Model Inference Engine** | Evaluate prompts against multiple LLMs with ease |
| ğŸ’¡ **Auto Suggestion Engine** | Generate intelligent prompt suggestions |
| ğŸ“š **Model Registry** | Manage and register multiple LLM configurations |
| ğŸ§ª **Test Dataset Generator** | Create synthetic datasets for evaluation |
| ğŸ”Œ **Extensible** | Easily integrate with new models and datasets |
| âœ… **Testable** | Designed with 100% test coverage in mind |

</div>

| Feature | Description |
|---------|-------------|
| âš¡ **Model Inference Engine** | Evaluate prompts against multiple LLMs with ease |
| ğŸ’¡ **Auto Suggestion Engine** | Generate intelligent prompt suggestions |
| ğŸ“š **Model Registry** | Manage and register multiple LLM configurations |
| ğŸ§ª **Test Dataset Generator** | Create synthetic datasets for evaluation |
| ğŸ”Œ **Extensible** | Easily integrate with new models and datasets |
| âœ… **Testable** | Designed with 100% test coverage in mind |

### ğŸ†• Latest Additions
- ğŸš€ **Async Inference Engine** â€” Concurrent model evaluations for faster benchmarking.
- ğŸ“ **Custom Scoring Strategies** â€” Plug in your own evaluation metrics.
- ğŸ’¾ **Persistent Storage** â€” JSON/DB backends for saving configurations and results.
- ğŸ–¥ **CLI Support** â€” Manage models and run evaluations from the terminal.
- ğŸ“œ **Enhanced Logging** â€” Detailed logs for debugging and performance tracking.

---

## ğŸ“¦ Installation

> **Tip:** Use a virtual environment to keep dependencies isolated.

<div align="center">
  <img src="https://img.shields.io/badge/Install-PyPI-blue?style=for-the-badge&logo=pypi">
</div>

**From PyPI**
```bash
pip install llm-evaluation-framework
```

**From Source**
```bash
git clone https://github.com/isathish/LLMEvaluationFramework.git
cd LLMEvaluationFramework
pip install -e .[dev]
```

---

## ğŸš€ Quick Start

> **Pro Tip:** Explore the [Usage Guide](docs/usage.md) for advanced workflows.

---

<details>
<summary>ğŸ” Model Inference</summary>

```python
from llm_evaluation_framework import ModelInferenceEngine

engine = ModelInferenceEngine(model_name="gpt-4")
result = engine.evaluate("What is the capital of France?")
print(result)
```
</details>

<details>
<summary>ğŸ’¡ Auto Suggestions</summary>

```python
from llm_evaluation_framework import AutoSuggestionEngine

suggestion_engine = AutoSuggestionEngine(model_name="gpt-4")
suggestions = suggestion_engine.suggest("Write a poem about the ocean.")
print(suggestions)
```
</details>

<details>
<summary>ğŸ“š Model Registry</summary>

```python
from llm_evaluation_framework import ModelRegistry

ModelRegistry.register("gpt-4", {"provider": "OpenAI", "max_tokens": 4096})
print(ModelRegistry.list_models())
```
</details>

<details>
<summary>ğŸ§ª Test Dataset Generation</summary>

```python
from llm_evaluation_framework import TestDatasetGenerator

generator = TestDatasetGenerator()
dataset = generator.generate(num_samples=5, topic="math problems")
print(dataset)
```
</details>

---

## ğŸ“Š Workflow Overview

```mermaid
flowchart TD
    A[Input Prompt] --> B[Model Inference Engine]
    B --> C[Scoring Strategies]
    C --> D[Evaluation Results]
    D --> E[Persistent Storage]
```

---
## ğŸ“¸ Screenshots

<p align="center">
  <img src="https://via.placeholder.com/800x400?text=CLI+Demo" alt="CLI Demo">
</p>

<p align="center">
  <img src="https://via.placeholder.com/800x400?text=Evaluation+Results" alt="Evaluation Results">
</p>

---

## âš™ï¸ Advanced Configuration

> Extend the framework to suit your needs.

You can customize the framework by:
- Adding new model backends
- Defining custom scoring strategies
- Configuring persistent storage (JSON/DB)
- Extending CLI commands

Example:
```python
from llm_evaluation_framework.evaluation import CustomScoringStrategy

class MyScore(CustomScoringStrategy):
    def score(self, prediction, reference):
        return custom_logic(prediction, reference)
```

---

## ğŸ— Project Structure

<details>
<summary>ğŸ“‚ Click to Expand</summary>

```
</details>
LLMEvaluationFramework/
â”œâ”€â”€ llm_evaluation_framework/   # Core framework code
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ tests/                      # Unit tests
â”œâ”€â”€ setup.py                    # Installation script
â””â”€â”€ README.md                   # Project overview
```

---

## ğŸ“„ Documentation

> Full API reference available in [`docs/api-reference.md`](docs/api-reference.md)

Full documentation is available at **[ğŸ“š Online Documentation](https://isathish.github.io/llmevaluationframework/)**.

You can also explore the local [`docs/`](docs/) folder:

- ğŸ“˜ [Getting Started](docs/getting-started.md)
- ğŸ“– [Usage Guide](docs/usage.md)
- ğŸ¤ [Contributing Guide](docs/contributing.md)

---

## ğŸ¤ Contributing

We welcome contributions!  
Please read the [Contributing Guide](docs/contributing.md) for details.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

<p align="center">Made with â¤ï¸ for the LLM community</p>
