"""Extract text embedding features."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/80_embeddings.ipynb.

# %% ../nbs/80_embeddings.ipynb 3
from __future__ import annotations
from sklearn.base import BaseEstimator, TransformerMixin
from .store import TextFeatureStore
from model2vec import StaticModel
import numpy as np
from fastcore.basics import patch
from . import __version__

# %% auto 0
__all__ = ['Model2VecEmbedder']

# %% ../nbs/80_embeddings.ipynb 4
class Model2VecEmbedder(BaseEstimator, TransformerMixin):
	""" Sci-kit Learn pipeline component to extract embeddings using Model2Vec. """
	def __init__(self, 
			  feature_store:TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component
			  model_name:str = 'minishlab/potion-base-8M', # the model name to use
			  batch_size:int = 5000 # batch size for encoding text
			  ):
		
		self.feature_store = feature_store
		self.model_name = model_name
		self.model_ = StaticModel.from_pretrained(self.model_name)
		self.batch_size = batch_size

		self._embedder = f'Model2VecEmbedder(model_name={self.model_name}) Textplumber v{__version__}'


# %% ../nbs/80_embeddings.ipynb 6
@patch
def fit(self:Model2VecEmbedder, X, y=None):
	""" Fit is implemented, but does nothing. """
	return self

# %% ../nbs/80_embeddings.ipynb 7
@patch
def transform(self:Model2VecEmbedder, X):
	""" Generate embeddings for the texts using Model2Vec. 
	If the embeddings are already in the feature store, they are used instead of 
	recomputing them. Processing is done in batches to avoid memory issues. """

	if self.feature_store.get_config('embedder') != self._embedder:
		embeddings = [None] * len(X) # embedder changed, force recompute of embeddings
	else:
		embeddings = self.feature_store.get_embeddings_from_texts(X)
	if any(x is None for x in embeddings):
		embeddings = []
		for i in range(0, len(X), self.batch_size):
			X_batch = X[i:i+self.batch_size]
			embeddings_batch = self.model_.encode(X_batch)
			embeddings_batch = np.array(embeddings_batch, dtype=np.double) # returning as floats seemed to be causing issues with kmeans pipeline component
			embeddings.append(embeddings_batch)
		embeddings = np.concatenate(embeddings, axis=0)
		self.feature_store.update_embeddings(X, embeddings)
		self.feature_store.set_config('embedder', self._embedder)
	else:
		# all the embeddings are already in the feature store so no need to reprocess
		pass
	return embeddings



# %% ../nbs/80_embeddings.ipynb 8
@patch
def get_feature_names_out(self:Model2VecEmbedder, input_features=None):
	""" Get the feature names out from the model. """
	return [f'emb_{i}' for i in range(self.model_.dim)]
