"""Functions to help understand text data and evaluate text classification models."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/93_report.ipynb.

# %% ../nbs/93_report.ipynb 4
from __future__ import annotations
import pandas as pd
from sklearn.metrics import confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import textwrap
from sklearn.pipeline import Pipeline
from supertree import SuperTree
import textwrap
from sklearn.utils.validation import check_is_fitted
from sklearn.exceptions import NotFittedError
from IPython.display import HTML, display
import io
import base64
import numpy as np
from scipy.sparse import issparse
from datasets import ClassLabel, Dataset, DatasetDict
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from IPython.display import display, HTML
from sklearn.pipeline import Pipeline, FeatureUnion


# %% auto 0
__all__ = ['preview_dataset', 'cast_column_to_label', 'get_label_names', 'preview_label_counts', 'preview_split_by_label_column',
           'preview_text_field', 'preview_row_text', 'preview_splits', 'plt_svg', 'plot_confusion_matrix',
           'save_results', 'plot_confusion_matrices', 'get_classifier_feature_names_in',
           'plot_logistic_regression_features_from_pipeline', 'plot_decision_tree_from_pipeline',
           'get_selected_feature_names', 'preview_selected_features', 'preview_pipeline_features']

# %% ../nbs/93_report.ipynb 6
def _get_preview_css():
    """ Internal function used to get CSS used for previewing datasets and pipeline features. """
    return """
	<style>
	details {
		background:#f0f0f0;
		color:#000;
		border-radius: 0.5em; 
		padding: 0;
		margin-bottom:1em;
		border: 1px solid #ccc;
		width: auto;
        font-size: 0.9em;
        overflow-wrap: break-word;
	}
			
	details summary {
		cursor: pointer;
		font-weight: bold;
		font-size: 1em;
		padding: 1em;
		color: #666;
	}

	details.warnings {
		background-color: #f8d7da;
	}
			
	details.notices {
		background-color: #d4edda;
	}

	details[open] summary {
		color: black;
		margin-bottom: 1em;
	}
	details ul, details p {
		margin-left: 1em;
	}
	details pre {
		background-color: #f0f0f0;
		overflow: auto;
	}
	details h4, .featureunion > h4 {
		margin: 0;
		margin-left: 1em;
		padding: 0;
		font-size: 1em;
	}

	.featureunion > h4 {
	margin: 1em;
	}

	details {
	width: 100%;
	}

	.featureunion {
	width:100%;
	border: 1px dashed #ccc;
	border-radius: 0.5em; 
	margin-bottom:1em;
	}

	.featureunion-column {
	float: left;
	margin: 0.5%;
	}

	.featureunion::after {
	content: '';
	display: table;
	clear: both;
	}		
	</style>"""

# %% ../nbs/93_report.ipynb 7
def preview_dataset(dataset:Dataset|DatasetDict # A Huggingface dataset or dataset dict, typically the result of load_dataset()
					):
	""" Output information about a Huggingface dataset. """

	if isinstance(dataset, Dataset):
		split_name = dataset.split
		dataset = DatasetDict({split_name: dataset})

	css = _get_preview_css()
	display(HTML(css))

	collate_fields = {}
	for split in dataset.keys():
		split_summary = f"Split: {split} ({len(dataset[split])} samples)"
		fields_list = ', '.join(dataset[split].features.keys())
		split_detail = f"<p>Available fields: {fields_list}</p>"
		split_detail += '<ul>'
		for feature in dataset[split].features:
			if feature not in collate_fields:
				collate_fields[feature] = {'unique_counts': [], 'types': [], 'proportion_of_total_samples_unique': [], 'unique_values': []}
			unique_count = len(set(dataset[split][feature]))
			split_detail += f"<li>Field '{feature}' has {unique_count} unique values<pre>{dataset[split].features[feature]}</pre>"
			if unique_count/len(dataset[split]) < 0.1:
				unique_values = list(set(dataset[split][feature]))
				unique_values.sort()
				collate_fields[feature]['unique_values'].append(tuple(unique_values))
			#split_detail += f"<p>Unique Values: {unique_count}</p></li>"
			collate_fields[feature]['unique_counts'].append(unique_count)
			collate_fields[feature]['proportion_of_total_samples_unique'].append(unique_count/len(dataset[split]))
			collate_fields[feature]['types'].append(dataset[split].features[feature])
		split_detail += '</ul>'
	
		display(HTML(f"""
		<details>
		<summary>{split_summary}</summary>
		{split_detail}
		</details>"""))

	notices = []
	has_warning = False
	for field in collate_fields:
		# if all ClassLabel
		if all([str(collate_fields[field]['types'][i]).startswith('ClassLabel') for i in range(len(collate_fields[field]['types']))]):
			notice = f"Field '{field}' is a label column (ClassLabel).\n"
			notices.append(notice)
		# if all unique_counts are the same then probably a ClassLabel
		if len(set(collate_fields[field]['unique_counts'])) == 1 and collate_fields[field]['proportion_of_total_samples_unique'][0] < 0.1 and not str(collate_fields[field]['types'][0]).startswith('ClassLabel'):
			has_warning = True
			notice = f"Field '{field}' appears to be a label column and should probably be cast as ClassLabel with cast_column_to_label(dataset, '{field}').\n"
			notice += '<ul>'
			notice += f"<li>Unique counts are identical ({collate_fields[field]['unique_counts'][0]})</li>\n"
			notice += f"<li>Unique counts are a low proportion of total rows.</li>\n"
			# if all unique_values are the same then probably a ClassLabel
			if len(set(collate_fields[field]['unique_values'])) == 1:
				if len(dataset.keys()) > 1:
					notice += f"<li>Unique values are identical between splits: {collate_fields[field]['unique_values'][0]}</li>\n"
				else:
					notice += f"<li>Unique values: {collate_fields[field]['unique_values'][0]}</li>\n"
			notice += '</ul>'
			notices.append(notice)
		# if type contains dtype='string' and proportion > 0.8 then probably a text column
		if "dtype='string'" in str(collate_fields[field]['types'][0]) and collate_fields[field]['proportion_of_total_samples_unique'][0] > 0.8:
			notices.append(f"Field '{field}' appears to be a text column.\n")
	
	if len(notices) > 0:
		if has_warning:
			summary = "Warnings/Notices"
			detail_class = "warnings"
		else:
			summary = "Notices"
			detail_class = "notices"
		detail = '<ul>'
		for notice in notices:
			detail += '<li>' + notice.strip() + '</li>'
		detail += '</ul>'
		
		display(HTML(f"""
		<details class="{detail_class}">
		<summary>{summary}</summary>
		{detail}
		</details>"""))


# %% ../nbs/93_report.ipynb 13
def cast_column_to_label(dataset:DatasetDict, # A Huggingface dataset dict, typically the result of load_dataset()
						 label_column:str # The name of the column to cast to ClassLabel
						 ) -> DatasetDict:
	""" Cast a column to a ClassLabel. """

	first_split = list(dataset.keys())[0]
	if isinstance(dataset[first_split].features[label_column], ClassLabel):
		print(f"Column '{label_column}' is already a ClassLabel.")
		return dataset
	else:
		class_feature = ClassLabel(names=dataset[first_split].unique(label_column))
		for split in dataset.keys():
			dataset[split] = dataset[split].cast_column(label_column, class_feature)	
		return dataset

# %% ../nbs/93_report.ipynb 15
def get_label_names(dataset:DatasetDict|Dataset, # A Huggingface dataset or dataset dict, typically the result of load_dataset()
					label_column:str # The name of the column get the label names from
					) -> list[str]: # list of label names
	""" Get label names from field in a Huggingface dataset. """
	# this handles the case where a split is passed
	if not isinstance(dataset, DatasetDict):
		return dataset.features[label_column].names
	else:
		first_split = list(dataset.keys())[0]
		if dataset[first_split].features[label_column]._type == 'ClassLabel':
			return dataset[first_split].features[label_column].names
		else:
			raise ValueError(f"Field '{label_column}' is not a ClassLabel. Cast it with cast_column_to_label(dataset, '{label_column}') first.")

# %% ../nbs/93_report.ipynb 22
def preview_label_counts(df, label_column, label_names):
	""" Preview label counts from a dataframe (this will be made an internal function in a future version - use preview_split_by_label_column instead). """
	summary = pd.DataFrame(df.groupby([label_column])[label_column].count())
	summary.columns = ['count']
	summary.insert(0, 'label_name', summary.index)
	summary['label_name'] = summary['label_name'].apply(lambda x: label_names[x])
	display(summary)

# %% ../nbs/93_report.ipynb 23
def preview_split_by_label_column(dataset:DatasetDict, # A Huggingface dataset dataset dict, typically the result of load_dataset()
							label_column:str # The name of the column to preview
							):
	""" Output label counts per split for a Huggingface dataset. """
	label_names = get_label_names(dataset, label_column)

	dfs = {}
	for split in dataset.keys():
		dfs[split] = dataset[split].to_pandas()
		dfs[split].insert(1, 'label_name', dfs[split][label_column].apply(lambda x: dataset[split].features[label_column].int2str(x)))
		preview_label_counts(dfs[split], label_column, label_names)

# %% ../nbs/93_report.ipynb 26
def preview_text_field(text: str, # Text to preview
					   width: int = 80 # Width to wrap the text to
					   ):
	""" Display a text field, wrapping the text to 80 characters. This may be moved to an internal function in a future version. """
	for line in text.split("\r\n"):
		print(textwrap.fill(line, width=width))

# %% ../nbs/93_report.ipynb 29
def preview_row_text(df: pd.DataFrame, # DataFrame containing the data
					 selected_index: int, # Index of the row to preview 
					 text_column: str = 'text', # column name for text field
					 limit: int = -1 # Limit the length of the text field
					 ):
	""" Output the text fields of a row in the DataFrame """

	if selected_index not in df.index:
		print(f"Index {selected_index} not in DataFrame")
		return

	summary = df.loc[selected_index].to_frame().drop(text_column)
	summary.columns = ['Value']
	summary.index.name = 'Attribute'
	display(summary)

	print(f"{text_column}:")
	text = df[text_column].loc[selected_index]
	if limit > 1:
		if len(text) > limit:
			text = text[:limit] + "..."
	preview_text_field(text)

# %% ../nbs/93_report.ipynb 34
def preview_splits(X_train, 
				   y_train, 
				   X_test, 
				   y_test, 
				   label_names = None, 
				   target_classes = None, 
				   target_names = None):
	""" Display the number of samples in each class for train and test sets. """
	if label_names is not None:
		raise DeprecationWarning("label_names is deprecated and will be removed in the near future, use target_classes and target_names instead")

	class_name_lookup = {}
	for idx, class_identifier in enumerate(target_classes):
		class_name_lookup[class_identifier] = target_names[idx]

	print(f"Train: {len(X_train)} samples, {len(set(y_train))} classes")
	train_label_counts = pd.DataFrame(y_train).value_counts().to_frame()
	# insert label_names column in position 0 based on class_name_lookup
	train_label_counts.insert(0, 'label_name', train_label_counts.index.map(lambda x: class_name_lookup[x[0]]))

	display(train_label_counts)
	print(f"Test: {len(X_test)} samples, {len(set(y_test))} classes")
	test_label_counts = pd.DataFrame(y_test).value_counts().to_frame()
	test_label_counts.insert(0, 'label_name', test_label_counts.index.map(lambda x: class_name_lookup[x[0]]))

	display(test_label_counts)

# %% ../nbs/93_report.ipynb 40
def plt_svg(fig:plt.Figure = None # Optional figure to display, if None uses current figure
			): 
	""" Display an SVG in a notebook with save functionality (see note) """  
	plt.rcParams['svg.fonttype'] = 'none'  
	plt.rcParams['font.family'] = 'sans-serif'
	plt.rcParams['font.sans-serif'] = ['Tahoma'] + plt.rcParams['font.sans-serif']
	if fig is None:
		fig = plt.gcf()
	f = io.BytesIO()
	fig.savefig(f, format='svg', bbox_inches='tight')
	plt.close(fig)
	svg = f.getvalue()
	svg_url = 'data:image/svg+xml;base64,' + base64.b64encode(svg).decode()
	display(HTML(f'<img src="{svg_url}"></img>'))

# %% ../nbs/93_report.ipynb 43
def plot_confusion_matrix(y_test, 
						  y_predicted, 
						  target_classes, 
						  target_names,
						  figsize=(10, 8),
						  renderer:str='svg', # 'svg' or 'img'
						  title:str|None=None, # Title for the plot
						  ): 
	""" Output a confusion matrix with counts and proportions and appropriate labels. """ 
	# Compute confusion matrix
	cm = confusion_matrix(y_test, y_predicted, labels=target_classes)

	# Compute row and column totals
	row_totals = cm.sum(axis=1)  # Row totals
	col_totals = cm.sum(axis=0)  # Column totals
	overall_total = cm.sum()  # Overall total

	# Compute normalized proportions
	cm_normalized = cm / cm.sum(axis=1, keepdims=True)  # Normalize rows (proportions)

	# Combine counts and proportions into annotations
	annotations = np.empty_like(cm).astype(str)
	for i in range(cm.shape[0]):
		for j in range(cm.shape[1]):
			annotations[i, j] = f"{cm[i, j]}\n({cm_normalized[i, j]:.2f})"

	# Create updated axis labels with totals
	xticklabels_with_totals = [f"{label}\n(Total: {total})" for label, total in zip(target_names, col_totals)]
	yticklabels_with_totals = [f"{label} (Total: {total})" for label, total in zip(target_names, row_totals)]

	# Create heatmap without totals in the matrix.
	fig, ax = plt.subplots(figsize=figsize)
	sns.heatmap(cm,
				annot=annotations,
				fmt='',
				cmap='Blues',
				xticklabels=xticklabels_with_totals,
				yticklabels=yticklabels_with_totals,
				cbar=True)
	
	plt.xlabel('Predicted Labels')
	plt.ylabel('Actual Labels')
	if title is not None:
		plt.title(title)
	# add note to bottom of plot
	note = f"Note: cells show counts and row-wise proportions.\n"
	# add note to bottom right of plot right in corner
	plt.figtext(0.1, 0.005, note, horizontalalignment='left', fontsize=8)

	if renderer == 'svg':
		plt_svg(fig)
	else:
		plt.show()


# %% ../nbs/93_report.ipynb 46
def save_results(results_file, pipeline, experiment_descriptor, dataset_descriptor, y_test, y_pred, target_classes, target_names, classifier_step_name = 'classifier'):
	""" Save results from an experiment """
	
	params = pipeline.get_params()

	results = {
		'datetime': pd.Timestamp.now(),
		'experiment': experiment_descriptor,
		'dataset': dataset_descriptor,
		'classifier': pipeline.named_steps[classifier_step_name].__class__.__name__,
		'parameters': params,
		'accuracy_f1': accuracy_score(y_test, y_pred),
		'macro_precision': precision_score(y_test, y_pred, average='macro'),
		'macro_recall': recall_score(y_test, y_pred, average='macro'),
		'macro_f1': f1_score(y_test, y_pred, average='macro'),
		'weighted_precision': precision_score(y_test, y_pred, average='weighted'),
		'weighted_recall': recall_score(y_test, y_pred, average='weighted'),
		'weighted_f1': f1_score(y_test, y_pred, average='weighted'),
	}

	for i, label in enumerate(target_classes):
		# add precision, recall and f1 for each label
		label_name = target_names[i].lower().replace(' ', '_')
		results[f'{label_name}_precision'] = precision_score(y_test, y_pred, average=None, labels=[label])[0]
		results[f'{label_name}_recall'] = recall_score(y_test, y_pred, average=None, labels=[label])[0]
		results[f'{label_name}_f1'] = f1_score(y_test, y_pred, average=None, labels=[label])[0]

	# add dict as row to a new results_df dataframe - keys will be columns
	results_df = pd.DataFrame([results])

	# read if exists
	try:
		results_df_existing = pd.read_csv(results_file)
		results_df = pd.concat([results_df_existing, results_df], axis=0)
	except FileNotFoundError:
		pass

	# save the results
	results_df.to_csv(results_file, index=False)
	#print('Results saved to ', results_file)

	return results_df


# %% ../nbs/93_report.ipynb 49
def plot_confusion_matrices(
	tests,
	predictions,  
	target_classes,
	target_names,
	model_names=None,  # Optional: list of names for each prediction
	n_col=2,
	figsize=(16, 8),
	renderer: str = 'svg',  # 'svg' or 'img'
	title: str | None = None
):
	""" Plot grid of confusion matrices for multiple models. """
	if model_names is None:
		model_names = [f"Model {i+1}" for i in range(len(predictions))]
	predictions = {name: pred for name, pred in zip(model_names, predictions)}

	n_models = len(predictions)
	n_row = (n_models + n_col - 1) // n_col

	fig, axes = plt.subplots(n_row, n_col, figsize=figsize, squeeze=False)	
	axes = axes.flatten()

	for idx, (model_name, y_predicted) in enumerate(predictions.items()):
		y_test = tests[idx]
		cm = confusion_matrix(y_test, y_predicted, labels=target_classes)
		row_totals = cm.sum(axis=1)
		col_totals = cm.sum(axis=0)
		cm_normalized = cm / cm.sum(axis=1, keepdims=True)

		# Annotations: counts and proportions
		annotations = np.empty_like(cm).astype(str)
		for i in range(cm.shape[0]):
			for j in range(cm.shape[1]):
				annotations[i, j] = f"{cm[i, j]}\n({cm_normalized[i, j]:.2f})"

		xticklabels_with_totals = [f"{label}\n(Total: {total})" for label, total in zip(target_names, col_totals)]
		yticklabels_with_totals = [f"{label} (Total: {total})" for label, total in zip(target_names, row_totals)]

		ax = axes[idx]
		sns.heatmap(
			cm,
			annot=annotations,
			fmt='',
			cmap='Blues',
			xticklabels=xticklabels_with_totals,
			yticklabels=yticklabels_with_totals,
			cbar = False,
			#cbar=True if idx == 0 else False,
			ax=ax
		)
		ax.set_xlabel('Predicted Labels')
		ax.set_ylabel('Actual Labels')
		ax.set_title(model_name)
		
	# Hide any unused subplots
	for j in range(idx + 1, len(axes)):
		fig.delaxes(axes[j])

	if title is not None:
		plt.suptitle(title, fontsize=16)
	note = "Note: cells show counts and row-wise proportions."
	plt.figtext(0.1, 0.01, note, horizontalalignment='left', fontsize=8)

	#TODO - ADD KEY

	if renderer == 'svg':
		plt_svg(fig) 
	else:
		plt.show()

# %% ../nbs/93_report.ipynb 52
def get_classifier_feature_names_in(pipeline:Pipeline, # fitted pipeline
									classifier_step_name = 'classifier' # name of the classifier step in pipeline
									):
	""" Get the feature names that were the input to the classifier step from a fitted pipeline. """
	feature_names = None
	for i, step in enumerate(pipeline.named_steps):
		if hasattr(pipeline.named_steps[step], 'get_feature_names_out'):
			feature_names = pipeline.named_steps[step].get_feature_names_out(feature_names)
		if step == classifier_step_name:
			return feature_names

# %% ../nbs/93_report.ipynb 54
def plot_logistic_regression_features_from_pipeline(pipeline, target_classes, target_names, top_n=20, classifier_step_name='classifier', features_step_name='features', renderer = 'svg'):
	""" Plot the most discriminative features for a logistic regression classifier in a fitted pipeline. """
	# Get the classifier and feature names
	classifier = pipeline.named_steps[classifier_step_name]
	feature_names = get_classifier_feature_names_in(pipeline, classifier_step_name)
	class_name_lookup = {target_classes[i]: target_names[i] for i in range(len(target_classes))}
	# for binary classification
	if len(classifier.classes_) == 2:  # Binary classification
		log_odds = classifier.coef_[0]  # Single row for binary classification
		odds_ratio = np.exp(log_odds)  # Convert log odds to odds ratio

		feature_importance = pd.DataFrame({
			'Feature': feature_names,
			'Log Odds (Logit)': log_odds,
			'Odds Ratio': odds_ratio
		}).sort_values(by='Log Odds (Logit)', ascending=False)

		feature_importance['abs_log_odds'] = np.abs(feature_importance['Log Odds (Logit)'])
		feature_importance = feature_importance.sort_values(by='abs_log_odds', ascending=False).head(top_n)
		feature_importance = feature_importance.drop(columns=['abs_log_odds'])

		if len(feature_importance) < top_n:
			top_n = len(feature_importance)

		plt.figure(figsize=(10, 6))
		sns.barplot(x='Log Odds (Logit)', y='Feature', data=feature_importance.head(top_n))
		plt.title(f"Most Discriminative Features (Log Odds)")
		plt.xlabel("Log Odds (Logit)")
		plt.ylabel("Feature")

		plt.tight_layout()
		plt.subplots_adjust(bottom=0.12)
		plt.gcf().text(0.5, 0.02, f"Top {top_n} features ranked on Absolute Log Odds (Logit). Direction indicates positive/negative association with class '{class_name_lookup[classifier.classes_[1]]}'.", ha='center', fontsize=9)
		
		if renderer == 'svg':
			plt_svg(plt.gcf())
		else:
			plt.show()

		# Display the feature importance DataFrame
		display(feature_importance.head(top_n))

	else:  # Multi-class classification
		for class_idx, class_identifier in enumerate(classifier.classes_):
			if class_identifier not in target_classes:
				continue
			class_name = class_name_lookup[classifier.classes_[class_idx]]
			log_odds = classifier.coef_[class_idx]  # Coefficients for the current class
			odds_ratio = np.exp(log_odds)  # Convert log odds to odds ratio

			# Create a DataFrame for feature importance
			feature_importance = pd.DataFrame({
				'Feature': feature_names,
				'Log Odds (Logit)': log_odds,
				'Odds Ratio': odds_ratio
			}).sort_values(by='Log Odds (Logit)', ascending=False)

			feature_importance['abs_log_odds'] = np.abs(feature_importance['Log Odds (Logit)'])
			feature_importance = feature_importance.sort_values(by='abs_log_odds', ascending=False).head(top_n)
			feature_importance = feature_importance.drop(columns=['abs_log_odds'])

			if len(feature_importance) < top_n:
				top_n = len(feature_importance)

			plt.figure(figsize=(10, 6))
			sns.barplot(x='Log Odds (Logit)', y='Feature', data=feature_importance.head(top_n))
			plt.title(f"Class-Specific Predictors of '{class_name}' vs All Other Classes")
			plt.xlabel("Log Odds (Logit)")
			plt.ylabel("Feature")

			plt.tight_layout()
			plt.subplots_adjust(bottom=0.12)
			plt.gcf().text(0.5, 0.02, f"Top {top_n} features ranked on Absolute Log Odds (Logit). Direction indicates positive/negative association with class '{class_name}'.", ha='center', fontsize=9)
			if renderer == 'svg':
				plt_svg(plt.gcf())
			else:
				plt.show()

			# Display the feature importance DataFrame
			display(feature_importance.head(top_n))

# %% ../nbs/93_report.ipynb 61
def plot_decision_tree_from_pipeline(pipeline, # The pipeline containing the classifier
									X_train, # The training data
					   				y_train, # The training labels
					   				target_classes, # The target classes
									target_names, # The target names
									classifier_step_name = 'classifier', # The name of the classifier step in the pipeline
									features_step_name = 'features', # The name of the final preprocessing step = probably the name of the step prior to the classifier
					): # outputs a tree plot
	""" Plot the decision tree of the classifier from a pipeline using SuperTree """

	# supertree seems to assume that the target classes are 0, 1, 2, ... n - recoding ...
	if type(y_train) == np.ndarray:
		y_train = y_train.tolist()
	new_target_classes = list(range(0, len(target_classes)))
	y_train = [
		new_target_classes[target_classes.index(label)] if label in target_classes else label
		for label in y_train
	]

	preprocessor = Pipeline(pipeline.steps[:-1])
	X_train_preprocessed = preprocessor.fit_transform(X_train, y_train)

	if issparse(X_train_preprocessed):
		X_train_preprocessed = X_train_preprocessed.toarray()

	feature_names = preprocessor.named_steps[features_step_name].get_feature_names_out()
	super_tree = SuperTree(pipeline.named_steps[classifier_step_name], X_train_preprocessed, y_train, feature_names, target_names)
	super_tree.show_tree()

# %% ../nbs/93_report.ipynb 64
def get_selected_feature_names(pipeline, # the pipeline to get the feature names from
							   features_step_name = 'features', # the name of the step in the pipeline that contains the features 
							   selector_step_name = 'selector', # the name of the step in the pipeline that contains the selector
							   ) -> list: # returns a list of the selected feature names
	""" Get the selected features from the pipeline (Deprecated). """
	raise DeprecationWarning("get_selected_feature_names is deprecated, use preview_pipeline_features to inspect features through a pipeline")

	feature_names = pipeline.named_steps[features_step_name].get_feature_names_out()
	selected_feature_names = pipeline.named_steps[selector_step_name].get_feature_names_out(feature_names)
	return selected_feature_names

# %% ../nbs/93_report.ipynb 69
def preview_selected_features(pipeline, # the pipeline to preview the selected features from
							   features_step_name = 'features', # the name of the step in the pipeline that contains the features 
							   selector_step_name = 'selector', # the name of the step in the pipeline that contains the selector
							   ):
	""" Preview (i.e. prints) the selected features from the pipeline (Deprecated - this will be removed in 0.0.10). """
	raise DeprecationWarning("preview_selected_features is deprecated, use preview_pipeline_features to inspect features through a pipeline")
	selected_feature_names = get_selected_feature_names(pipeline, features_step_name, selector_step_name)
	if len(selected_feature_names) == 0:
		print("No features selected.")
	else:
		for feature in selected_feature_names:
			print(feature)

# %% ../nbs/93_report.ipynb 71
def _get_features_step(step:tuple, # step in the pipeline
			  feature_names:list, # feature names in
			  ) -> tuple: # markup, feature_names
	""" Get the markup to output features for a step in a pipeline (steps can be another Pipeline or FeatureUnion or a Sci-kit learn pipeline component). """
	if isinstance(step[1], Pipeline):
		markup, feature_names = _get_features_pipeline_markup(step[1], feature_names)
	elif isinstance(step[1], FeatureUnion):
		markup, feature_names = _get_features_featureunion_markup(step, feature_names)
	else:
		markup, feature_names = _get_features_step_markup(step, feature_names)
	return markup, feature_names

def _get_features_featureunion_markup(step:tuple, # step in the pipeline
									  feature_names:list, # feature names in
									  ) -> tuple: # markup, feature_names
	""" Get the markup to output features from a FeatureUnion step. """
	markup = ''
	markup += '<div class="featureunion">'
	markup += f'<h4>{step[0]} {step[1].__class__.__name__}</h4>'
	column_width = 100/round(len(step[1].transformer_list)) - 1.1
	column_feature_names = feature_names
	for sub_step in step[1].transformer_list:
		markup += f'<div class="featureunion-column" style="width: {column_width}%">'
		column_markup, column_feature_names = _get_features_step(sub_step, column_feature_names)
		markup += column_markup
		markup += '</div>'
	markup += '</div>'
	feature_names = step[1].get_feature_names_out(feature_names)
	return markup, feature_names

def _get_features_step_markup(step:tuple, # step in the pipeline
							  feature_names:list, # feature names in
							  ) -> tuple: # markup, feature_names
	""" Get the markup to output features for a pipeline component (i.e. transformer, estimator). """
	if hasattr(step[1], 'get_feature_names_out'):
		feature_names = step[1].get_feature_names_out(feature_names)
		details = f'<h4>Features Out ({len(feature_names)})</h4>'
		formatted_text = textwrap.fill(", ".join(feature_names), 100)
		details += f'<p>{formatted_text}</p>'
	elif hasattr(step[1], 'is_text_handler'):
		details = '<p>This step receives and returns text.</p>'
	elif hasattr(step[1], 'predict'):
		details = f'<p>This step is an estimator.</p>'
		details = f'<h4>Features In ({len(feature_names)})</h4>'
		formatted_text = textwrap.fill(", ".join(feature_names), 100)
		details += f'<p>{formatted_text}</p>'
	else:
		details = f'<p>This step does not output feature names.</p>'

	summary = f'{step[0]} {step[1].__class__.__name__}'
	markup = f"""
	<details>
	<summary>{summary}</summary>
	{details}
	</details>
	"""		

	return markup, feature_names

def _get_features_pipeline_markup(pipeline:Pipeline, # pipeline to get features from
								  feature_names:list, # feature names in
								  ) -> tuple: # markup, feature_names
	""" Get the markup to output features for each step in a pipeline. """

	if type(pipeline) != Pipeline:
		raise ValueError('pipeline must be a Pipeline object.')

	try:
		check_is_fitted(pipeline)
	except NotFittedError as e:
		raise NotFittedError('This pipeline is not fitted. Fit it before invoking preview_pipeline_features.')

	markup = ''
	for step in pipeline.steps:
		step_markup, feature_names = _get_features_step(step, feature_names)
		markup += step_markup

	return markup, feature_names

# %% ../nbs/93_report.ipynb 72
def preview_pipeline_features(pipeline:Pipeline, # pipeline to preview
							  ):
	""" Outputs the features at each step in a pipeline. """

	markup, _ = _get_features_pipeline_markup(pipeline, None)

	css = _get_preview_css()
	display(HTML(css))

	display(HTML(markup))

