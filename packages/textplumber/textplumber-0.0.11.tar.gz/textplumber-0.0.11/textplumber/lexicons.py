"""Extract features from texts based on lexicons."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/50_lexicons.ipynb.

# %% ../nbs/50_lexicons.ipynb 3
from __future__ import annotations
import os
from sklearn.base import BaseEstimator, TransformerMixin
from .store import TextFeatureStore
import numpy as np
from fastcore.basics import patch
import pandas as pd
from collections import Counter

# %% auto 0
__all__ = ['LexiconCountVectorizer', 'get_empath_lexicons', 'get_sentiment_lexicons']

# %% ../nbs/50_lexicons.ipynb 4
class LexiconCountVectorizer(BaseEstimator, TransformerMixin):
	""" A Sci-kit Learn pipeline component to get document-level counts for one or more lexicons. 
		This component should be used after the SpacyPreprocessor component with the same feature store. """ 
	def __init__(self,
			  	 feature_store:TextFeatureStore, # the feature store to use - this should be the same feature store used in the SpacyPreprocessor component
				 lexicons:dict, # the lexicons to use - a dictionary with the lexicon name as the key and the lexicon (a list of tokens to count) as the value
				 lowercase:bool = True, # whether to lowercase the tokens
				 ):

		self.feature_store = feature_store

		if not isinstance(lexicons, dict):
			raise ValueError("Lexicons should be a dictionary with lexicon name as the key and the lexicon (a list of tokens to count) as the value.")

		self.lexicons = lexicons
		self.lowercase = lowercase

# %% ../nbs/50_lexicons.ipynb 5
@patch
def fit(self:LexiconCountVectorizer, X, y=None):
	""" Fit the vectorizer to the tokens in the feature store. """
	return self

# %% ../nbs/50_lexicons.ipynb 6
@patch
def transform(self:LexiconCountVectorizer, X):
	""" Transform the texts to a matrix of counts. """
	docs_tokens = self.feature_store.get_tokens_from_texts(X) 
	docs_tokens = [[token.lower() for token in doc] if self.lowercase else doc for doc in docs_tokens]
	
	lexicon_sets = {k: set(v) for k, v in self.lexicons.items()}
	X_out = []
	for doc in docs_tokens:
		token_counts = Counter(doc)
		lexicon_counts = [sum(token_counts[token] for token in lexicon_sets[lexicon] if token in token_counts) for lexicon in self.lexicons]
		X_out.append(lexicon_counts)

	return np.array(X_out)



# %% ../nbs/50_lexicons.ipynb 7
@patch
def get_feature_names_out(self:LexiconCountVectorizer, input_features=None):
	""" Get the feature names out from the vectorizer. """
	return list(self.lexicons.keys())

# %% ../nbs/50_lexicons.ipynb 10
def get_empath_lexicons(save_to:str|None = 'lexicons_empath.txt' # where to save the file, None will not save
					) -> dict: # a dictionary with the name of each empath category as the key and the lexicon (the corresponding list of tokens to count) as the value
	""" Get the empath lexicons from the empath github repo. """

	empath_text = None

	if save_to is not None:
		if os.path.exists(save_to):
			with open(save_to, 'r', encoding='utf-8') as f:
				empath_text = f.read()
		else:
			save_path = os.path.dirname(save_to)
			if save_path != '' and not os.path.exists(save_path):
				os.makedirs(save_path)

	if not empath_text:
		import requests

		empath_lexicon = 'https://raw.githubusercontent.com/Ejhfast/empath-client/refs/heads/master/empath/data/categories.tsv'
		empath_text = requests.get(empath_lexicon).text.strip()

	if save_to is not None:
		with open(save_to, 'w', encoding='utf-8') as f:
			f.write(empath_text)

	empath_lexicons = {}
	lines = empath_text.split('\n')
	for line in lines:
		tokens = line.split()
		tokens = [token for token in tokens if token != '']
		if len(tokens) > 0:
			# first token is name and a candidate token
			empath_lexicons[tokens[0]] = tokens

	return empath_lexicons

# %% ../nbs/50_lexicons.ipynb 15
def get_sentiment_lexicons(save_to:str|None = 'lexicons_sentiment.txt' # where to save the file, None will not save
						   ): # a dictionary 'positive' and 'negative' as keys and corresponding lists of tokens to count as values
	
	vader_loaded = False
	if save_to is not None:
		if os.path.exists(save_to):
			vader = pd.read_csv(save_to, sep='\t', header=0)
			vader_loaded = True
		else:
			save_path = os.path.dirname(save_to)
			if save_path != '' and not os.path.exists(save_path):
				os.makedirs(save_path)

	if not vader_loaded:
		wordlists = ['https://raw.githubusercontent.com/verachell/English-word-lists-parts-of-speech-approximate/refs/heads/main/other-categories/mostly-adjectives.txt',
					'https://raw.githubusercontent.com/verachell/English-word-lists-parts-of-speech-approximate/refs/heads/main/verbs/mostly-verbs-infinitive.txt',
					'https://raw.githubusercontent.com/verachell/English-word-lists-parts-of-speech-approximate/refs/heads/main/verbs/mostly-verbs-past-tense.txt',
					'https://raw.githubusercontent.com/verachell/English-word-lists-parts-of-speech-approximate/refs/heads/main/verbs/mostly-verbs-present-tense.txt',
					'https://raw.githubusercontent.com/verachell/English-word-lists-parts-of-speech-approximate/refs/heads/main/verbs/transitive-past-tense.txt',
					'https://raw.githubusercontent.com/verachell/English-word-lists-parts-of-speech-approximate/refs/heads/main/verbs/transitive-present-tense.txt',
					]

		# read in the word lists for verbs and adjectives
		wordlist_df = pd.concat([pd.read_csv(wordlist, header=None, names=['token']) for wordlist in wordlists])
		wordlist_df = wordlist_df.drop_duplicates()

		vader_lexicon = 'https://raw.githubusercontent.com/cjhutto/vaderSentiment/refs/heads/master/vaderSentiment/vader_lexicon.txt'
		vader = pd.read_csv(vader_lexicon, sep='\t', header=None, names=['token', 'score', 'sd', 'ob'])

		# remove any rows where the token is not in the word lists
		vader = vader[vader['token'].isin(wordlist_df['token'])]

		# split values into separate columns
		vader['ob'] = vader['ob'].apply(lambda x: x[1:-1].split(','))
		vader = pd.concat([vader, vader['ob'].apply(pd.Series).add_prefix('ob')], axis=1)

		# calculate min and max
		ob_columns = ['ob0', 'ob1', 'ob2', 'ob3', 'ob4', 'ob5', 'ob6', 'ob7', 'ob8', 'ob9']
		vader[ob_columns] = vader[ob_columns].apply(pd.to_numeric)
		vader['ob_min'] = vader[ob_columns].min(axis=1)
		vader['ob_max'] = vader[ob_columns].max(axis=1)

		# drop ob and obx columns
		vader.drop(columns=['ob'] + ob_columns, inplace=True)

		# remove any rows where min is less than 0 and max is greater than 0 (i.e. where there isn't consensus that a word is positive or negative)
		outcasts = vader[(vader['ob_min'] < 0) & (vader['ob_max'] > 0)]
		vader = vader[~vader.index.isin(outcasts.index)]

		if save_to is not None:
			vader.to_csv(save_to, sep='\t', index=False)

	# building word lists of positive and negative words
	sentiment_lexicon_positive = vader[vader['score'] > 0]['token'].to_list()
	sentiment_lexicon_negative = vader[vader['score'] < 0]['token'].to_list()

	# sort them
	sentiment_lexicon_positive.sort()
	sentiment_lexicon_negative.sort()

	return {'positive': sentiment_lexicon_positive, 'negative': sentiment_lexicon_negative}

