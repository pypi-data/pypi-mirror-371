"""Sentiment estimator and feature extractor using VADER."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/55_vader.ipynb.

# %% ../nbs/55_vader.ipynb 5
from __future__ import annotations
from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin
from .store import TextFeatureStore
from .core import pass_tokens
from vaderSentiment.vaderSentiment import *
import numpy as np
from fastcore.basics import patch
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from IPython.display import HTML, display
from wordcloud import WordCloud
import pandas as pd
import matplotlib.pyplot as plt

# %% auto 0
__all__ = ['VaderSentimentExtractor', 'VaderSentimentEstimator', 'SentimentIntensityInterpreter', 'sentiment_wordcloud',
           'VaderSentimentProfileExtractor', 'VaderSentimentPOSNgramsExtractor']

# %% ../nbs/55_vader.ipynb 8
class VaderSentimentExtractor(BaseEstimator, TransformerMixin):
	""" Sci-kit Learn pipeline component to extract sentiment features using VADER. """
	def __init__(self, 
			  feature_store:TextFeatureStore = None, # (not implemented currently)
			  output:str = 'polarity', # 'polarity' (VADER's compound score), 'proportions' (ratios for proportions of text that are positive, neutral or negative), or 'allstats' (equivalent to 'polarity' + 'proportions'), 'labels' (positive, neutral, negative)
			  neutral_threshold:float = 0.05, # threshold for neutral sentiment
			):
		
		self.feature_store = feature_store
		if output not in ['polarity', 'proportions', 'allstats', 'labels']: # note: 'profileallstats' is experimental and not listed in the docs
			raise ValueError(f"output must be one of ['polarity', 'proportions', 'allstats', 'labels'], got {output}")
		self.output = output
		self.neutral_threshold = neutral_threshold

		self.analyzer_ = SentimentIntensityAnalyzer()

# %% ../nbs/55_vader.ipynb 10
@patch
def fit(self:VaderSentimentExtractor, X, y=None):
	""" Fit is implemented, but does nothing. """
	return self

# %% ../nbs/55_vader.ipynb 11
@patch
def convert_score_to_label(self:VaderSentimentExtractor, score: float, label_mapping = None) -> str:
	""" Convert VADER score to label. """
	if score >= self.neutral_threshold:
		label = 'positive'
	elif score <= self.neutral_threshold * -1:
		label = 'negative'
	else:
		label = 'neutral'
	if label_mapping is not None:
		label = label_mapping[label]
	return label

# %% ../nbs/55_vader.ipynb 12
@patch
def convert_scores_to_labels(self:VaderSentimentExtractor, scores: list[float], label_mapping = None):
	""" Convert VADER score to label. """
	for score in scores:
		yield self.convert_score_to_label(score)

# %% ../nbs/55_vader.ipynb 13
@patch
def transform(self:VaderSentimentExtractor, X):
	""" Extracts the sentiment from the text using VADER. """
	results = []
	for text in X:
		scores = self.analyzer_.polarity_scores(text)
		if self.output == 'proportions':
			results.append([scores['pos'], scores['neu'], scores['neg']])
		elif self.output == 'labels':
			compound = scores['compound']
			results.append(self.convert_score_to_label(compound))
		elif self.output == 'allstats':
			results.append([scores['pos'], scores['neu'], scores['neg'], scores['compound']])
		else: # default
			results.append([scores['compound']])
	return np.atleast_2d(results)  # Ensure the output is always a 2D array

# %% ../nbs/55_vader.ipynb 14
@patch
def get_feature_names_out(self:VaderSentimentExtractor, input_features=None):
	""" Get the feature names out from the model. """
	if self.output == 'proportions':
		return ['positive', 'neutral', 'negative']
	elif self.output == 'labels':
		return ['label']
	elif self.output == 'allstats':
		return ['positive', 'neutral', 'negative', 'compound']
	else: # default
		return ['polarity']


# %% ../nbs/55_vader.ipynb 16
class VaderSentimentEstimator(VaderSentimentExtractor, ClassifierMixin):
	""" Sci-kit Learn pipeline component to predict sentiment using VADER. """

	def __init__(self,
				 output:str = 'labels', # 'polarity' (VADER's compound score) or 'labels' (positive, neutral, negative)
				 neutral_threshold:float = 0.05, # threshold for neutral sentiment (see note for VaderSentimentExtractor)
				 label_mapping:dict|None = None, # (ignored if labels is None) mapping of labels to desired labels - keys should be 'positive', 'neutral', 'negative' and values should be desired labels
				 ):
		
		super().__init__()
		if output not in ['polarity', 'labels']:
			raise ValueError(f"output must be one of ['polarity', 'labels'], got {output}")
		self.output = output
		self.label_mapping = label_mapping
		self.neutral_threshold = neutral_threshold

# %% ../nbs/55_vader.ipynb 19
@patch
def predict(self:VaderSentimentEstimator, X):
	""" Predict the sentiment of texts using VADER. """
	y_predicted = self.transform(X).ravel()
	if self.output == 'labels' and self.label_mapping is not None:
		for i, prediction in enumerate(y_predicted):
			y_predicted[i] = self.label_mapping[prediction]
		dtype = type(list(self.label_mapping.values())[0])
	elif self.output == 'labels':
		dtype = str
	else:
		dtype = float
	return np.array(y_predicted, dtype=dtype)

# %% ../nbs/55_vader.ipynb 67
class SentimentIntensityInterpreter(SentimentIntensityAnalyzer):
	""" A class to aide interpretation of VADER scores. """
	def __init__(self, 
				 lexicon_file:str = 'vader_lexicon.txt', # path to a custom lexicon file
				 emoji_lexicon:str = 'emoji_utf8_lexicon.txt' # dictionary of emoji lexicon, if not provided, the default VADER emoji lexicon will be used
				 ):
		super().__init__(lexicon_file=lexicon_file, emoji_lexicon=emoji_lexicon)

# %% ../nbs/55_vader.ipynb 68
@patch
def polarity_scores(self:SentimentIntensityInterpreter, 
					text: str) -> tuple[dict, list]:

	"""
	A method based on the VADER polarity_scores method that collates the lexicon words 
	influencing the scoring of a text for improved interpretability. """

	# convert emojis to their textual descriptions
	text_no_emoji = ""
	prev_space = True
	for chr in text:
		if chr in self.emojis:
			# get the textual description
			description = self.emojis[chr]
			if not prev_space:
				text_no_emoji += ' '
			text_no_emoji += description
			prev_space = False
		else:
			text_no_emoji += chr
			prev_space = chr == ' '
	text = text_no_emoji.strip()

	sentitext = SentiText(text)

	sentiments = []
	# list to capture the features to help explain the scoring or to allow reporting on specific lexicon words
	features = []
	words_and_emoticons = sentitext.words_and_emoticons
	for i, item in enumerate(words_and_emoticons):
		valence = 0
		# check for vader_lexicon words that may be used as modifiers or negations
		if item.lower() in BOOSTER_DICT:
			sentiments.append(valence)
			continue
		if (i < len(words_and_emoticons) - 1 and item.lower() == "kind" and
				words_and_emoticons[i + 1].lower() == "of"):
			sentiments.append(valence)
			continue

		sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)
		
		# if non-zero valence then add word to the features list
		features.append([item,sentiments[-1]])

	sentiments = self._but_check(words_and_emoticons, sentiments)

	valence_dict = self.score_valence(sentiments, text)

	# output the valence dict and features
	return valence_dict, features



# %% ../nbs/55_vader.ipynb 69
@patch
def explain(self:SentimentIntensityInterpreter, 
			text: str):
	""" Prints a visual explanation of the text with each word's colour 
	gradients from red to green based on the word score, with indicators for 
	compound score and proportions of positive, negative and neutral. """
	valence_dict, features = self.polarity_scores(text)
	output_text = []
	start_pos = 0

	# any characters in self.emojis replace with the emoji + text description
	for emoji in self.emojis:
		text = text.replace(emoji, f'{emoji} <span class="ttppee">{self.emojis[emoji]}</span>')

	# iterate through the features and identify the start and end pos of each word in the text
	spans = []
	for word, score in features:
		# find first character position in text starting at start_pos
		next_word_pos = text.find(word, start_pos)
		if next_word_pos == -1:
			raise ValueError(f"Word '{word}' not found in text starting from position {start_pos}.")
		else:
			start_pos = next_word_pos + len(word)
			spans.append((next_word_pos, start_pos))

	for word, score in features:
		score_norm = max(-1, min(1, score / 4))
		if score_norm > 0:
			colour = f"rgb({int((1-score_norm)*255)},255,{int((1-score_norm)*255)})"
		elif score_norm < 0:
			colour = f"rgb(255,{int((1+score_norm)*255)},{int((1+score_norm)*255)})"
		else:
			colour = "rgb(255,255,255)"
		# get word from vader sentiment lexicon - words and emoticons
		if word.lower() in self.lexicon:
			lexicon_score = self.lexicon[word.lower()]
		elif word.lower() in self.emojis:
			lexicon_score = self.emojis[word.lower()]
		else:
			lexicon_score = 0

		score_sign = []
		score_comments = []
		if score == lexicon_score:
			pass
		elif np.sign(score) != np.sign(lexicon_score):
			score_sign.append("‚õî")   
			score_comments.append("negated")
		elif abs(score) > abs(lexicon_score):
			score_sign.append("‚¨ÜÔ∏è")
			score_comments.append("boosted")
		elif abs(score) < abs(lexicon_score):
			score_sign.append("‚¨áÔ∏è") 
			score_comments.append("dampened")
		score_sign = "".join(score_sign) if score_sign else ""
		score_comments = " ".join(score_comments) if score_comments else ""

		word_html = f"<span class='sent-word' style='background-color:{colour};' title='score: {score} {score_comments}'>{score_sign}{word}</span>"
		output_text.append(word_html)

	pos = valence_dict.get('pos', 0)
	neu = valence_dict.get('neu', 0)
	neg = valence_dict.get('neg', 0)
	compound = valence_dict.get('compound', 0)
	bar_max_height = 60

	pos_height = int(pos * bar_max_height)
	neu_height = int(neu * bar_max_height)
	neg_height = int(neg * bar_max_height)

	css = """
	<style>
	.sent-word {
		color: #000;
		padding: 2px;
		border-radius: 3px;
		font-size: 14px;
		margin-left: 0px;
		margin-bottom: 5px;
		white-space: nowrap;
		overflow-wrap: normal;
		word-break: normal;
		border: 1px solid #ccc;
		display: inline-block;
	}
	.sent-bars {
		margin-top:10px; margin-bottom:30px; background:#fff; border-radius:5px; width:100px; height:100px; display:inline-block; position:relative; vertical-align:top;border:1px solid #ccc;
	}
	.sent-bar {
		position:absolute; bottom:15px; width:24px; border:1px solid #bbb; border-radius:4px 4px 0 0; text-align:center; color:#000; font-size:15px;
	}
	.sent-bar-line {
		position:absolute; bottom:15px; left:5px; width:90px;margin:0 auto; height:1px; background:#999;
	}
	.sent-bar-pos { background:rgb(0,200,0);}
	.sent-bar-neu { background:rgb(255,255,255);}
	.sent-bar-neg { background:rgb(200,0,0);}
	.sent-bar span:first-child {
		position:absolute; bottom:-14px; left:0; width:100%; text-align:center; font-size:12px;
	}
	.sent-bars-title, .sent-compound-title {
		position:absolute; width:100%; font-weight:bold; top:-15px; left:0; font-size:12px;
	}
	.sent-compound {
		display:inline-block; vertical-align:top;border:1px solid #ccc;border-radius:5px;margin-top:10px; position:relative;
		width:100px; height:100px; font-size:14px; font-weight:bold; margin-bottom:30px;
	}
	.sent-compound span {
	color:#000; 
	display:inline-block;
	line-height:100px;
	width:100px;
	text-align:center; 
	}

	.text-wrapper {
		font-size: 14px;width:100%;max-width:800px;overflow-wrap:anywhere;line-height:1.5;
	}
	span.ttppee {font-style:italic;}

	span.icon {font-size:12px;}
	</style>
	"""

	proportions_indicator = f"""
	<svg class='sent-bars' width='100' height='100' style='vertical-align:top; margin-right:10px;'>
		<title>Proportions: pos={pos:.3f}, neu={neu:.3f}, neg={neg:.3f}</title>
		<rect width='24' height='{pos_height}' x='8' y='{100-pos_height-15}' fill='rgb(0,200,0)' rx='4' ry='4'/>
		<rect width='24' height='{neu_height}' x='38' y='{100-neu_height-15}' fill='rgb(255,255,255)' rx='4' ry='4' stroke='#bbb'/>
		<rect width='24' height='{neg_height}' x='68' y='{100-neg_height-15}' fill='rgb(200,0,0)' rx='4' ry='4'/>
		<line x1='5' y1='{100-15}' x2='95' y2='{100-15}' stroke='#999' stroke-width='1'/>
		<text x='20' y='95' text-anchor='middle' font-size='12' fill='#000'>pos</text>
		<text x='50' y='95' text-anchor='middle' font-size='12' fill='#000'>neu</text>
		<text x='80' y='95' text-anchor='middle' font-size='12' fill='#000'>neg</text>
		<text x='50' y='15' text-anchor='middle' font-size='12' font-weight='bold' fill='#000'>Proportions</text>
	</svg>
	"""

	compound_norm = max(-1, min(1, compound))
	if compound_norm > 0:
		compound_colour = f"rgb({int((1-compound_norm)*255)},255,{int((1-compound_norm)*255)})"
	elif compound_norm < 0:
		compound_colour = f"rgb(255,{int((1+compound_norm)*255)},{int((1+compound_norm)*255)})"
	else:
		compound_colour = "rgb(255,255,255)"

	compound_indicator = f"""
	<svg class='sent-compound' width='100' height='100' style='vertical-align:top; margin-right:10px;'>
		<title>Compound</title>
		<rect width='100' height='100' fill='{compound_colour}' rx='5' ry='5'/>
		<text x='50' y='15' text-anchor='middle' font-size='12' font-weight='bold' fill='#000'>Compound</text>
		<text x='50' y='60' text-anchor='middle' font-size='14' font-weight='bold' fill='#000'>{compound:.3f}</text>
	</svg>
	"""

	rendered_text = text
	# replacing the words in the text with HTML in backwards order
	for i, word_html in enumerate(output_text[::-1]):
		start, end = spans[-(i+1)]
		rendered_text = rendered_text[:start] + word_html + rendered_text[end:]

	rendered_text = rendered_text.replace('<span class="ttppee">', '<span class="icon" title="The text in italics was not in the original text, this is the description of the emoji that VADER uses for scoring.">üõà</span> <span class="ttppee">') # info utf8 icon: 

	display(HTML(css + f"<div class='text-wrapper'>{rendered_text}</div>"))
	display(HTML(f"""
	<div>
		{compound_indicator} 
		{proportions_indicator}
	</div>
	"""))

# %% ../nbs/55_vader.ipynb 77
def _make_colorizer(word2score):
	def colorizer(word, font_size, position, orientation, random_state=None, **kwargs):
		"""
		Colorizer function for WordCloud that returns a color based on the word's mean sentiment score.
		Dark gray for high absolute value, light gray for low.
		"""
		score = word2score.get(word, 0)
		norm_score = max(-1, min(1, score / 4))
		color_value = abs(int(norm_score * 255))
		return f"rgb({color_value}, {color_value}, {color_value})"  # Shades of gray
	return colorizer

# %% ../nbs/55_vader.ipynb 78
def sentiment_wordcloud(texts: list[str], 
						plot_mode: str|None = None, # select how the plot is generated, must be one of 'class_valence' or None (default), 'class', or 'valence' (there are additional notes below)
					   max_words: int = 200,
					   neutral_threshold: float = 0.05, # threshold for neutral sentiment scores, default is 0.05
					   font_path: str = None, # path to a font file for the word cloud
					   ) -> None:
	"""
	Generates a word cloud indicating the salience of VADER lexicon words across a list of texts. Note: 
	this is new functionality and is likely to change in future releases. 
	"""

	subplot_width=800
	subplot_height=600

	if plot_mode is None:
		plot_mode = 'class_valence'

	if plot_mode not in ['class_valence', 'class', 'valence']:
		raise ValueError("plot_mode must be one of 'class_valence' (default), 'class', or 'valence'.")

	vader = SentimentIntensityInterpreter()
	positive_features = []
	negative_features = []
	
	for text in texts:
		scores, features = vader.polarity_scores(text)
		if plot_mode == 'class_valence':
			if (scores['compound'] >= neutral_threshold):
				for feature in features:
					if feature[1] > 0:
						positive_features.append((feature[0].lower(), feature[1]))
			elif scores['compound'] <= -neutral_threshold:
				for feature in features:
					if feature[1] < 0:
						negative_features.append((feature[0].lower(), feature[1]))
		elif plot_mode == 'class':
			if (scores['compound'] >= neutral_threshold):
				for feature in features:
					if feature[1] != 0:
						positive_features.append((feature[0].lower(), feature[1]))
			elif scores['compound'] <= -neutral_threshold:
				for feature in features:
					if feature[1] != 0:
						negative_features.append((feature[0].lower(), feature[1]))
		elif plot_mode == 'valence':
			for feature in features:
				if feature[1]/4 < -neutral_threshold:
					negative_features.append((feature[0].lower(), feature[1]))
				elif feature[1]/4 > neutral_threshold:
					positive_features.append((feature[0].lower(), feature[1]))

	posdf = pd.DataFrame(positive_features, columns=['word', 'score'])
	negdf = pd.DataFrame(negative_features, columns=['word', 'score'])

	# Precompute mean sentiment per word
	pos_word2score = posdf.groupby('word')['score'].mean().to_dict()
	neg_word2score = negdf.groupby('word')['score'].mean().to_dict()

	# Generate word clouds
	pos_freq = posdf['word'].value_counts()
	neg_freq = negdf['word'].value_counts()

	pos_colorizer = _make_colorizer(pos_word2score)
	neg_colorizer = _make_colorizer(neg_word2score)

	# Create two subplots side by side
	fig, axes = plt.subplots(2, 1, figsize=(subplot_width / 100, subplot_height / 100 * 2), dpi=600)

	if plot_mode == 'class_valence':
		title_pos = "Positive-valence VADER words, Texts predicted Positive"
		title_neg = "Negative-valence VADER words, Texts predicted Negative"
	elif plot_mode == 'class':
		title_pos = "All VADER words, Texts predicted Positive"
		title_neg = "All VADER words, Texts predicted Negative"
	elif plot_mode == 'valence':
		title_pos = "Positive-valence VADER words, All texts"
		title_neg = "Negative-valence VADER words, All texts"

	# Positive word cloud
	wordcloud_pos = WordCloud(background_color="white", width=subplot_width, height=subplot_height, max_words = max_words, margin = 4, font_path = font_path).generate_from_frequencies(pos_freq)
	axes[0].imshow(wordcloud_pos.recolor(color_func=pos_colorizer), interpolation="bilinear")
	axes[0].set_title(title_pos, fontsize=16)
	axes[0].axis("off")

	# Negative word cloud
	wordcloud_neg = WordCloud(background_color="white", width=subplot_width, height=subplot_height, max_words = max_words, margin = 4, font_path = font_path).generate_from_frequencies(neg_freq)
	axes[1].imshow(wordcloud_neg.recolor(color_func=neg_colorizer), interpolation="bilinear")
	axes[1].set_title(title_neg, fontsize=16)
	axes[1].axis("off")

	plt.tight_layout()
	plt.show()
	

	

# %% ../nbs/55_vader.ipynb 83
class VaderSentimentProfileExtractor(BaseEstimator, TransformerMixin):
	""" Sci-kit Learn pipeline component to extract document-level sentiment profiles 
	consisting of document-level and sentence-level features with their order in the 
	document represented using VADER. 
	(This class is experimental and there may be breaking changes in the future). """
	def __init__(self, 
			  feature_store:TextFeatureStore = None, # (not implemented currently)
			  output:str = 'profile', # the feature output options are documented below. Valid values: 'profile' (default), 'profileonly', 'profilesections', 'profileallstats'
			  profile_first_n:int = 3, # number of sentences at start of doc to profile (ignored for 'profilesections')
			  profile_last_n:int = 3, # number of sentences at end of doc to profile (ignored for 'profilesections')
			  profile_sample_n:int = 4, # number of sentences to sample from doc sentences after first and last removed (ignored for 'profilesections')
			  profile_min_sentence_chars:int = 10, # minimum number of characters in body sentences for a sentence to be considered for the profile (ignored for 'profilesections')
			  profile_sections:int = 10, # number of sections to split the document into for profiling (for 'profilesections' only)
			):
		
		self.feature_store = feature_store
		if output not in ['profile', 'profilesections', 'profileallstats', 'profileonly']: 
			raise ValueError(f"output must be one of ['profile', 'profilesections', 'profileallstats', 'profileonly'], got {output}")
		self.output = output
		self.profile_first_n = profile_first_n
		self.profile_last_n = profile_last_n
		self.profile_sample_n = profile_sample_n
		self.profile_min_sentence_chars = profile_min_sentence_chars
		self.profile_sections = profile_sections

		# seeding random number generator for reproducibility
		np.random.seed(55)
		try:
			nltk.data.find('tokenizers/punkt_tab')
		except LookupError:
			nltk.download('punkt_tab')

		self.analyzer_ = SentimentIntensityAnalyzer()

# %% ../nbs/55_vader.ipynb 85
@patch
def fit(self:VaderSentimentProfileExtractor, X, y=None):
	""" Fit is implemented, but does nothing. """
	return self

# %% ../nbs/55_vader.ipynb 86
@patch
def section_profile(self:VaderSentimentProfileExtractor, text):
	""" Mean pooling of VADER scores across document sections . """
	
	sentences = sent_tokenize(text)
	sentiment_scores = [self.analyzer_.polarity_scores(sentence)['compound'] for sentence in sentences]

	sentiment_scores = np.array(sentiment_scores)

	X_meanpooled = [np.mean(chunk) if len(chunk) > 0 else 0
					for chunk in np.array_split(sentiment_scores, self.profile_sections)]
	X_meanpooled = np.array(X_meanpooled)
	return X_meanpooled

# %% ../nbs/55_vader.ipynb 87
@patch
def profile(self:VaderSentimentProfileExtractor, 
				text: str, # the document text
				doc_level_scores: dict, # VADER scores for document text
				) -> list[float]: # a document profile vector consisting of the document level scores and sentence-level scores across the document
	""" Create a document profile with VADER scores, which makes use of document level scores and sentence-level scores across the document. """
	sentences = sent_tokenize(text)
	if self.output == 'profileonly':
		scores = []
	else:
		scores = [doc_level_scores['compound'], doc_level_scores['neg'], doc_level_scores['neu'], doc_level_scores['pos']]
	sentences_to_score = []
	if len(sentences) < self.profile_first_n + self.profile_last_n + self.profile_sample_n:
		if len(sentences) < self.profile_first_n: ## padding to end of start if needed
			sentences = sentences + [''] * (self.profile_first_n + self.profile_last_n + self.profile_sample_n - len(sentences))
		elif len(sentences) < self.profile_first_n + self.profile_last_n: # # padding to start of end if needed
			sentences = sentences[:self.profile_first_n] + [''] * (self.profile_first_n + self.profile_last_n + self.profile_sample_n - len(sentences)) + sentences[self.profile_first_n:]
		else:
			sentences = sentences[:self.profile_first_n] + sentences[self.profile_first_n:-self.profile_last_n] + [''] * (self.profile_first_n + self.profile_last_n + self.profile_sample_n - len(sentences)) + sentences[-self.profile_last_n:]

	if self.profile_min_sentence_chars > 0 and len(sentences) > self.profile_first_n + self.profile_last_n + self.profile_sample_n:
		overlap = len(sentences) - self.profile_first_n - self.profile_last_n - self.profile_sample_n
		for i in range(self.profile_first_n, len(sentences) - self.profile_last_n):
			if len(sentences[i].strip()) < self.profile_min_sentence_chars:
				sentences[i] = None
				overlap -= 1
				if overlap == 0:
					break
		sentences = [sentence for sentence in sentences if sentence is not None]

	sentences_to_score.extend(sentences[:self.profile_first_n])
	sentences_to_score.extend(sentences[-self.profile_last_n:])
	sentences = sentences[self.profile_first_n:-self.profile_last_n]
	if len(sentences) == self.profile_sample_n:
		sentences_to_score.extend(sentences)
	elif len(sentences) > self.profile_sample_n:
		sample_indices = np.random.choice(len(sentences), self.profile_sample_n, replace=False)
		sample_indices.sort()
		sentences_to_score.extend([sentences[i] for i in sample_indices])
	del sentences

	for i, sentence in enumerate(sentences_to_score):
		if self.output == 'profileallstats':
			sentence_scores = self.analyzer_.polarity_scores(sentence)
			scores.extend([sentence_scores['compound'], sentence_scores['neg'], sentence_scores['neu'], sentence_scores['pos']])
		else:
			scores.append(self.analyzer_.polarity_scores(sentence)['compound'])

	if self.output =='profile' and len(scores) < 4 + self.profile_first_n + self.profile_last_n + self.profile_sample_n:
		print('scores')
		print(scores)
		print('sentences to score')	
		print(sentences_to_score)
		print('full text')
		print(text)
		raise ValueError(f"VADER profile vector is too short")

	return scores

# %% ../nbs/55_vader.ipynb 88
@patch
def transform(self:VaderSentimentProfileExtractor, X):
	""" Extracts the sentiment from the text using VADER. """
	results = []
	for text in X:
		scores = self.analyzer_.polarity_scores(text)
		if self.output == 'profilesections':
			results.append(self.section_profile(text))
		else:
			results.append(self.profile(text, scores))
	return np.atleast_2d(results)  # Ensure the output is always a 2D array

# %% ../nbs/55_vader.ipynb 89
@patch
def get_feature_names_out(self:VaderSentimentProfileExtractor, input_features=None):
	""" Get the feature names out from the model. """
	if self.output == 'profileonly':
		return [f'introduction_sentence_{i}' for i in range(self.profile_first_n)] + [f'conclusion_sentence_{i}' for i in range(self.profile_last_n)] + [f'body_sentence_sample_{i}' for i in range(self.profile_sample_n)]
	elif self.output == 'profileallstats':
		return ['doc_compound', 'doc_negative', 'doc_neutral', 'doc_positive'] + [f'introduction_sentence_{i}_compound' for i in range(self.profile_first_n)] + [f'introduction_sentence_{i}_negative' for i in range(self.profile_first_n)] + [f'introduction_sentence_{i}_neutral' for i in range(self.profile_first_n)] + [f'introduction_sentence_{i}_positive' for i in range(self.profile_first_n)] + [f'conclusion_sentence_{i}_compound' for i in range(self.profile_last_n)] + [f'conclusion_sentence_{i}_negative' for i in range(self.profile_last_n)] + [f'conclusion_sentence_{i}_neutral' for i in range(self.profile_last_n)] + [f'conclusion_sentence_{i}_positive' for i in range(self.profile_last_n)] + [f'body_sentence_sample_{i}_compound' for i in range(self.profile_sample_n)] + [f'body_sentence_sample_{i}_negative' for i in range(self.profile_sample_n)] + [f'body_sentence_sample_{i}_neutral' for i in range(self.profile_sample_n)] + [f'body_sentence_sample_{i}_positive' for i in range(self.profile_sample_n)]
	elif self.output == 'profilesections':
		return [f'section_{i}' for i in range(self.profile_sections)]
	else: # 'profile'
		return ['doc_compound', 'doc_negative', 'doc_neutral', 'doc_positive'] + [f'introduction_sentence_{i}' for i in range(self.profile_first_n)] + [f'conclusion_sentence_{i}' for i in range(self.profile_last_n)] + [f'body_sentence_sample_{i}' for i in range(self.profile_sample_n)]


# %% ../nbs/55_vader.ipynb 90
@patch
def plot_sentiment_structure(self: VaderSentimentProfileExtractor, 
                             X: list[str], 
                             y: list, 
                             target_classes: list = None, 
                             target_names: list = None,
                             n_clusters:int = 5, # Number of clusters per class
                             samples_per_cluster:int = 5, # number of samples to plot per cluster
                             renderer:str='svg', # 'svg' or 'png'
                             ):
    """
    Plot the sentiment structure of documents. Requires the VaderSentimentProfileExtractor to be instantiated with 
    output='profilesections' or 'profileonly'. For each class, cluster the documents by sentiment structure into n_clusters, and plot up to samples_per_cluster.
    (Experimental feature, will change in future).
    """
    if self.output not in ['profileonly', 'profilesections']:
        print(f"plot_sentiment_structure is only supported for output='profileonly' or output='profilesections'")
        return

    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    import matplotlib.colors as mcolors
    import numpy as np
    from matplotlib.colors import LinearSegmentedColormap
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from textplumber.report import plt_svg

    n_labels = len(target_classes)
    cluster_gap = 1  # Space between clusters (in y units)

    # Figure sizing
    size_scaler = 8
    fig = plt.figure(figsize=(20, size_scaler * n_labels + 1))
    import matplotlib.gridspec as gridspec
    gs = gridspec.GridSpec(n_labels, 1, hspace=0.2)
    axes = [fig.add_subplot(gs[i, 0]) for i in range(n_labels)]
    if n_labels == 1:
        axes = [axes]

    cmap = LinearSegmentedColormap.from_list('my_cmap', ['red', 'white', 'green'])
    norm = mcolors.Normalize(vmin=-1, vmax=1)
    bar_height = 1

    n_sections = self.profile_sections if self.output == 'profilesections' else self.profile_sample_n + self.profile_first_n + self.profile_last_n

    for idx, (label, ax) in enumerate(zip(target_classes, axes)):
        # Filter X for current label
        X_for_label = [X[i] for i in range(len(X)) if y[i] == label]
        if not X_for_label:
            continue

        X_features = self.transform(X_for_label)
        X_scaled = StandardScaler().fit_transform(X_features)

        # Cluster
        clusterer = KMeans(n_clusters=n_clusters, random_state=0).fit(X_scaled)
        labels_pred = clusterer.labels_

        # Plot up to 5 closest samples per cluster
        y_pos = 0  # Row counter for plotting
        for cluster_id in range(n_clusters, -1, -1):
            cluster_indices = np.where(labels_pred == cluster_id)[0]
            n_in_cluster = len(cluster_indices)
            if n_in_cluster == 0:
                continue
            # Find up to 5 closest to cluster center
            cluster_center = clusterer.cluster_centers_[cluster_id]
            distances = np.linalg.norm(X_scaled[cluster_indices] - cluster_center, axis=1)
            closest_indices = cluster_indices[np.argsort(distances)[:samples_per_cluster]]

            # Draw border around this cluster block (excluding the gap)
            cluster_block_top = y_pos
            cluster_block_height = len(closest_indices)
            rect_border = patches.Rectangle(
                (0, cluster_block_top), 1, cluster_block_height, 
                linewidth=0.1, edgecolor='black', facecolor='none', zorder=10
            )
            ax.add_patch(rect_border)

            # Label the cluster to the left, vertically centered on the block
            cluster_label_y = cluster_block_top + cluster_block_height / 2 - 0.5
            ax.text(0, cluster_label_y, 
                    f"Cluster {cluster_id} \n{n_in_cluster} samples", 
                    va='center', ha='right', fontsize=10, color='black',
                    transform=ax.transData)

            for k in closest_indices:
                sentiment_scores = X_features[k]
                for j, score in enumerate(sentiment_scores):
                    start = j / n_sections
                    width = 1 / n_sections
                    rect = patches.Rectangle(
                        (start, y_pos), width, bar_height, 
                        color=cmap(norm(score)), linewidth=0
                    )
                    ax.add_patch(rect)
                y_pos += 1  # Next row

            y_pos += cluster_gap  # Add gap after each cluster

        ax.set_xlim(0, 1)
        ax.set_ylim(0, y_pos)
        ax.set_yticks([])
        ax.set_xticks([])
        # Remove y axis label
        ax.set_ylabel('')
        ax.grid(False)
        if target_names:
            ax.set_title(f"{target_names[idx]}")
        # Remove all subplot borders (spines)
        for spine in ax.spines.values():
            spine.set_visible(False)
        # ax.set_xlabel("Position in Document", fontsize=12, labelpad=10)
        ax.xaxis.set_label_coords(0.95, -0.08)
        ax.set_xlabel('')

    # Add colorbar
    cax = fig.add_axes([0.78, 0.95, 0.1, 0.02])
    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
    sm.set_array([])
    fig.colorbar(sm, cax=cax, orientation='horizontal', label='Sentiment Score')
    cax.set_aspect(0.1)
    for spine in cax.spines.values():
        spine.set_linewidth(0.1)

    fig.suptitle("Sentiment structure of documents based on VADER predictions", fontsize=16, y=0.99)
    fig.text(0.08, 0.04, "Note: For each class, documents are clustered using KMeans by their sentence-level VADER sentiment structure. ", ha='left', fontsize=10, color='black')
    fig.text(0.08, 0.03, f"Cluster labels and count are shown on the left. Up to {samples_per_cluster} representative documents per cluster are represented for each cluster (one document per row).", ha='left', fontsize=10, color='black')
    if self.output == 'profilesections':
        fig.text(0.08, 0.02, f"Sentiment scores per sentence are pooled into {n_sections} sections across each document and the mean score for the pool calculated. These are represented in the order they appear in the document (i.e. left most are at start of document, right at end).", ha='left', fontsize=10, color='black')
    else:
        fig.text(0.08, 0.02, f"Sentiment scores are extracted from the first {self.profile_first_n} sentences, last {self.profile_last_n} sentences, and {self.profile_sample_n} sampled sentences in the document and these are represented in the order they appear in the document (i.e. left most are at start of document, right at end).", ha='left', fontsize=10, color='black')
    fig.text(0.08, 0.01, "The intensity of colors of each section represent the mean VADER compound score for sentences in that section. ", ha='left', fontsize=10, color='black')
    if renderer == 'svg':
        plt_svg(fig)
    else:
        plt.show()

# %% ../nbs/55_vader.ipynb 106
class VaderSentimentPOSNgramsExtractor(BaseEstimator, TransformerMixin):
	""" Sci-kit Learn pipeline component to extract ngrams based on POS 
	tags and sentiment from VADER lexicon.
	(This class is experimental and there may be breaking changes in the future, 
	including the possibility of complete removal). """
	def __init__(self, 
			  feature_store:TextFeatureStore = None, # (not implemented currently)
			  output:str = 'sentimentposngrams', # sentimentposngrams or sentintensityposngrams, this is experimental and likely to change
			  ngram_range:tuple = (2, 2), # ngram range for POS ngrams
			):
		
		self.feature_store = feature_store
		if output not in ['sentimentposngrams', 'sentintensityposngrams']: 
			raise ValueError(f"output must be one of ['sentimentposngrams', 'sentintensityposngrams'], got {output}")
		self.output = output
		self.ngram_range = ngram_range

		self.analyzer_ = SentimentIntensityAnalyzer()

# %% ../nbs/55_vader.ipynb 107
@patch
def convert_score_to_token_label(self:VaderSentimentPOSNgramsExtractor, 
								 score: float) -> str:
	""" Convert VADER score to a token label (experimental). """
	if score < 0:
		label = 'SENT_NEG'
	else:
		label = 'SENT_POS'
	if self.output == 'sentintensityposngrams':
		label += str(int(abs(score)))
	return label

# %% ../nbs/55_vader.ipynb 108
@patch
def get_sentiment_pos_ngrams(self:VaderSentimentPOSNgramsExtractor,
	text):
	""" Get ngrams of POS features and lexicon+pos features (experimental).  """
	doc_tokens = self.feature_store.get_tokens_from_texts([text])[0]
	doc_tokens = [token.lower() for token in doc_tokens]

	doc_pos = self.feature_store.get_pos_from_texts([text])[0]
	lexicon_words = list(self.analyzer_.lexicon.keys())
	lexicon_words_in_doc = list(set(doc_tokens).intersection(set(lexicon_words)))
	lexicon_labels = [self.convert_score_to_token_label(self.analyzer_.lexicon[word]) for word in lexicon_words_in_doc]
	# append lexicon labels to doc_pos at the same index as the lexicon words
	for i, token in enumerate(doc_tokens):
		if token in lexicon_words_in_doc:
			doc_pos[i] = lexicon_labels[lexicon_words_in_doc.index(token)] + '_' + doc_pos[i]
	return doc_pos


# %% ../nbs/55_vader.ipynb 109
@patch
def fit(self:VaderSentimentPOSNgramsExtractor, X, y=None):
	""" Fit derives all ngrams. """
	X_raw = []
	for text in X:
		X_raw.append(self.get_sentiment_pos_ngrams(text))
	self.vectorizer_ = CountVectorizer(tokenizer=pass_tokens,
								lowercase=False, 
								stop_words=None, 
								token_pattern=None, 
								min_df=1,
								max_df=1.0,
								max_features=None,
								ngram_range=self.ngram_range,
								vocabulary= None)
	self.vectorizer_.fit(X_raw)
	vocab = self.vectorizer_.get_feature_names_out()
	# if ngram doesn't contain SENT_ then removing here
	vocab = [x for x in vocab if 'SENT_' in x]
	# reminder: double-pass here is to remove all non-SENT_ ngrams - so only creating features derived from lexicon
	self.vectorizer_ = CountVectorizer(tokenizer=pass_tokens, 
								lowercase=False, 
								stop_words=None, 
								token_pattern=None, 
								min_df=1,
								max_df=1.0,
								max_features=None,
								ngram_range=self.ngram_range,
								vocabulary= vocab)
	self.vectorizer_.fit(X_raw)
	return self

# %% ../nbs/55_vader.ipynb 110
@patch
def transform(self:VaderSentimentPOSNgramsExtractor, X):
	""" Transform into sentiment ngrams. """
	results = []
	for text in X:
		results.append(self.get_sentiment_pos_ngrams(text))
	results = self.vectorizer_.transform(results)
	results = results.toarray()
	return np.atleast_2d(results)  # Ensure the output is always a 2D array

# %% ../nbs/55_vader.ipynb 111
@patch
def get_feature_names_out(self:VaderSentimentPOSNgramsExtractor, input_features=None):
	""" Get the feature names out from the model. """
	return self.vectorizer_.get_feature_names_out()

