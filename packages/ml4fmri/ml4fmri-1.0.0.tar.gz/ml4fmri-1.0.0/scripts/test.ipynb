{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae20948",
   "metadata": {},
   "source": [
    "# This is DEV test notebook, it has little useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78882d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module reloading script, fetches updates to ml4fmri package\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "PKG = \"ml4fmri\"\n",
    "# adjust as needed; this assumes your repo root is the CWD and sources are in ../src\n",
    "SRC = Path().joinpath(\"../src\").resolve()\n",
    "\n",
    "def _ensure_src_on_path():\n",
    "    p = str(SRC)\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "def fresh_import_ml4fmri():\n",
    "    \"\"\"\n",
    "    Remove any cached ml4fmri modules and import from SRC.\n",
    "    Returns the ml4fmri module and (if available) meanMLP class.\n",
    "    \"\"\"\n",
    "    _ensure_src_on_path()\n",
    "\n",
    "    # purge previous imports so code changes take effect\n",
    "    to_delete = [k for k in list(sys.modules) if k == PKG or k.startswith(PKG + \".\")]\n",
    "    for k in to_delete:\n",
    "        del sys.modules[k]\n",
    "\n",
    "    importlib.invalidate_caches()\n",
    "\n",
    "    ml4fmri = __import__(PKG)\n",
    "    meanMLP = None\n",
    "    try:\n",
    "        from ml4fmri.models import meanMLP \n",
    "    except Exception as e:\n",
    "        print(\"Note: could not import meanMLP yet:\", e)\n",
    "\n",
    "    try:\n",
    "        import inspect\n",
    "        print(\"ml4fmri imported from:\", Path(inspect.getfile(ml4fmri)).parent)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"ml4fmri ready\")\n",
    "    return ml4fmri\n",
    "\n",
    "# --- first import (or manual reload later) ---\n",
    "ml4fmri = fresh_import_ml4fmri()\n",
    "\n",
    "# get sample data for experiments\n",
    "from abide import load_data as load_abide\n",
    "from cobre import load_data as load_cobre\n",
    "\n",
    "data, labels = load_abide()\n",
    "data.shape, labels.shape\n",
    "\n",
    "data, labels = data[:100], labels[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b5a71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abide import load_data as load_abide\n",
    "from cobre import load_data as load_cobre\n",
    "\n",
    "data, labels = load_abide()\n",
    "DATA, LABELS = data, labels\n",
    "data.shape, labels.shape\n",
    "\n",
    "# data, labels = data[:100], labels[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb82cb",
   "metadata": {},
   "source": [
    "# test cvbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a00df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml4fmri = fresh_import_ml4fmri()\n",
    "from ml4fmri import cvbench # runs CV experiments with implemented models on the given data\n",
    "# report = cvbench(data, labels, n_folds=10)\n",
    "report = cvbench(data, labels, ['LR', 'meanMLP'], n_folds=10)\n",
    "report.plot_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ad4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_scores_h(metric='test_f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8755cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract report dataframes\n",
    "\n",
    "test_df = report.get_test_dataframe() # returns dataframe with test metrics: \n",
    "# classification metrics and loss on the test data and training time per fold for each model.\n",
    "# Useful for model comparison.\n",
    "\n",
    "train_df = report.get_train_dataframe() # returns dataframe with training logs: \n",
    "# losses and classification metrics on the training and validation data per fold for each model. \n",
    "# Useful for training inspection.\n",
    "\n",
    "meta_df = report.get_meta() # dictionary with metadata about the training process:\n",
    "# ['models', 'n_folds', 'val_ratio', 'random_state', 'input_size', 'n_classes', 'train_indices', 'val_indices', 'test_indices']\n",
    "# the last 3 give you CV indices per fold, so that you can potentially replicate the splits in your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ebdecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_training_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4246653",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = report.get_train_dataframe()\n",
    "test_df = report.get_test_dataframe()\n",
    "meta = report.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.save() # saves the report to disk: \n",
    "# cvbench_train.csv with train dataframe, \n",
    "# cvbench_test.csv with test dataframe,\n",
    "# cvbench_meta.json with metadata,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meanmlp-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
