"""
Test Runner (RFS v4)

ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ
- ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ì§€ì›
- ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ì¸¡ì •
- í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
"""

import asyncio
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import xml.etree.ElementTree as ET

try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
    from rich.tree import Tree
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

from ...core import Result, Success, Failure

if RICH_AVAILABLE:
    console = Console()
else:
    console = None


class TestFramework(Enum):
    """ì§€ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    PYTEST = "pytest"
    UNITTEST = "unittest"
    ASYNCIO = "asyncio"
    CUSTOM = "custom"


class TestType(Enum):
    """í…ŒìŠ¤íŠ¸ ìœ í˜•"""
    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    END_TO_END = "e2e"
    SECURITY = "security"


@dataclass
class TestConfig:
    """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    framework: TestFramework = TestFramework.PYTEST
    test_paths: List[str] = field(default_factory=lambda: ["tests/"])
    patterns: List[str] = field(default_factory=lambda: ["test_*.py", "*_test.py"])
    parallel: bool = True
    max_workers: int = 4
    coverage: bool = True
    coverage_threshold: float = 80.0
    verbose: bool = True
    fail_fast: bool = False
    timeout: int = 300  # 5ë¶„
    environment_vars: Dict[str, str] = field(default_factory=dict)
    fixtures_path: Optional[str] = None
    mock_config: Optional[Dict[str, Any]] = None


@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    framework: TestFramework
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    error_tests: int = 0
    execution_time: float = 0.0
    coverage_percentage: Optional[float] = None
    failed_test_details: List[Dict[str, Any]] = field(default_factory=list)
    coverage_report: Optional[Dict[str, Any]] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥  ê³„ì‚°"""
        if self.total_tests == 0:
            return 0.0
        return (self.passed_tests / self.total_tests) * 100
    
    @property
    def is_successful(self) -> bool:
        """í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€"""
        return self.failed_tests == 0 and self.error_tests == 0


class TestRunner:
    """ê³ ê¸‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config: Optional[TestConfig] = None):
        self.config = config or TestConfig()
        self.project_path = Path.cwd()
        self.test_history: List[TestResult] = []
    
    async def run_tests(self, test_type: Optional[TestType] = None, **kwargs) -> Result[TestResult, str]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            # ì„¤ì • ì—…ë°ì´íŠ¸
            for key, value in kwargs.items():
                if hasattr(self.config, key):
                    setattr(self.config, key, value)
            
            if console:
                console.print(Panel(
                    f"ğŸ§ª RFS v4 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘\n\n"
                    f"ğŸ”§ í”„ë ˆì„ì›Œí¬: {self.config.framework.value}\n"
                    f"ğŸ“ ê²½ë¡œ: {', '.join(self.config.test_paths)}\n"
                    f"âš¡ ë³‘ë ¬ ì‹¤í–‰: {'ì˜ˆ' if self.config.parallel else 'ì•„ë‹ˆì˜¤'}\n"
                    f"ğŸ“Š ì»¤ë²„ë¦¬ì§€: {'ì˜ˆ' if self.config.coverage else 'ì•„ë‹ˆì˜¤'}\n"
                    f"ğŸ¯ íƒ€ì…: {test_type.value if test_type else 'ëª¨ë“  íƒ€ì…'}",
                    title="í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
                    border_style="blue"
                ))
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            start_time = time.time()
            
            if self.config.framework == TestFramework.PYTEST:
                result = await self._run_pytest(test_type)
            elif self.config.framework == TestFramework.UNITTEST:
                result = await self._run_unittest(test_type)
            elif self.config.framework == TestFramework.ASYNCIO:
                result = await self._run_asyncio_tests(test_type)
            else:
                return Failure(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬: {self.config.framework}")
            
            if result.is_failure():
                return result
            
            test_result = result.unwrap()
            test_result.execution_time = time.time() - start_time
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ í‘œì‹œ
            if console:
                await self._display_test_results(test_result)
            
            # ì´ë ¥ì— ì¶”ê°€
            self.test_history.append(test_result)
            
            return Success(test_result)
            
        except Exception as e:
            return Failure(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    
    async def _run_pytest(self, test_type: Optional[TestType]) -> Result[TestResult, str]:
        """pytest ì‹¤í–‰"""
        try:
            # pytest ëª…ë ¹ì–´ êµ¬ì„±
            cmd = ['python', '-m', 'pytest']
            
            # í…ŒìŠ¤íŠ¸ ê²½ë¡œ ì¶”ê°€
            for path in self.config.test_paths:
                if Path(path).exists():
                    cmd.append(path)
            
            # ì˜µì…˜ ì¶”ê°€
            if self.config.verbose:
                cmd.append('-v')
            
            if self.config.fail_fast:
                cmd.append('-x')
            
            if self.config.parallel and self.config.max_workers > 1:
                cmd.extend(['-n', str(self.config.max_workers)])
            
            if self.config.coverage:
                cmd.extend([
                    '--cov=.',
                    '--cov-report=xml',
                    '--cov-report=html',
                    '--cov-report=term-missing'
                ])
            
            # JUnit XML ë¦¬í¬íŠ¸
            cmd.extend(['--junit-xml=test-results.xml'])
            
            # í…ŒìŠ¤íŠ¸ íƒ€ì… í•„í„°ë§
            if test_type:
                cmd.extend(['-m', test_type.value])
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env = {**os.environ, **self.config.environment_vars}
            
            if console:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    console=console
                ) as progress:
                    task = progress.add_task("pytest ì‹¤í–‰ ì¤‘...", total=None)
                    
                    # pytest ì‹¤í–‰
                    process = await asyncio.create_subprocess_exec(
                        *cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.STDOUT,
                        env=env
                    )
                    
                    output_lines = []
                    while True:
                        line = await process.stdout.readline()
                        if not line:
                            break
                        
                        line_str = line.decode().strip()
                        output_lines.append(line_str)
                    
                    await process.wait()
                    progress.remove_task(task)
            
            # ê²°ê³¼ íŒŒì‹±
            test_result = await self._parse_pytest_results(process.returncode, output_lines)
            return Success(test_result)
            
        except Exception as e:
            return Failure(f"pytest ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    
    async def _parse_pytest_results(self, return_code: int, output_lines: List[str]) -> TestResult:
        """pytest ê²°ê³¼ íŒŒì‹±"""
        result = TestResult(framework=TestFramework.PYTEST)
        
        try:
            # JUnit XML íŒŒì‹±
            xml_file = Path("test-results.xml")
            if xml_file.exists():
                tree = ET.parse(xml_file)
                root = tree.getroot()
                
                result.total_tests = int(root.get('tests', 0))
                result.failed_tests = int(root.get('failures', 0))
                result.error_tests = int(root.get('errors', 0))
                result.skipped_tests = int(root.get('skipped', 0))
                result.passed_tests = result.total_tests - result.failed_tests - result.error_tests - result.skipped_tests
                
                # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸ ì •ë³´
                for testcase in root.findall('.//testcase'):
                    failure = testcase.find('failure')
                    error = testcase.find('error')
                    
                    if failure is not None or error is not None:
                        result.failed_test_details.append({
                            'name': testcase.get('name', ''),
                            'classname': testcase.get('classname', ''),
                            'time': float(testcase.get('time', 0)),
                            'message': (failure or error).get('message', '') if (failure or error) is not None else '',
                            'details': (failure or error).text if (failure or error) is not None else ''
                        })
            
            # ì»¤ë²„ë¦¬ì§€ ì •ë³´ íŒŒì‹±
            coverage_xml = Path("coverage.xml")
            if coverage_xml.exists() and self.config.coverage:
                try:
                    tree = ET.parse(coverage_xml)
                    root = tree.getroot()
                    coverage_elem = root.find('.//coverage')
                    if coverage_elem is not None:
                        result.coverage_percentage = float(coverage_elem.get('line-rate', 0)) * 100
                except:
                    pass
            
        except Exception as e:
            if console:
                console.print(f"âš ï¸  ê²°ê³¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {str(e)}", style="yellow")
        
        return result
    
    async def _run_unittest(self, test_type: Optional[TestType]) -> Result[TestResult, str]:
        """unittest ì‹¤í–‰"""
        try:
            # unittest ëª…ë ¹ì–´ êµ¬ì„±
            cmd = ['python', '-m', 'unittest']
            
            if self.config.verbose:
                cmd.append('-v')
            
            # í…ŒìŠ¤íŠ¸ ë””ìŠ¤ì»¤ë²„ë¦¬
            cmd.extend(['discover', '-s', self.config.test_paths[0], '-p', 'test_*.py'])
            
            # ì‹¤í–‰
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT
            )
            
            stdout, _ = await process.communicate()
            output = stdout.decode()
            
            # ê²°ê³¼ íŒŒì‹± (ê°„ë‹¨í•œ êµ¬í˜„)
            result = TestResult(framework=TestFramework.UNITTEST)
            
            # unittest ì¶œë ¥ì—ì„œ ì •ë³´ ì¶”ì¶œ
            lines = output.split('\n')
            for line in lines:
                if 'Ran' in line and 'test' in line:
                    import re
                    match = re.search(r'Ran (\d+) test', line)
                    if match:
                        result.total_tests = int(match.group(1))
                
                if 'FAILED' in line:
                    match = re.search(r'FAILED \(.*failures=(\d+).*\)', line)
                    if match:
                        result.failed_tests = int(match.group(1))
            
            result.passed_tests = result.total_tests - result.failed_tests
            
            return Success(result)
            
        except Exception as e:
            return Failure(f"unittest ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    
    async def _run_asyncio_tests(self, test_type: Optional[TestType]) -> Result[TestResult, str]:
        """asyncio ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            # pytest-asyncio ì‚¬ìš©
            cmd = ['python', '-m', 'pytest', '--asyncio-mode=auto']
            
            for path in self.config.test_paths:
                if Path(path).exists():
                    cmd.append(path)
            
            if self.config.verbose:
                cmd.append('-v')
            
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.STDOUT
            )
            
            stdout, _ = await process.communicate()
            
            # pytest ê²°ê³¼ íŒŒì‹± ì¬ì‚¬ìš©
            result = await self._parse_pytest_results(process.returncode, stdout.decode().split('\n'))
            result.framework = TestFramework.ASYNCIO
            
            return Success(result)
            
        except Exception as e:
            return Failure(f"asyncio í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
    
    async def _display_test_results(self, result: TestResult) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ í‘œì‹œ"""
        if not console:
            return
        
        # ê²°ê³¼ ìš”ì•½ í…Œì´ë¸”
        summary_table = Table(title="í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½", show_header=True, header_style="bold magenta")
        summary_table.add_column("í•­ëª©", style="cyan", width=15)
        summary_table.add_column("ê°’", style="white", justify="right")
        summary_table.add_column("ë¹„ìœ¨", style="green", justify="right")
        
        summary_table.add_row("ì´ í…ŒìŠ¤íŠ¸", str(result.total_tests), "100%")
        summary_table.add_row(
            "í†µê³¼", 
            f"[green]{result.passed_tests}[/green]", 
            f"[green]{result.success_rate:.1f}%[/green]"
        )
        
        if result.failed_tests > 0:
            failure_rate = (result.failed_tests / result.total_tests) * 100 if result.total_tests > 0 else 0
            summary_table.add_row(
                "ì‹¤íŒ¨", 
                f"[red]{result.failed_tests}[/red]", 
                f"[red]{failure_rate:.1f}%[/red]"
            )
        
        if result.skipped_tests > 0:
            skip_rate = (result.skipped_tests / result.total_tests) * 100 if result.total_tests > 0 else 0
            summary_table.add_row(
                "ê±´ë„ˆëœ€", 
                f"[yellow]{result.skipped_tests}[/yellow]", 
                f"[yellow]{skip_rate:.1f}%[/yellow]"
            )
        
        summary_table.add_row("ì‹¤í–‰ ì‹œê°„", f"{result.execution_time:.2f}ì´ˆ", "")
        
        if result.coverage_percentage is not None:
            coverage_color = "green" if result.coverage_percentage >= self.config.coverage_threshold else "red"
            summary_table.add_row(
                "ì½”ë“œ ì»¤ë²„ë¦¬ì§€", 
                f"[{coverage_color}]{result.coverage_percentage:.1f}%[/{coverage_color}]",
                f"ëª©í‘œ: {self.config.coverage_threshold}%"
            )
        
        console.print(summary_table)
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸ ì •ë³´
        if result.failed_test_details:
            console.print("\n")
            failure_tree = Tree("âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸")
            
            for failure in result.failed_test_details:
                test_node = failure_tree.add(f"[red]{failure['name']}[/red] ({failure['classname']})")
                if failure['message']:
                    test_node.add(f"ë©”ì‹œì§€: {failure['message']}")
                if failure['time']:
                    test_node.add(f"ì‹¤í–‰ ì‹œê°„: {failure['time']:.3f}ì´ˆ")
            
            console.print(failure_tree)
        
        # ìµœì¢… ìƒíƒœ
        if result.is_successful:
            console.print(Panel(
                f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n\n"
                f"ğŸ¯ ì„±ê³µë¥ : {result.success_rate:.1f}%\n"
                f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {result.execution_time:.2f}ì´ˆ"
                + (f"\nğŸ“Š ì½”ë“œ ì»¤ë²„ë¦¬ì§€: {result.coverage_percentage:.1f}%" if result.coverage_percentage else ""),
                title="í…ŒìŠ¤íŠ¸ ì„±ê³µ",
                border_style="green"
            ))
        else:
            console.print(Panel(
                f"âŒ {result.failed_tests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨\n\n"
                f"ğŸ¯ ì„±ê³µë¥ : {result.success_rate:.1f}%\n"
                f"â±ï¸  ì‹¤í–‰ ì‹œê°„: {result.execution_time:.2f}ì´ˆ\n\n"
                f"ğŸ’¡ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
                title="í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                border_style="red"
            ))
    
    async def generate_test_template(self, test_name: str, test_type: TestType) -> Result[str, str]:
        """í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        try:
            template = self._get_test_template(test_name, test_type)
            
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ê²°ì •
            test_dir = Path(self.config.test_paths[0])
            test_dir.mkdir(parents=True, exist_ok=True)
            
            test_file = test_dir / f"test_{test_name.lower()}.py"
            
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write(template)
            
            if console:
                console.print(Panel(
                    f"âœ… í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ\n\n"
                    f"ğŸ“ íŒŒì¼: {test_file}\n"
                    f"ğŸ§ª ìœ í˜•: {test_type.value}\n"
                    f"ğŸ”§ í”„ë ˆì„ì›Œí¬: {self.config.framework.value}",
                    title="í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿",
                    border_style="green"
                ))
            
            return Success(f"í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„± ì™„ë£Œ: {test_file}")
            
        except Exception as e:
            return Failure(f"í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def _get_test_template(self, test_name: str, test_type: TestType) -> str:
        """í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        if self.config.framework == TestFramework.PYTEST:
            return f'''"""
{test_name.title()} í…ŒìŠ¤íŠ¸

{test_type.value} í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸
"""

import pytest
import asyncio
from unittest.mock import Mock, patch

# RFS v4 ì„í¬íŠ¸
from rfs_v4 import Result, Success, Failure


class Test{test_name.title()}:
    """
    {test_name.title()} í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
    """
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""
        pass
    
    def teardown_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ í›„ ì •ë¦¬"""
        pass
    
    def test_{test_name}_success(self):
        """ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
    
    def test_{test_name}_failure(self):
        """ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
    
    @pytest.mark.asyncio
    async def test_{test_name}_async(self):
        """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
    
    @pytest.mark.parametrize("input_data,expected", [
        ("test1", "expected1"),
        ("test2", "expected2"),
    ])
    def test_{test_name}_parametrized(self, input_data, expected):
        """ë§¤ê°œë³€ìˆ˜í™” í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
    
    def test_{test_name}_with_mock(self):
        """ëª¨í‚¹ì„ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸"""
        with patch('module.function') as mock_func:
            # Given (ì¤€ë¹„)
            mock_func.return_value = "mocked_result"
            
            # When (ì‹¤í–‰)
            
            # Then (ê²€ì¦)
            mock_func.assert_called_once()
            assert True  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
'''
        
        else:  # unittest
            return f'''"""
{test_name.title()} í…ŒìŠ¤íŠ¸

{test_type.value} í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ unittest ê¸°ë°˜ í…ŒìŠ¤íŠ¸
"""

import unittest
import asyncio
from unittest.mock import Mock, patch

# RFS v4 ì„í¬íŠ¸
from rfs_v4 import Result, Success, Failure


class Test{test_name.title()}(unittest.TestCase):
    """
    {test_name.title()} í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
    """
    
    def setUp(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""
        pass
    
    def tearDown(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ í›„ ì •ë¦¬"""
        pass
    
    def test_{test_name}_success(self):
        """ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
    
    def test_{test_name}_failure(self):
        """ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
    
    def test_{test_name}_async(self):
        """ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸"""
        async def async_test():
            # Given (ì¤€ë¹„)
            
            # When (ì‹¤í–‰)
            
            # Then (ê²€ì¦)
            self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„
        
        asyncio.run(async_test())
    
    @patch('module.function')
    def test_{test_name}_with_mock(self, mock_func):
        """ëª¨í‚¹ì„ ì‚¬ìš©í•œ í…ŒìŠ¤íŠ¸"""
        # Given (ì¤€ë¹„)
        mock_func.return_value = "mocked_result"
        
        # When (ì‹¤í–‰)
        
        # Then (ê²€ì¦)
        mock_func.assert_called_once()
        self.assertTrue(True)  # TODO: ì‹¤ì œ í…ŒìŠ¤íŠ¸ êµ¬í˜„


if __name__ == '__main__':
    unittest.main()
'''
    
    def get_test_history(self) -> List[TestResult]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ"""
        return self.test_history.copy()
    
    def get_coverage_report(self) -> Optional[Dict[str, Any]]:
        """ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ ì¡°íšŒ"""
        if not self.test_history:
            return None
        
        latest_result = self.test_history[-1]
        return latest_result.coverage_report