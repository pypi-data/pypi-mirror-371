Metadata-Version: 2.4
Name: reasoning-kernel
Version: 0.0.2
Summary: Model Synthesis Architecture (MSA) reasoning engine built on ReasoningFleet's FastAPI/Semantic Kernel foundation with NumPyro probabilistic programming
Project-URL: Homepage, https://github.com/Qredence/Reasoning-Kernel
Project-URL: Documentation, https://github.com/Qredence/Reasoning-Kernel/tree/main/docs
Project-URL: Repository, https://github.com/Qredence/Reasoning-Kernel
Project-URL: Bug Reports, https://github.com/Qredence/Reasoning-Kernel/issues
Project-URL: Source Code, https://github.com/Qredence/Reasoning-Kernel
Project-URL: Changelog, https://github.com/Qredence/Reasoning-Kernel/releases
Project-URL: Funding, https://github.com/sponsors/Qredence
Author-email: Qredence <contact@qredence.ai>
Maintainer-email: Qredence Team <maintainers@qredence.ai>
License: Apache-2.0
Keywords: ai,async,fastapi,machine-learning,msa,numpyro,probabilistic-programming,reasoning,semantic-kernel
Classifier: Development Status :: 4 - Beta
Classifier: Framework :: AsyncIO
Classifier: Framework :: FastAPI
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: <3.13,>=3.10
Requires-Dist: aiohttp<4.0.0,>=3.9.0
Requires-Dist: arviz>=0.17.0
Requires-Dist: autoflake>=2.3.1
Requires-Dist: azure-ai-inference>=1.0.0b1
Requires-Dist: azure-ai-projects
Requires-Dist: azure-core>=1.29.5
Requires-Dist: azure-identity>=1.15.0
Requires-Dist: bcrypt>=4.0.0
Requires-Dist: bleach>=6.1.0
Requires-Dist: click>=8.1.0
Requires-Dist: daytona
Requires-Dist: daytona-sdk
Requires-Dist: fastapi<1.0.0,>=0.104.0
Requires-Dist: graphviz>=0.20.1
Requires-Dist: hiredis>=2.3.0
Requires-Dist: httpx>=0.25.0
Requires-Dist: itsdangerous>=2.1.0
Requires-Dist: jax[cpu]>=0.4.20
Requires-Dist: jaxlib>=0.4.20
Requires-Dist: langextract
Requires-Dist: networkx>=3.2.1
Requires-Dist: numpy>=1.24.0
Requires-Dist: numpyro>=0.15.0
Requires-Dist: openai>=1.10.0
Requires-Dist: pandas>=2.0.0
Requires-Dist: psutil>=5.9.8
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: pydantic-settings>=2.1.0
Requires-Dist: pydantic<3.0.0,>=2.5.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: redis>=6.0.0
Requires-Dist: redisvl>=0.8.0
Requires-Dist: rich>=13.0.0
Requires-Dist: scipy>=1.11.0
Requires-Dist: semantic-kernel==1.35.3
Requires-Dist: sentence-transformers>=2.2.2
Requires-Dist: sqlalchemy>=2.0.0
Requires-Dist: structlog>=23.2.0
Requires-Dist: tabulate>=0.9.0
Requires-Dist: uvicorn[standard]<1.0.0,>=0.24.0
Requires-Dist: vulture>=2.7
Provides-Extra: all
Requires-Dist: aioredis>=2.0.1; extra == 'all'
Requires-Dist: aiortc>=1.9.0; extra == 'all'
Requires-Dist: azure-ai-inference>=1.0.0b6; extra == 'all'
Requires-Dist: azure-core-tracing-opentelemetry>=1.0.0b11; extra == 'all'
Requires-Dist: azure-core>=1.29.0; extra == 'all'
Requires-Dist: azure-cosmos~=4.7; extra == 'all'
Requires-Dist: azure-identity>=1.15.0; extra == 'all'
Requires-Dist: azure-search-documents>=11.6.0b4; extra == 'all'
Requires-Dist: dapr-ext-fastapi>=1.14.0; extra == 'all'
Requires-Dist: dapr>=1.14.0; extra == 'all'
Requires-Dist: flask-dapr>=1.14.0; extra == 'all'
Requires-Dist: google-cloud-aiplatform<2.0.0,>=1.60.0; extra == 'all'
Requires-Dist: google-generativeai~=0.8; extra == 'all'
Requires-Dist: opentelemetry-api~=1.25; extra == 'all'
Requires-Dist: opentelemetry-exporter-otlp~=1.25; extra == 'all'
Requires-Dist: opentelemetry-instrumentation-fastapi~=0.46b0; extra == 'all'
Requires-Dist: opentelemetry-instrumentation-httpx~=0.46b0; extra == 'all'
Requires-Dist: opentelemetry-instrumentation-logging~=0.46b0; extra == 'all'
Requires-Dist: opentelemetry-sdk~=1.25; extra == 'all'
Requires-Dist: pandas~=2.2; extra == 'all'
Requires-Dist: prometheus-client>=0.20.0; extra == 'all'
Requires-Dist: pyarrow<22.0,>=12.0; extra == 'all'
Requires-Dist: sentence-transformers>=2.2.0; extra == 'all'
Requires-Dist: transformers>=4.36.0; extra == 'all'
Requires-Dist: types-redis~=4.6.0.20240425; extra == 'all'
Requires-Dist: usearch~=2.16; extra == 'all'
Requires-Dist: websockets<16,>=13; extra == 'all'
Provides-Extra: azure
Requires-Dist: azure-ai-inference>=1.0.0b6; extra == 'azure'
Requires-Dist: azure-core-tracing-opentelemetry>=1.0.0b11; extra == 'azure'
Requires-Dist: azure-core>=1.29.0; extra == 'azure'
Requires-Dist: azure-cosmos~=4.7; extra == 'azure'
Requires-Dist: azure-identity>=1.15.0; extra == 'azure'
Requires-Dist: azure-search-documents>=11.6.0b4; extra == 'azure'
Provides-Extra: dapr
Requires-Dist: dapr-ext-fastapi>=1.14.0; extra == 'dapr'
Requires-Dist: dapr>=1.14.0; extra == 'dapr'
Requires-Dist: flask-dapr>=1.14.0; extra == 'dapr'
Provides-Extra: data-io
Requires-Dist: pyarrow<22.0,>=12.0; extra == 'data-io'
Requires-Dist: usearch~=2.16; extra == 'data-io'
Provides-Extra: data-science
Requires-Dist: pandas~=2.2; extra == 'data-science'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: flake8>=6.0.0; extra == 'dev'
Requires-Dist: isort>=5.12.0; extra == 'dev'
Requires-Dist: jupyter>=1.0.0; extra == 'dev'
Requires-Dist: mypy>=1.7.0; extra == 'dev'
Requires-Dist: pre-commit>=3.6.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.1.0; extra == 'dev'
Requires-Dist: pytest-mock>=3.12.0; extra == 'dev'
Requires-Dist: pytest-xdist>=3.5.0; extra == 'dev'
Requires-Dist: pytest>=7.4.0; extra == 'dev'
Requires-Dist: ruff>=0.5.0; extra == 'dev'
Provides-Extra: google
Requires-Dist: google-cloud-aiplatform<2.0.0,>=1.60.0; extra == 'google'
Requires-Dist: google-generativeai~=0.8; extra == 'google'
Provides-Extra: huggingface
Requires-Dist: sentence-transformers>=2.2.0; extra == 'huggingface'
Requires-Dist: transformers>=4.36.0; extra == 'huggingface'
Provides-Extra: observability
Requires-Dist: opentelemetry-api~=1.25; extra == 'observability'
Requires-Dist: opentelemetry-exporter-otlp~=1.25; extra == 'observability'
Requires-Dist: opentelemetry-instrumentation-fastapi~=0.46b0; extra == 'observability'
Requires-Dist: opentelemetry-instrumentation-httpx~=0.46b0; extra == 'observability'
Requires-Dist: opentelemetry-instrumentation-logging~=0.46b0; extra == 'observability'
Requires-Dist: opentelemetry-sdk~=1.25; extra == 'observability'
Requires-Dist: prometheus-client>=0.20.0; extra == 'observability'
Provides-Extra: realtime
Requires-Dist: aiortc>=1.9.0; extra == 'realtime'
Requires-Dist: websockets<16,>=13; extra == 'realtime'
Provides-Extra: redis
Requires-Dist: aioredis>=2.0.1; extra == 'redis'
Requires-Dist: types-redis~=4.6.0.20240425; extra == 'redis'
Description-Content-Type: text/markdown

# Reasoning Kernel

![Reasoning Kernel Banner](https://via.placeholder.com/800x200/1e1e1e/ffffff?text=Reasoning+Kernel)

A **Semantic Kernel-native** reasoning system implementing the **Model Synthesis Architecture (MSA)** for open-world cognitive reasoning. Built entirely on Microsoft Semantic Kernel with plugin-based modularity and enterprise-grade orchestration.

## Project status

This repository is an active work in progress. APIs, configuration, documentation, and examples may change without notice.

- Documentation update plan: `docs/documentation-restructure-plan.md`
- Recent documentation changes: `docs/documentation-update-summary.md`
- System overview and scope: `docs/full-system.md`

## 🚀 Core Features

- **🧠 SK-Native Architecture**: Built entirely on Microsoft Semantic Kernel patterns
- **🔌 Plugin Ecosystem**: Modular reasoning capabilities as SK plugins  
- **📋 Intelligent Planning**: SK planners for complex reasoning orchestration
- **💾 Multi-Tier Memory**: Redis/PostgreSQL integration via SK memory abstractions
- **🎯 MSA Pipeline**: Five-stage reasoning process as plugin chains
- **🌐 Multi-Model Support**: Azure OpenAI, Google Gemini, and local models
- **⚡ Production Ready**: FastAPI, streaming, and enterprise deployment

## 🚀 Quick Start

### Prerequisites

- Python 3.10+ (3.13+ not yet supported due to dependency compatibility)
- Azure OpenAI or Google AI Studio API access
- Redis (optional, for memory features)

### Installation

#### One-Line Installation (Recommended)

For macOS and Linux:

```bash
curl -fsSL https://raw.githubusercontent.com/Qredence/Reasoning-Kernel/main/setup/install.sh | bash
```

For Windows:

```cmd
curl -fsSL https://raw.githubusercontent.com/Qredence/Reasoning-Kernel/main/setup/install.bat -o install.bat
install.bat
```

#### Manual Installation

```bash
# Clone the repository
git clone https://github.com/Qredence/Reasoning-Kernel.git
cd Reasoning-Kernel

# Install with Semantic Kernel support
uv venv && source .venv/bin/activate
uv pip install -e ".[azure,google]"

# Alternative: Install with pip
pip install -e ".[azure,google]"
```

> For a complete setup guide (including environment, optional services, and troubleshooting), see the Installation Guide: `docs/guides/installation.md`.

### Configuration

Set up your environment variables:

```bash
# Azure OpenAI (Recommended)
export AZURE_OPENAI_ENDPOINT="your-endpoint"
export AZURE_OPENAI_API_KEY="your-key"
export AZURE_OPENAI_DEPLOYMENT="gpt-4"
export AZURE_OPENAI_API_VERSION="2024-12-01-preview"

# Google AI (Alternative)
export GOOGLE_AI_API_KEY="your-key"

# Optional: Redis for memory
export REDIS_URL="redis://localhost:6379"
```

### Basic Usage

#### Python SDK

```python
import asyncio
from reasoning_kernel.core.kernel_config import KernelManager
from reasoning_kernel.services.redis_service import create_redis_services
from reasoning_kernel.reasoning_kernel import ReasoningKernel, ReasoningConfig

async def main():
  # Initialize Semantic Kernel (uses Azure OpenAI env vars)
  km = KernelManager()
  await km.initialize()

  # Optional: Redis for memory (uses REDIS_URL or host/port)
  memory_service, _ = create_redis_services()

  # Initialize reasoning system
  rk = ReasoningKernel(kernel=km.kernel, redis_client=memory_service, config=ReasoningConfig())

  # Perform reasoning
  result = await rk.reason(
    "A factory machine has failed and production is stopped. "
    "Analyze the situation and suggest solutions."
  )

  print(result.success, result.overall_confidence)

asyncio.run(main())
```

#### CLI Usage

```bash
# Basic reasoning
reasoning-kernel "Analyze supply chain disruption scenario"

# Use specific reasoning mode
reasoning-kernel --mode knowledge "Factory production failure analysis"

# Interactive mode
reasoning-kernel --interactive

# JSON output for automation
reasoning-kernel --output json "Market analysis request"
```

## 📚 Documentation

- Getting started and concepts
  - Core concepts: `docs/core_concepts.md`
  - Full system overview: `docs/full-system.md`
  - Product requirements (PRD): `docs/reasoning-kernel-PRD.md`
- Architecture
  - MSA framework: `docs/architecture/msa-framework.md`
  - Semantic Kernel architecture: `docs/architecture/semantic-kernel-architecture.md`
  - Thinking exploration: `docs/architecture/thinking-exploration-reasoning-kernel.md`
- API & Services
  - REST API reference: `docs/api/rest-api.md`
- Memory & Redis (MCP)
  - MCP Redis integration: `docs/mcp_redis_integration.md`
  - Redis schema: `docs/memory/redis_schema.md`
  - Visual schema: `docs/memory/redis_visual_schema.md`
  - Implementation summary: `docs/redis-world-model-implementation-summary.md`
- Plugins
  - Plugin development guide: `docs/plugins/development-guide.md`
- Sandbox / Daytona
  - Daytona integration guide: `docs/sandbox/daytona-integration.md`
- CLI Documentation
  - User Guide: `docs/cli/user_guide.md`
  - Command Reference: `docs/cli/command_reference.md`
  - Interactive Tutorials: `docs/cli/tutorials.md`
  - Example Library: `docs/cli/examples.md`
  - Troubleshooting Guide: `docs/cli/troubleshooting.md`
- Papers and resources
  - MSA paper guide: `docs/guides/msa-paper.md`
  - Understanding the paper: `docs/guides/understanding_the_paper.md`
  - Resources: `docs/guides/ressource.md`

## 🧪 Examples

- Gemini integration demo: `examples/gemini_integration_demo.py`
- MCP Redis example: `examples/mcp_redis_example.py`
- Redis world model integration: `examples/redis_world_model_integration_demo.py`
- MSA paper demo: `examples/msa_paper_demo.py`
- Tests overview: `tests/README.md`

## 🏗️ Architecture Overview

The Reasoning Kernel is built on a **Semantic Kernel-native architecture** with the following core components:

### Plugin Ecosystem

```mermaid
graph TB
    subgraph "Semantic Kernel Core"
        K[Kernel Instance]
        P[Planners]
        M[Memory]
        S[AI Services]
    end
    
    subgraph "Reasoning Plugins"
        AR[Abstract Reasoning]
        CR[Causal Reasoning]
        AnR[Analogical Reasoning]
        LR[Logical Reasoning]
    end
    
    subgraph "MSA Pipeline Plugins"
        PP[Parsing Plugin]
        KP[Knowledge Plugin]
        GP[Graph Plugin]
        SP[Synthesis Plugin]
        IP[Inference Plugin]
    end
    
    K --> AR
    K --> PP
    P --> SP
    M --> KP
    S --> CR
```

### MSA Reasoning Pipeline

The system implements a five-stage reasoning process:

1. **Parse**: Transform natural language into structured representations
2. **Knowledge**: Retrieve relevant background knowledge from memory
3. **Graph**: Build causal dependency graphs
4. **Synthesize**: Generate probabilistic programs (NumPyro)
5. **Inference**: Execute models and compute results

### Key Benefits

- **🔌 Modular**: Each reasoning capability as an independent SK plugin
- **🎯 Orchestrated**: SK planners handle complex reasoning workflows
- **💾 Memory-Aware**: Multi-tier memory system for context and knowledge
- **🌐 Multi-Model**: Support for Azure OpenAI, Google, and local models
- **⚡ Scalable**: Production-ready with FastAPI and async processing

## 🔧 Configuration

### Environment Variables

**Primary AI Provider - Azure OpenAI (Required currently):**

```bash
AZURE_OPENAI_API_KEY=your_azure_openai_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=your_deployment_name
AZURE_OPENAI_API_VERSION=2024-12-01-preview
```

**Optional AI Provider - Google AI (Gemini):**

```bash
GOOGLE_AI_API_KEY=your_gemini_api_key
GOOGLE_AI_GEMINI_MODEL_ID=gemini-2.5-pro
GOOGLE_AI_EMBEDDING_MODEL_ID=text-embedding-004
```

**Optional configuration:**

```bash
LOG_LEVEL=INFO                    # Logging level
LOG_FORMAT=json                   # Logging format (json or text)
MCMC_NUM_WARMUP=1000             # MCMC warmup steps
MCMC_NUM_SAMPLES=2000            # MCMC sampling steps
MAX_KNOWLEDGE_ENTITIES=50        # Max entities to extract
UNCERTAINTY_THRESHOLD=0.8        # Uncertainty reporting threshold
```

> Note: Gemini support is optional. The current kernel initialization requires Azure OpenAI credentials.

### Structured Logging

The Reasoning Kernel features comprehensive structured logging with JSON output for production environments:

#### Features

- **JSON formatted logs** with structured data for easy parsing
- **Request correlation IDs** automatically added to all requests via `X-Request-ID` header
- **Performance metrics** with request duration tracking
- **Service context** automatically added to all log entries
- **Error logging** with full context and error details

#### Logging Configuration

```bash
# Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Set log format (json for structured, text for development)
LOG_FORMAT=json
```

#### Log Structure

JSON logs include the following fields:

```json
{
  "event": "Request completed",
  "service": "reasoning-kernel", 
  "component": "request",
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "method": "POST",
  "path": "/api/v1/reason",
  "endpoint": "/api/v1/reason",
  "status_code": 200,
  "duration": 0.145,
  "timestamp": 1703875200.123,
  "level": "info"
}
```

#### Usage in Code

```python
from reasoning_kernel.core.logging_config import get_logger, performance_context

# Get a structured logger
logger = get_logger("my_component")

# Log with additional context
logger.info("Processing request", user_id="123", operation="synthesis")

# Track performance with automatic duration logging
with performance_context("model_synthesis", logger):
    # Your code here
    pass
```

## 🏗️ Architecture

The system is built on a modern, scalable architecture:

- **FastAPI**: High-performance async web framework
- **Semantic Kernel**: Microsoft's AI orchestration platform
- **NumPyro**: Probabilistic programming with JAX
- **Pydantic**: Type-safe data validation
- **JAX**: Hardware-accelerated computing
- **Redis Cloud**: Vector search and knowledge storage via MCP integration

### Third-Party Integrations

- **MCP Redis Cloud**: Vendored Model Context Protocol server for Redis Cloud integration (`third_party/mcp-redis-cloud/`)
  - Provides vector search, document storage, and caching capabilities
  - MIT licensed with preserved attribution
  - Integration wrapper at `reasoning_kernel/integrations/mcp_redis.py`

## 🧪 Development

```bash
# Install in development mode
pip install -e .

# Start with hot reload
uvicorn reasoning_kernel.main:app --host 0.0.0.0 --port 5000 --reload
```

### Code Quality

```bash
# Format code
black reasoning_kernel/
isort reasoning_kernel/

# Type checking
mypy reasoning_kernel/

# Static analysis (requires Datadog CLI)
datadog-ci sarif --config static-analysis.datadog.yml --output results.sarif
datadog-ci sarif upload --service reasoning-kernel results.sarif

# Run static analysis locally with Docker
docker run --rm -v $(pwd):/workspace \
  datadog/datadog-static-analyzer:latest \
  --config /workspace/static-analysis.datadog.yml \
  /workspace
```

## 🔍 Static Analysis

The project uses Datadog static analysis to ensure code quality and security. The configuration is defined in `static-analysis.datadog.yml` and includes:

- Python best practices and code style
- Security vulnerability detection
- Framework-specific rules (Django, Flask)
- GitHub Actions workflow validation

### Running Static Analysis Locally

#### Option 1: Using Datadog CLI (Recommended)

```bash
# Install Datadog CLI
npm install -g @datadog/datadog-ci

# Run static analysis
# Run static analysis and generate SARIF file
datadog-ci static-analysis scan --config static-analysis.datadog.yml --sarif-file results.sarif

# Upload SARIF results to Datadog
datadog-ci sarif upload --service reasoning-kernel results.sarif
```

#### Option 2: Using Docker

```bash
# Run static analysis with Docker
docker run --rm -v $(pwd):/workspace \
  datadog/datadog-static-analyzer:latest \
  --config /workspace/static-analysis.datadog.yml \
  /workspace
```

### CI/CD Integration

Static analysis runs automatically on:

- All pull requests
- Pushes to the main branch

#### Required Secrets

To enable the CI workflow, configure these GitHub repository secrets:

- `DD_APP_KEY`: Your Datadog application key
- `DD_API_KEY`: Your Datadog API key

The workflow will:

- ✅ Post results as PR comments
- ✅ Create check status for PRs  
- ❌ Block merging on critical/high severity violations
- 📊 Track metrics in Datadog dashboard

## 📊 Performance

The MSA Reasoning Engine is designed for production use:

- **Concurrent Sessions**: Handle multiple reasoning sessions simultaneously
- **Hardware Acceleration**: JAX-based computation with GPU support
- **Scalable Architecture**: Async processing with FastAPI
- **Memory Efficient**: Streaming inference and garbage collection

## 🤝 Contributing

We welcome contributions! Please see our contributing guidelines and code of conduct.

## 📄 License

This project is licensed under the Apache-2.0 License (see `pyproject.toml`).

## 🙏 Acknowledgments

- Microsoft Semantic Kernel team for the AI orchestration framework
- NumPyro/JAX teams for probabilistic programming capabilities
- The broader AI reasoning research community

---

## Built with ❤️ for advanced AI reasoning capabilities
