#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Redis Toolkit æ•ˆèƒ½æ¸¬è©¦ç¯„ä¾‹
æ¸¬è©¦å„ç¨®æ“ä½œçš„æ•ˆèƒ½è¡¨ç¾
"""

import time
import statistics
from redis_toolkit import RedisToolkit, RedisOptions


def benchmark_operation(operation_name: str, operation_func, iterations: int = 1000):
    """æ•ˆèƒ½æ¸¬è©¦è¼”åŠ©å‡½æ•¸"""
    print(f"\nğŸ“Š æ¸¬è©¦ {operation_name} ({iterations} æ¬¡æ“ä½œ)")
    
    times = []
    for i in range(iterations):
        start_time = time.perf_counter()
        operation_func(i)
        end_time = time.perf_counter()
        times.append(end_time - start_time)
    
    total_time = sum(times)
    avg_time = statistics.mean(times)
    min_time = min(times)
    max_time = max(times)
    ops_per_second = iterations / total_time
    
    print(f"  ç¸½æ™‚é–“: {total_time:.3f} ç§’")
    print(f"  å¹³å‡æ™‚é–“: {avg_time*1000:.3f} æ¯«ç§’/æ“ä½œ")
    print(f"  æœ€å¿«: {min_time*1000:.3f} æ¯«ç§’")
    print(f"  æœ€æ…¢: {max_time*1000:.3f} æ¯«ç§’")
    print(f"  ååé‡: {ops_per_second:.0f} æ“ä½œ/ç§’")
    
    return total_time, ops_per_second


def test_basic_operations():
    """æ¸¬è©¦åŸºæœ¬æ“ä½œæ•ˆèƒ½"""
    print("=== åŸºæœ¬æ“ä½œæ•ˆèƒ½æ¸¬è©¦ ===")
    
    toolkit = RedisToolkit(options=RedisOptions(is_logger_info=False))
    
    # æ¸¬è©¦å­—ä¸²æ“ä½œ
    def string_set_operation(i):
        toolkit.setter(f"string_{i}", f"æ¸¬è©¦å­—ä¸²_{i}")
    
    def string_get_operation(i):
        toolkit.getter(f"string_{i}")
    
    # æ¸¬è©¦å­—å…¸æ“ä½œ
    def dict_set_operation(i):
        toolkit.setter(f"dict_{i}", {"id": i, "åç¨±": f"é …ç›®_{i}", "å€¼": i * 10})
    
    def dict_get_operation(i):
        toolkit.getter(f"dict_{i}")
    
    # åŸ·è¡Œæ¸¬è©¦
    benchmark_operation("å­—ä¸²è¨­å®š", string_set_operation)
    benchmark_operation("å­—ä¸²å–å¾—", string_get_operation)
    benchmark_operation("å­—å…¸è¨­å®š", dict_set_operation)
    benchmark_operation("å­—å…¸å–å¾—", dict_get_operation)
    
    toolkit.cleanup()


def test_batch_operations():
    """æ¸¬è©¦æ‰¹æ¬¡æ“ä½œæ•ˆèƒ½"""
    print("\n=== æ‰¹æ¬¡æ“ä½œæ•ˆèƒ½æ¸¬è©¦ ===")
    
    toolkit = RedisToolkit(options=RedisOptions(is_logger_info=False))
    
    # æº–å‚™æ¸¬è©¦è³‡æ–™
    batch_sizes = [10, 50, 100, 500]
    
    for batch_size in batch_sizes:
        print(f"\nğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # æº–å‚™æ‰¹æ¬¡è³‡æ–™
        batch_data = {
            f"batch_key_{i}": {"ç´¢å¼•": i, "è³‡æ–™": f"æ‰¹æ¬¡è³‡æ–™_{i}"}
            for i in range(batch_size)
        }
        
        # æ¸¬è©¦æ‰¹æ¬¡è¨­å®š
        start_time = time.perf_counter()
        toolkit.batch_set(batch_data)
        batch_set_time = time.perf_counter() - start_time
        
        # æ¸¬è©¦æ‰¹æ¬¡å–å¾—
        keys = list(batch_data.keys())
        start_time = time.perf_counter()
        result = toolkit.batch_get(keys)
        batch_get_time = time.perf_counter() - start_time
        
        # æ¸¬è©¦å€‹åˆ¥æ“ä½œï¼ˆç”¨æ–¼æ¯”è¼ƒï¼‰
        start_time = time.perf_counter()
        for key, value in batch_data.items():
            toolkit.setter(f"individual_{key}", value)
        individual_set_time = time.perf_counter() - start_time
        
        individual_keys = [f"individual_{key}" for key in keys]
        start_time = time.perf_counter()
        for key in individual_keys:
            toolkit.getter(key)
        individual_get_time = time.perf_counter() - start_time
        
        # é¡¯ç¤ºçµæœ
        print(f"  æ‰¹æ¬¡è¨­å®š: {batch_set_time:.3f} ç§’ ({batch_size/batch_set_time:.0f} æ“ä½œ/ç§’)")
        print(f"  å€‹åˆ¥è¨­å®š: {individual_set_time:.3f} ç§’ ({batch_size/individual_set_time:.0f} æ“ä½œ/ç§’)")
        print(f"  æ•ˆèƒ½æå‡: {individual_set_time/batch_set_time:.1f}x")
        
        print(f"  æ‰¹æ¬¡å–å¾—: {batch_get_time:.3f} ç§’ ({batch_size/batch_get_time:.0f} æ“ä½œ/ç§’)")
        print(f"  å€‹åˆ¥å–å¾—: {individual_get_time:.3f} ç§’ ({batch_size/individual_get_time:.0f} æ“ä½œ/ç§’)")
        print(f"  æ•ˆèƒ½æå‡: {individual_get_time/batch_get_time:.1f}x")
    
    toolkit.cleanup()


def test_data_size_impact():
    """æ¸¬è©¦è³‡æ–™å¤§å°å°æ•ˆèƒ½çš„å½±éŸ¿"""
    print("\n=== è³‡æ–™å¤§å°æ•ˆèƒ½æ¸¬è©¦ ===")
    
    toolkit = RedisToolkit(options=RedisOptions(is_logger_info=False))
    
    # æ¸¬è©¦ä¸åŒå¤§å°çš„è³‡æ–™
    data_sizes = [
        ("å°å‹", "x" * 100),                    # 100 ä½å…ƒçµ„
        ("ä¸­å‹", "x" * 1000),                   # 1KB
        ("å¤§å‹", "x" * 10000),                  # 10KB
        ("è¶…å¤§å‹", "x" * 100000),               # 100KB
    ]
    
    for size_name, data in data_sizes:
        print(f"\nğŸ“ {size_name}è³‡æ–™ ({len(data)} ä½å…ƒçµ„)")
        
        # æ¸¬è©¦è¨­å®š
        start_time = time.perf_counter()
        for i in range(100):
            toolkit.setter(f"{size_name}_data_{i}", data)
        set_time = time.perf_counter() - start_time
        
        # æ¸¬è©¦å–å¾—
        start_time = time.perf_counter()
        for i in range(100):
            toolkit.getter(f"{size_name}_data_{i}")
        get_time = time.perf_counter() - start_time
        
        # è¨ˆç®—ååé‡
        data_throughput_set = (len(data) * 100) / set_time / 1024 / 1024  # MB/s
        data_throughput_get = (len(data) * 100) / get_time / 1024 / 1024  # MB/s
        
        print(f"  è¨­å®š: {set_time:.3f} ç§’ ({100/set_time:.0f} æ“ä½œ/ç§’, {data_throughput_set:.1f} MB/s)")
        print(f"  å–å¾—: {get_time:.3f} ç§’ ({100/get_time:.0f} æ“ä½œ/ç§’, {data_throughput_get:.1f} MB/s)")
    
    toolkit.cleanup()


def test_pubsub_performance():
    """æ¸¬è©¦ç™¼å¸ƒè¨‚é–±æ•ˆèƒ½"""
    print("\n=== ç™¼å¸ƒè¨‚é–±æ•ˆèƒ½æ¸¬è©¦ ===")
    
    received_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨å…§éƒ¨å‡½æ•¸ä¸­ä¿®æ”¹
    
    def message_handler(channel: str, data):
        received_count[0] += 1
    
    # å»ºç«‹è¨‚é–±è€…
    subscriber = RedisToolkit(
        channels=["æ•ˆèƒ½æ¸¬è©¦"],
        message_handler=message_handler,
        options=RedisOptions(is_logger_info=False)
    )
    
    # å»ºç«‹ç™¼å¸ƒè€…
    publisher = RedisToolkit(options=RedisOptions(is_logger_info=False))
    
    # ç­‰å¾…è¨‚é–±è€…å•Ÿå‹•
    time.sleep(0.5)
    
    # æ¸¬è©¦ç™¼å¸ƒæ•ˆèƒ½
    message_counts = [100, 500, 1000]
    
    for msg_count in message_counts:
        print(f"\nğŸ“¤ ç™¼å¸ƒ {msg_count} æ¢è¨Šæ¯")
        
        received_count[0] = 0
        test_message = {"æ¸¬è©¦": True, "ç·¨è™Ÿ": 0, "è³‡æ–™": "æ•ˆèƒ½æ¸¬è©¦è¨Šæ¯"}
        
        start_time = time.perf_counter()
        for i in range(msg_count):
            test_message["ç·¨è™Ÿ"] = i
            publisher.publisher("æ•ˆèƒ½æ¸¬è©¦", test_message)
        publish_time = time.perf_counter() - start_time
        
        # ç­‰å¾…è¨Šæ¯è™•ç†å®Œæˆ
        time.sleep(2)
        
        publish_rate = msg_count / publish_time
        receive_rate = received_count[0] / 2.0  # é™¤ä»¥ç­‰å¾…æ™‚é–“
        
        print(f"  ç™¼å¸ƒé€Ÿç‡: {publish_rate:.0f} è¨Šæ¯/ç§’")
        print(f"  æ¥æ”¶é€Ÿç‡: {receive_rate:.0f} è¨Šæ¯/ç§’")
        print(f"  è¨Šæ¯å®Œæ•´æ€§: {received_count[0]}/{msg_count} ({received_count[0]/msg_count*100:.1f}%)")
    
    # æ¸…ç†
    subscriber.cleanup()
    publisher.cleanup()


def test_retry_performance():
    """æ¸¬è©¦æ•ˆèƒ½"""
    print("\n=== æ•ˆèƒ½æ¸¬è©¦ ===")
    
    # æ¸¬è©¦ç¬¬ä¸€å€‹å¯¦ä¾‹
    toolkit_with_retry = RedisToolkit(
        options=RedisOptions(is_logger_info=False)
    )
    
    # æ¸¬è©¦ç¬¬äºŒå€‹å¯¦ä¾‹
    toolkit_without_retry = RedisToolkit(
        options=RedisOptions(is_logger_info=False)
    )
    
    test_iterations = 1000
    test_data = {"æ¸¬è©¦": "é‡è©¦æ©Ÿåˆ¶æ•ˆèƒ½", "å€¼": 12345}
    
    # æ¸¬è©¦å•Ÿç”¨é‡è©¦çš„æ•ˆèƒ½
    start_time = time.perf_counter()
    for i in range(test_iterations):
        toolkit_with_retry.setter(f"retry_test_{i}", test_data)
    with_retry_time = time.perf_counter() - start_time
    
    # æ¸¬è©¦ä¸å•Ÿç”¨é‡è©¦çš„æ•ˆèƒ½
    start_time = time.perf_counter()
    for i in range(test_iterations):
        toolkit_without_retry.setter(f"no_retry_test_{i}", test_data)
    without_retry_time = time.perf_counter() - start_time
    
    print(f"ğŸ“ˆ é‡è©¦æ©Ÿåˆ¶æ•ˆèƒ½å°æ¯” ({test_iterations} æ¬¡æ“ä½œ)")
    print(f"  å•Ÿç”¨é‡è©¦: {with_retry_time:.3f} ç§’ ({test_iterations/with_retry_time:.0f} æ“ä½œ/ç§’)")
    print(f"  ä¸å•Ÿç”¨é‡è©¦: {without_retry_time:.3f} ç§’ ({test_iterations/without_retry_time:.0f} æ“ä½œ/ç§’)")
    print(f"  æ•ˆèƒ½å·®ç•°: {with_retry_time/without_retry_time:.1f}x")
    
    toolkit_with_retry.cleanup()
    toolkit_without_retry.cleanup()


if __name__ == "__main__":
    try:
        print("ğŸƒâ€â™‚ï¸ Redis Toolkit æ•ˆèƒ½æ¸¬è©¦")
        print("=" * 50)
        
        test_basic_operations()
        test_batch_operations()
        test_data_size_impact()
        test_pubsub_performance()
        test_retry_performance()
        
        print("\n" + "=" * 50)
        print("ğŸ æ•ˆèƒ½æ¸¬è©¦å®Œæˆï¼")
        
        print("\nğŸ’¡ æ•ˆèƒ½å„ªåŒ–å»ºè­°:")
        print("  âœ… ä½¿ç”¨æ‰¹æ¬¡æ“ä½œè™•ç†å¤§é‡è³‡æ–™")
        print("  âœ… æ ¹æ“šè³‡æ–™å¤§å°é¸æ“‡åˆé©çš„æ“ä½œæ–¹å¼")
        print("  âœ… åœ¨ç©©å®šç’°å¢ƒä¸­å¯è€ƒæ…®é—œé–‰é‡è©¦æ©Ÿåˆ¶")
        print("  âœ… ç™¼å¸ƒè¨‚é–±é©åˆå³æ™‚è¨Šæ¯å‚³é")
        
    except Exception as e:
        print(f"âŒ æ•ˆèƒ½æ¸¬è©¦å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()