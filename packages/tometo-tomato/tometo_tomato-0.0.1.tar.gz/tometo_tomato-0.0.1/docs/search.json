[
  {
    "objectID": "doc_sources/project_analysis.html",
    "href": "doc_sources/project_analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This document summarizes the analysis of the tometo_tomato.py project, highlighting its strengths, weaknesses, and potential areas for improvement.\n\n\nThe tometo_tomato.py project is a well-structured and functional solution to a practical problem. It effectively uses modern data tools (DuckDB, rapidfuzz) and provides a user-friendly CLI. The recent improvements (output logic, uniqueness, handling spaces in field names) have made it more robust. The main challenge seems to be the rapidfuzz extension installation/loading, which is an external dependency issue.\nIt‚Äôs a solid foundation that can be further enhanced with more robust error handling, comprehensive testing, and potentially more advanced CSV parsing.\n\n\n\n\nClear Purpose: The project clearly addresses the problem of fuzzy joining tabular data, which is a common real-world challenge.\nLeverages Powerful Tools: Uses DuckDB for efficient data processing and rapidfuzz (or its fallbacks) for robust fuzzy matching.\nModular Design: The code is broken down into functions (parse_args, read_header, build_join_pairs, prepare_select_clauses, try_load_rapidfuzz, choose_score_expr, main), which improves readability and maintainability.\nCLI Interface: Provides a command-line interface, making it easy to use and integrate into workflows.\nError Handling (Fuzzy Functions): Includes logic to try and install rapidfuzz and fall back to built-in Levenshtein/Damerau-Levenshtein functions if rapidfuzz is unavailable.\nOutput Control: Allows specifying output files for clean and ambiguous matches, and now conditionally generates the ambiguous output.\nUniqueness Handling: Implements logic to ensure unique output records based on join fields.\nHandles Field Names with Spaces: The recent fix ensures it can handle column names with spaces.\nClear Comments: The code has good comments explaining the purpose of functions and complex logic.\n\n\n\n\n\nrapidfuzz Installation/Loading Robustness:\n\nThe HTTP 403 error when installing rapidfuzz from within the Python script is a recurring issue. While a manual workaround exists, the script‚Äôs try_load_rapidfuzz could be more robust. It might be beneficial to:\n\nProvide clearer instructions on how to pre-install extensions if the automatic method fails.\nConsider using duckdb.install_extension() and duckdb.load_extension() directly, and perhaps catching more specific exceptions.\nIf the HTTP 403 is a persistent issue, consider bundling the extension or providing an alternative download mechanism.\n\n\nError Handling (General):\n\nThe script exits with sys.exit(1) on some errors (e.g., no join pair found, no fuzzy function available). While functional, a more structured error handling (e.g., custom exceptions) could make the script more robust for integration into larger systems.\n\nInput/Reference File Handling:\n\nThe read_header function is a bit simplistic (naive split on comma). While it works for simple CSVs, it might break for CSVs with commas within quoted fields. Using Python‚Äôs csv module for reading headers would be more robust.\nThe script assumes header=true and all_varchar=true for read_csv_auto. While common, making these configurable might add flexibility.\n\nmain Function Complexity:\n\nThe main function is quite long and performs many different tasks (argument parsing, join pair building, select clause preparation, DuckDB connection, SQL execution, file post-processing, printing messages). Breaking it down into smaller, more focused functions could improve readability and maintainability. For example, a separate function for ‚Äúexecute_fuzzy_join_sql‚Äù or ‚Äúhandle_output_files‚Äù.\n\nTestability:\n\nThe script currently lacks unit tests. Adding unit tests for functions like build_join_pairs, prepare_select_clauses, and choose_score_expr would significantly improve code quality and prevent regressions.\n\nCLI Argument Defaults:\n\nThe default=None for --output-ambiguous is good. Consider if other arguments could benefit from more explicit defaults or validation.\n\nDocstrings and Type Hinting:\n\nThe functions have type hints, which is excellent. Ensuring comprehensive docstrings for all functions would further improve clarity.\n\n\n\n\n\n\nStrengths ‚Ä¢ Clear problem focus ‚Äì fuzzy CSV joins ‚Äì with a pragmatic tech-stack (DuckDB + rapidfuzz). ‚Ä¢ Good documentation: README, PRD and extra markdown files give context, usage examples, and Italian + English coverage. ‚Ä¢ Tests already present (tests/test_core.py) and CI workflow is configured. ‚Ä¢ Single-file Python implementation keeps the learning curve low and is easy to inspect. ‚Ä¢ Cross-platform: pure-Python, no compiled extensions required.\nAreas for improvement & concrete suggestions\n3 Code organisation ‚Äì Split the 400-line tometo_tomato.py into logical modules: ‚Ä¢ cli.py for argparse / typer ‚Ä¢ engine.py for DuckDB logic ‚Ä¢ io_utils.py for CSV/header helpers This boosts readability and unit-test granularity. ‚Äì Add type hints throughout and run mypy in CI. 4 Performance & memory ‚Äì For very large CSVs you materialise full cross-joins in DuckDB; memory can explode. Investigate DUCKDB_DISABLE_OBJECT_CACHE, chunked processing, or blocking with LIMIT. ‚Äì Consider writing scores into a temp table with ORDER BY/LIMIT 1 instead of ROW_NUMBER() if memory becomes a problem. 5 Robustness ‚Äì At runtime, call sql_safe_identifier or double-quote column names produced from user input to prevent SQL-injection or invalid identifiers (edge-cases with spaces, quotes, UTF-8). ‚Äì Validate that ‚Äìoutput-clean and ‚Äìoutput-ambiguous don‚Äôt overwrite one of the input files. 6 Logging & UX\n‚Äì Emit a JSON or TSV summary line for easy downstream parsing (records processed, matches, ambiguous count, elapsed time).\n7 Testing ‚Äì Expand tests to cover: ‚Ä¢ ‚Äìinfer-pairs with threshold edge-cases. ‚Ä¢ Large file sampling (property-based tests with hypothesis). ‚Ä¢ Failure paths (missing columns, unreadable file). ‚Äì Add linting (ruff, black) and static-type checks to CI. 8 Documentation ‚Äì Move command-line examples into docs/usage.md and wire up mkdocs or sphinx for a browsable site. ‚Äì Provide a benchmark section comparing execution time with and without rapidfuzz.\nNext steps If you‚Äôd like to apply any of these suggestions, let me know which direction you prefer. I will then tell you exactly which files are most likely to need edits so you can add them to the chat."
  },
  {
    "objectID": "doc_sources/project_analysis.html#project-analysis-tometo_tomato.py",
    "href": "doc_sources/project_analysis.html#project-analysis-tometo_tomato.py",
    "title": "Analysis",
    "section": "",
    "text": "This document summarizes the analysis of the tometo_tomato.py project, highlighting its strengths, weaknesses, and potential areas for improvement.\n\n\nThe tometo_tomato.py project is a well-structured and functional solution to a practical problem. It effectively uses modern data tools (DuckDB, rapidfuzz) and provides a user-friendly CLI. The recent improvements (output logic, uniqueness, handling spaces in field names) have made it more robust. The main challenge seems to be the rapidfuzz extension installation/loading, which is an external dependency issue.\nIt‚Äôs a solid foundation that can be further enhanced with more robust error handling, comprehensive testing, and potentially more advanced CSV parsing.\n\n\n\n\nClear Purpose: The project clearly addresses the problem of fuzzy joining tabular data, which is a common real-world challenge.\nLeverages Powerful Tools: Uses DuckDB for efficient data processing and rapidfuzz (or its fallbacks) for robust fuzzy matching.\nModular Design: The code is broken down into functions (parse_args, read_header, build_join_pairs, prepare_select_clauses, try_load_rapidfuzz, choose_score_expr, main), which improves readability and maintainability.\nCLI Interface: Provides a command-line interface, making it easy to use and integrate into workflows.\nError Handling (Fuzzy Functions): Includes logic to try and install rapidfuzz and fall back to built-in Levenshtein/Damerau-Levenshtein functions if rapidfuzz is unavailable.\nOutput Control: Allows specifying output files for clean and ambiguous matches, and now conditionally generates the ambiguous output.\nUniqueness Handling: Implements logic to ensure unique output records based on join fields.\nHandles Field Names with Spaces: The recent fix ensures it can handle column names with spaces.\nClear Comments: The code has good comments explaining the purpose of functions and complex logic.\n\n\n\n\n\nrapidfuzz Installation/Loading Robustness:\n\nThe HTTP 403 error when installing rapidfuzz from within the Python script is a recurring issue. While a manual workaround exists, the script‚Äôs try_load_rapidfuzz could be more robust. It might be beneficial to:\n\nProvide clearer instructions on how to pre-install extensions if the automatic method fails.\nConsider using duckdb.install_extension() and duckdb.load_extension() directly, and perhaps catching more specific exceptions.\nIf the HTTP 403 is a persistent issue, consider bundling the extension or providing an alternative download mechanism.\n\n\nError Handling (General):\n\nThe script exits with sys.exit(1) on some errors (e.g., no join pair found, no fuzzy function available). While functional, a more structured error handling (e.g., custom exceptions) could make the script more robust for integration into larger systems.\n\nInput/Reference File Handling:\n\nThe read_header function is a bit simplistic (naive split on comma). While it works for simple CSVs, it might break for CSVs with commas within quoted fields. Using Python‚Äôs csv module for reading headers would be more robust.\nThe script assumes header=true and all_varchar=true for read_csv_auto. While common, making these configurable might add flexibility.\n\nmain Function Complexity:\n\nThe main function is quite long and performs many different tasks (argument parsing, join pair building, select clause preparation, DuckDB connection, SQL execution, file post-processing, printing messages). Breaking it down into smaller, more focused functions could improve readability and maintainability. For example, a separate function for ‚Äúexecute_fuzzy_join_sql‚Äù or ‚Äúhandle_output_files‚Äù.\n\nTestability:\n\nThe script currently lacks unit tests. Adding unit tests for functions like build_join_pairs, prepare_select_clauses, and choose_score_expr would significantly improve code quality and prevent regressions.\n\nCLI Argument Defaults:\n\nThe default=None for --output-ambiguous is good. Consider if other arguments could benefit from more explicit defaults or validation.\n\nDocstrings and Type Hinting:\n\nThe functions have type hints, which is excellent. Ensuring comprehensive docstrings for all functions would further improve clarity."
  },
  {
    "objectID": "doc_sources/project_analysis.html#o3",
    "href": "doc_sources/project_analysis.html#o3",
    "title": "Analysis",
    "section": "",
    "text": "Strengths ‚Ä¢ Clear problem focus ‚Äì fuzzy CSV joins ‚Äì with a pragmatic tech-stack (DuckDB + rapidfuzz). ‚Ä¢ Good documentation: README, PRD and extra markdown files give context, usage examples, and Italian + English coverage. ‚Ä¢ Tests already present (tests/test_core.py) and CI workflow is configured. ‚Ä¢ Single-file Python implementation keeps the learning curve low and is easy to inspect. ‚Ä¢ Cross-platform: pure-Python, no compiled extensions required.\nAreas for improvement & concrete suggestions\n3 Code organisation ‚Äì Split the 400-line tometo_tomato.py into logical modules: ‚Ä¢ cli.py for argparse / typer ‚Ä¢ engine.py for DuckDB logic ‚Ä¢ io_utils.py for CSV/header helpers This boosts readability and unit-test granularity. ‚Äì Add type hints throughout and run mypy in CI. 4 Performance & memory ‚Äì For very large CSVs you materialise full cross-joins in DuckDB; memory can explode. Investigate DUCKDB_DISABLE_OBJECT_CACHE, chunked processing, or blocking with LIMIT. ‚Äì Consider writing scores into a temp table with ORDER BY/LIMIT 1 instead of ROW_NUMBER() if memory becomes a problem. 5 Robustness ‚Äì At runtime, call sql_safe_identifier or double-quote column names produced from user input to prevent SQL-injection or invalid identifiers (edge-cases with spaces, quotes, UTF-8). ‚Äì Validate that ‚Äìoutput-clean and ‚Äìoutput-ambiguous don‚Äôt overwrite one of the input files. 6 Logging & UX\n‚Äì Emit a JSON or TSV summary line for easy downstream parsing (records processed, matches, ambiguous count, elapsed time).\n7 Testing ‚Äì Expand tests to cover: ‚Ä¢ ‚Äìinfer-pairs with threshold edge-cases. ‚Ä¢ Large file sampling (property-based tests with hypothesis). ‚Ä¢ Failure paths (missing columns, unreadable file). ‚Äì Add linting (ruff, black) and static-type checks to CI. 8 Documentation ‚Äì Move command-line examples into docs/usage.md and wire up mkdocs or sphinx for a browsable site. ‚Äì Provide a benchmark section comparing execution time with and without rapidfuzz.\nNext steps If you‚Äôd like to apply any of these suggestions, let me know which direction you prefer. I will then tell you exactly which files are most likely to need edits so you can add them to the chat."
  },
  {
    "objectID": "doc_sources/RELEASING.html",
    "href": "doc_sources/RELEASING.html",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "This document outlines the process for releasing and versioning the tometo_tomato project, leveraging Git tags and setuptools_scm for automatic version management.\n\n\ntometo_tomato uses setuptools_scm to automatically determine its version based on Git tags. This means you do not need to manually update version numbers in pyproject.toml or setup.py. The version is derived directly from your Git history.\nThe version format typically follows Semantic Versioning: MAJOR.MINOR.PATCH.\n\nMAJOR: Incremented for incompatible API changes.\nMINOR: Incremented for adding functionality in a backward-compatible manner.\nPATCH: Incremented for backward-compatible bug fixes.\n\nsetuptools_scm also handles development versions: - X.Y.Z.devN: Indicates a development version, N commits after tag X.Y.Z. - +dirty: Appended if there are uncommitted changes in your working directory.\n\n\n\nFollow these steps to create a new release:\n\nEnsure Your Changes Are Committed: Before creating a release tag, make sure all the changes you want to include in the release are committed to your main branch (or the branch you are releasing from).\ngit status\ngit add .\ngit commit -m \"feat: Prepare for vX.Y.Z release\" # Or appropriate commit message\nCreate a Git Tag: Create an annotated Git tag for the new version. This tag will be used by setuptools_scm to determine the release version.\ngit tag -a vX.Y.Z -m \"Release vX.Y.Z\"\nReplace X.Y.Z with the actual version number (e.g., v1.0.0).\nPush the Tag to GitHub: Push the newly created tag to your GitHub repository.\ngit push origin vX.Y.Z\nYou might also want to push all commits if you haven‚Äôt already:\ngit push origin main\nGitHub Actions (Optional but Recommended): If your repository is configured with GitHub Actions for releases (e.g., a workflow that triggers on new tags), pushing the tag will automatically trigger the build, testing, and release process (e.g., publishing to PyPI, creating a GitHub Release).\nVerify the Version: After the release process is complete (and if you‚Äôve installed the new version), you can verify the version of the tometo_tomato tool:\npython3 src/tometo_tomato.py --version\nThis should output the exact version you tagged (e.g., tometo_tomato.py vX.Y.Z).\n\n\n\n\nDuring development, setuptools_scm will automatically generate a version string that reflects the current state of your repository. For example, if your last tag was v0.1.0 and you have made 5 commits since then, your development version might look like 0.1.0.dev5. If you have uncommitted changes, it will append +dirty (e.g., 0.1.0.dev5+dirty).\nThis provides immediate feedback on the exact code state you are working with, which is invaluable for debugging and collaboration."
  },
  {
    "objectID": "doc_sources/RELEASING.html#versioning-strategy",
    "href": "doc_sources/RELEASING.html#versioning-strategy",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "tometo_tomato uses setuptools_scm to automatically determine its version based on Git tags. This means you do not need to manually update version numbers in pyproject.toml or setup.py. The version is derived directly from your Git history.\nThe version format typically follows Semantic Versioning: MAJOR.MINOR.PATCH.\n\nMAJOR: Incremented for incompatible API changes.\nMINOR: Incremented for adding functionality in a backward-compatible manner.\nPATCH: Incremented for backward-compatible bug fixes.\n\nsetuptools_scm also handles development versions: - X.Y.Z.devN: Indicates a development version, N commits after tag X.Y.Z. - +dirty: Appended if there are uncommitted changes in your working directory."
  },
  {
    "objectID": "doc_sources/RELEASING.html#release-process",
    "href": "doc_sources/RELEASING.html#release-process",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "Follow these steps to create a new release:\n\nEnsure Your Changes Are Committed: Before creating a release tag, make sure all the changes you want to include in the release are committed to your main branch (or the branch you are releasing from).\ngit status\ngit add .\ngit commit -m \"feat: Prepare for vX.Y.Z release\" # Or appropriate commit message\nCreate a Git Tag: Create an annotated Git tag for the new version. This tag will be used by setuptools_scm to determine the release version.\ngit tag -a vX.Y.Z -m \"Release vX.Y.Z\"\nReplace X.Y.Z with the actual version number (e.g., v1.0.0).\nPush the Tag to GitHub: Push the newly created tag to your GitHub repository.\ngit push origin vX.Y.Z\nYou might also want to push all commits if you haven‚Äôt already:\ngit push origin main\nGitHub Actions (Optional but Recommended): If your repository is configured with GitHub Actions for releases (e.g., a workflow that triggers on new tags), pushing the tag will automatically trigger the build, testing, and release process (e.g., publishing to PyPI, creating a GitHub Release).\nVerify the Version: After the release process is complete (and if you‚Äôve installed the new version), you can verify the version of the tometo_tomato tool:\npython3 src/tometo_tomato.py --version\nThis should output the exact version you tagged (e.g., tometo_tomato.py vX.Y.Z)."
  },
  {
    "objectID": "doc_sources/RELEASING.html#development-workflow",
    "href": "doc_sources/RELEASING.html#development-workflow",
    "title": "Releasing and Versioning tometo_tomato",
    "section": "",
    "text": "During development, setuptools_scm will automatically generate a version string that reflects the current state of your repository. For example, if your last tag was v0.1.0 and you have made 5 commits since then, your development version might look like 0.1.0.dev5. If you have uncommitted changes, it will append +dirty (e.g., 0.1.0.dev5+dirty).\nThis provides immediate feedback on the exact code state you are working with, which is invaluable for debugging and collaboration."
  },
  {
    "objectID": "doc_sources/introduction.html",
    "href": "doc_sources/introduction.html",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "",
    "text": "Chiunque lavori con i dati molto spesso si imbatte in colonne che dovrebbero rispettare una codifica, un valore standard, una lista di valori noti, ma presentano invece errori di battitura, spazi in eccesso, caratteri speciali al posto di caratteri accentati, maiuscole/minuscole non coerenti, ecc..\nQui sotto ad esempio dei nomi ci citt√† italiane, riportati in modo errato:\n\nEsempi di nomi di citt√† errati\n\n\n\n\n\n\ncity\ntipo di errore\n\n\n\n\nCefalu‚Äô\nbisognerebbe usare la √π e non u'\n\n\nReggio Calabria\nIl nome corretto √® Reggio di Calabria\n\n\nRODENGO-SAIANO\nbisognerebbe usare Rodengo Saiano, senza - e non tutto maiuscolo\n\n\n\n Se volessi associare a queste citt√† il codice1 che Istat - l‚Äôistituto nazionale di statistica - assegna a ciascuna citt√†, non potrei farlo con una semplice operazione di join, perch√© i nomi non corrispondono esattamente. Associare un codice a ciascun comune √® operazione molto importante perch√© mi consente di unire dati provenienti da fonti diverse, che altrimenti non potrei confrontare, ma anche di generare ad esempio automaticamente mappe, perch√© i software spesso usano proprio questi codici per identificare i comuni.\n\n\n\n\n\n\nPay Attention\n\n\n\nChi fa didattica sui dati di solito infatti dice (urlando): CODES, NOT LABELS!!\n\n\nI codici correlati alle unit√† amministrative italiane sono pubblici e liberamente accessibili in CC-BY-4.0 su diverse sezioni del sito Istat. Uno di questi √® il SITUAS, in cui c‚Äô√® la pagina con l‚Äô‚ÄúElenco dei codici e delle denominazioni delle unit√† territoriali‚Äù, scaricabili in formato CSV e JSON.\n\nEsempio di dati ufficiali Istat sui comuni italiani\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017163\n\n\n\n Se provassi a fare un join tra i dati errati e questi dati ufficiali, non otterrei alcun risultato.\n\nüëâ Tom√©to Tomato √® stato creato per risolvere questo problema: consente di fare comodamente join non basati su corrispondenze esatte, ma su corrispondenze ‚Äúsimili‚Äù, e arricchire, modificare, correggere la propria brutta tabella di input, confrontandola con una tabella di riferimento.\n\n\n\nQueste sono le nostre due tabelle di esempio, disponibili come input.csv e ref_sample.csv in modo che sia possibile scaricarle e provare a eseguire gli esempi."
  },
  {
    "objectID": "doc_sources/introduction.html#introduzione",
    "href": "doc_sources/introduction.html#introduzione",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "",
    "text": "Chiunque lavori con i dati molto spesso si imbatte in colonne che dovrebbero rispettare una codifica, un valore standard, una lista di valori noti, ma presentano invece errori di battitura, spazi in eccesso, caratteri speciali al posto di caratteri accentati, maiuscole/minuscole non coerenti, ecc..\nQui sotto ad esempio dei nomi ci citt√† italiane, riportati in modo errato:\n\nEsempi di nomi di citt√† errati\n\n\n\n\n\n\ncity\ntipo di errore\n\n\n\n\nCefalu‚Äô\nbisognerebbe usare la √π e non u'\n\n\nReggio Calabria\nIl nome corretto √® Reggio di Calabria\n\n\nRODENGO-SAIANO\nbisognerebbe usare Rodengo Saiano, senza - e non tutto maiuscolo\n\n\n\n Se volessi associare a queste citt√† il codice1 che Istat - l‚Äôistituto nazionale di statistica - assegna a ciascuna citt√†, non potrei farlo con una semplice operazione di join, perch√© i nomi non corrispondono esattamente. Associare un codice a ciascun comune √® operazione molto importante perch√© mi consente di unire dati provenienti da fonti diverse, che altrimenti non potrei confrontare, ma anche di generare ad esempio automaticamente mappe, perch√© i software spesso usano proprio questi codici per identificare i comuni.\n\n\n\n\n\n\nPay Attention\n\n\n\nChi fa didattica sui dati di solito infatti dice (urlando): CODES, NOT LABELS!!\n\n\nI codici correlati alle unit√† amministrative italiane sono pubblici e liberamente accessibili in CC-BY-4.0 su diverse sezioni del sito Istat. Uno di questi √® il SITUAS, in cui c‚Äô√® la pagina con l‚Äô‚ÄúElenco dei codici e delle denominazioni delle unit√† territoriali‚Äù, scaricabili in formato CSV e JSON.\n\nEsempio di dati ufficiali Istat sui comuni italiani\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017163\n\n\n\n Se provassi a fare un join tra i dati errati e questi dati ufficiali, non otterrei alcun risultato.\n\nüëâ Tom√©to Tomato √® stato creato per risolvere questo problema: consente di fare comodamente join non basati su corrispondenze esatte, ma su corrispondenze ‚Äúsimili‚Äù, e arricchire, modificare, correggere la propria brutta tabella di input, confrontandola con una tabella di riferimento.\n\n\n\nQueste sono le nostre due tabelle di esempio, disponibili come input.csv e ref_sample.csv in modo che sia possibile scaricarle e provare a eseguire gli esempi."
  },
  {
    "objectID": "doc_sources/introduction.html#utilizzare-sql",
    "href": "doc_sources/introduction.html#utilizzare-sql",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "Utilizzare SQL",
    "text": "Utilizzare SQL\n\nFare il JOIN\nIl primo test √® quello di lanciare una semplice query SQL di join, per vedere cosa succede a partire dai nostri dati di esempio.\n\n\n\nThe raw data\n\n\ncity\nregion\n\n\n\n\nCefalu‚Äô\nSicilia\n\n\nReggio Calabria\nCALABRIA\n\n\nRODENGO-SAIANO\nLombardia\n\n\n\n\n\n\n\nThe reference data\n\n\ncity\nregion\ncity_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017164\n\n\n\n\n\n La query pu√≤ essere quella di sotto. √à impostata come un LEFT JOIN, in modo da mostrare tutte le righe della tabella di sinistra, anche se non trovano corrispondenza nella tabella di destra.\nSELECT\n  i.*,\n  r.city_code\nFROM read_csv_auto('input.csv') AS i\nLEFT JOIN read_csv_auto('ref_sample.csv') AS r\n  ON i.city = r.city AND i.region = r.region\n\n\n\n\n\n\nNota\n\n\n\nNota: l‚Äôuso di read_csv_auto nella query di sopra √® una funzionalit√† di DuckDB che consente di leggere direttamente file CSV senza doverli importare prima in una tabella. In questo modo √® possibile fare esperimenti veloci senza dover creare tabelle temporanee.\n\n\nIn output otteniamo un pessimo risultato: nessuna delle righe della tabella di sinistra trova corrispondenza nella tabella di destra, e quindi il campo city_code risulta sempre NULL.\n\nRisultato di un join tra dati errati e dati ufficiali\n\n\ncity\nregion\nistat_city_code\n\n\n\n\nCefalu‚Äô\nSicilia\nNULL\n\n\nReggio Calabria\nCALABRIA\nNULL\n\n\nRODENGO-SAIANO\nLombardia\nNULL\n\n\n\n\n\nFare il JOIN ‚Äúfuzzy‚Äù\nUn JOIN ‚Äúfuzzy‚Äù, sfumato, √® quello che consente di trovare corrispondenze anche quando i valori non sono esattamente uguali, ma ‚Äúsimili‚Äù. Ad esempio, potremmo voler considerare come corrispondenti i nomi di citt√† che differiscono per un solo carattere, o che hanno una distanza di Levenshtein (numero di operazioni necessarie per trasformare una stringa in un‚Äôaltra) inferiore a una certa soglia.\nPotremmo riscrivere la query di join precedente in questo modo, usando la funzione levenshtein di DuckDB per il campo city in modo da considererare come corrispondenti i nomi di citt√† che differiscono per al massimo 2 caratteri:\nSELECT\n  i.city AS input_city,\n  i.region AS input_region,\n  r.city AS ref_city,\n  r.region AS ref_region,\n  r.city_code,\n  levenshtein(i.city, r.city) AS levenshtein_distance\nFROM read_csv_auto('input.csv') AS i\nJOIN read_csv_auto('ref_sample.csv') AS r\n  ON i.region = r.region\n  AND levenshtein(i.city, r.city) &lt;= 2;\n\nRisultato di un join fuzzy tra dati errati e dati ufficiali\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\nlevenshtein_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n\n\n\nL‚Äôunica citt√† di cui c‚Äô√® un risulato √® soltanto Cefalu', perch√© la distanza di Levenshtein tra Cefalu' e Cefal√π √® 2: sostituzione di u' con √π e rimozione di '.\nSe aumentiamo la soglia a 10, non cambia nulla, perch√© ad esempio il Comune di Rodengo Saiano √® scritto in maiuscolo e con il trattino e la distanza tra RODENGO-SAIANO e Rodengo Saiano √® pari a 12:\nSELECT levenshtein('RODENGO-SAIANO', 'Rodengo Saiano') AS distance;\n\n12\nSe si imposta la soglia a 15 ne otteniamo in ogni caso soltanto 2, perch√© il JOIN del campo region non trova corrispondenza tra CALABRIA e Calabria. Quindi dovremmo usare una funzione di distanza tra stringhe anche per il campo region:\nSELECT\n  i.city AS input_city,\n  i.region AS input_region,\n  r.city AS ref_city,\n  r.region AS ref_region,\n  r.city_code,\n  levenshtein(i.city, r.city) AS levenshtein_distance\nFROM read_csv_auto('input.csv') AS i\nJOIN read_csv_auto('ref_sample.csv') AS r\n  ON levenshtein(i.city, r.city) &lt;= 15\n  AND levenshtein(LOWER(i.region), LOWER(r.region)) &lt; 10\nMa √® l‚Äôoutput non √® proprio quello che ci aspettiamo, non 3 righe in totale (una per ogni Comune), ma ben 8 righe:\n\nRisultato di un join fuzzy tra dati errati e dati ufficiali\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\nlevenshtein_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n\n\nCefalu‚Äô\nSicilia\nReggio di Calabria\nCalabria\n080063\n15\n\n\nCefalu‚Äô\nSicilia\nRodengo Saiano\nLombardia\n017163\n12\n\n\nReggio Calabria\nCALABRIA\nCefal√π\nSicilia\n082027\n12\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n3\n\n\nReggio Calabria\nCALABRIA\nRodengo Saiano\nLombardia\n017163\n10\n\n\nRODENGO-SAIANO\nLombardia\nCefal√π\nSicilia\n082027\n14\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017163\n12\n\n\n\nQuando si esegue un JOIN esatto, l‚Äôobiettivo √® trovare una singola e chiara corrispondenza per ogni riga. Nel mondo del ‚Äúfuzzy matching‚Äù, le regole cambiano. Abbassando le nostre pretese con soglie di distanza permissive, non stiamo pi√π chiedendo al database ‚Äútrova l‚Äôunica corrispondenza giusta‚Äù, ma piuttosto:\n\nPer ogni riga del mio input, trovami tutte le righe nel file di riferimento che soddisfano questi criteri di somiglianza generici.\n\nSe una riga di input √® ‚Äúvagamente simile‚Äù a pi√π righe di riferimento, il database creer√† una riga di output per ciascuna di queste coincidenze. Questo effetto di moltiplicazione √® noto come prodotto cartesiano delle coincidenze.\nDopo aver generato le possibili corrispondenze, dovremmo filtrarle per tenere solo la migliore per ogni record di partenza. Il processo pu√≤ essere suddiviso in tre fasi:\n\ntrovare tutte le possibili corrispondenze e calcolare le distanze\nassegnare un rango a ciascuna corrispondenza in base alla somma delle distanze\nselezionare solo la corrispondenza con il rango pi√π alto (cio√® la pi√π simile)\n\n-- Fase 1: trova tutti i candidati e calcola le distanze\nWITH all_matches AS (\n    SELECT\n        i.city AS input_city,\n        i.region AS input_region,\n        r.city AS ref_city,\n        r.region AS ref_region,\n        r.city_code,\n        levenshtein(i.city, r.city) AS city_distance,\n        levenshtein(i.region, r.region) AS region_distance\n    FROM read_csv_auto('input.csv') AS i\n    JOIN read_csv_auto('ref_sample.csv') AS r\n      ON levenshtein(i.city, r.city) &lt;= 15\n     AND levenshtein(i.region, r.region) &lt; 10\n),\n-- Fase 2: assegna un rango ai candidati\nranked_matches AS (\n    SELECT\n        *,\n        ROW_NUMBER() OVER (\n            PARTITION BY input_city, input_region\n            ORDER BY (city_distance + region_distance) ASC\n        ) AS rn\n    FROM all_matches\n)\n-- Fase 3: prendi solo il miglior candidato (rn = 1)\nSELECT\n    input_city,\n    input_region,\n    ref_city,\n    ref_region,\n    city_code,\n    city_distance,\n    region_distance\nFROM ranked_matches\nWHERE rn = 1;\n\nRisultato di un join fuzzy, con selezione della migliore corrispondenza\n\n\n\n\n\n\n\n\n\n\n\ninput_city\ninput_region\nref_city\nref_region\ncity_code\ncity_distance\nregion_distance\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n2\n0\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017163\n12\n0\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n3\n7\n\n\n\n\n\nUna sintesi per ridurre le distanze\nA seguira una tabellina di riepilogo, sull‚Äôesempio del nome del Comune di Forza¬∑d'Agr√≤, scritto per√≤ in modo errato Forza¬∑¬∑D‚ÄôAgro¬∑:\n\nc‚Äô√® la D maiuscola invece che minuscola;\nci sono due spazi in eccesso (uno tra Forza e D'Agro e uno alla fine);\nnon c‚Äô√® la o accentata, ma una o normale;\nc‚Äô√® un apostrofo tipografico ‚Äô, invece di uno dritto '.\n\n\n\n\n\n\n\nNota\n\n\n\nNota: qui sopra sono stati resi visibili gli spazi in eccesso con il carattere ¬∑.\n\n\n\n\n\nRiduzione della distanza con passaggi successivi di normalizzazione\n\n\n\n\n\n\n\n\n\ndescrizione\nleft\nright\ndistanza\nfunzione aggiunta\n\n\n\n\nInizio\nForza¬∑¬∑D‚ÄôAgro¬∑\nForza¬∑d'Agr√≤\n7\n\n\n\nIn minuscolo\nforza¬∑¬∑d‚Äôagro¬∑\nforza¬∑d'agr√≤\n6\nLOWER('value')\n\n\nRimossi spazi ridondanti\nforza¬∑d‚Äôagro\nforza¬∑d'agr√≤\n5\nregexp_replace(trim(LOWER('value')), '\\s+', ' ')\n\n\nRimossi accentati\nforza¬∑d‚Äôagro\nforza¬∑d'agro\n3\nstrip_accents(regexp_replace(trim(LOWER('value')), '\\s+', ' '))\n\n\nRimossi caratteri speciali\nforza¬∑dagro\nforza¬∑dagro\n0\nregexp_replace(strip_accents(regexp_replace(trim(LOWER('value')), '\\s+', ' ')), '[^a-zA-Z0-9 ]', '', 'g')\n\n\n\n\n\n üëâ Quindi un caso molto brutto, pieno di errori, pu√≤ essere normalizzato in modo da ridurre la distanza di Levenshtein a zero, e quindi trovare la corrispondenza esatta e Forza¬∑¬∑D'Agro¬∑ √® uguale a Forza¬∑d'Agr√≤ e quindi riesco a ricavarne il codice identificativo."
  },
  {
    "objectID": "doc_sources/introduction.html#note-finali",
    "href": "doc_sources/introduction.html#note-finali",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "Note finali",
    "text": "Note finali\nIn questo percorso abbiamo descritto alcuni elementi di base per misurare la ‚Äúdistanza‚Äù tra le stringhe e a normalizzare il testo per rendere il confronto pi√π efficace. Abbiamo visto come gestire maiuscole/minuscole, spazi superflui, accenti e caratteri speciali, che sono gli elementi di base di ogni processo di pulizia dei dati.\nTuttavia, questo √® solo l‚Äôinizio. Il mondo del ‚Äúfuzzy matching‚Äù √® molto pi√π vasto e complesso. Per affrontare dataset pi√π grandi e ‚Äúdisordinati‚Äù, si ricorre spesso a metodi pi√π sofisticati, che vanno oltre il semplice conteggio delle modifiche:\n\nAlgoritmi fonetici: Invece di guardare come sono scritte le parole, questi algoritmi le codificano in base a come suonano. Metodi come Metaphone o Soundex sono bravissimi a capire che ‚ÄúSmith‚Äù e ‚ÄúSmythe‚Äù sono probabilmente la stessa cosa.\nModelli statistici (n-gram): Questi metodi scompongono le stringhe in piccole parti (es. coppie o triplette di caratteri) e ne confrontano la frequenza, risultando molto efficaci nel trovare somiglianze anche quando l‚Äôordine delle parole √® diverso.\n\nStrumenti di pulizia dati come OpenRefine integrano decine di questi algoritmi avanzati, permettendo di raggruppare e correggere dati simili con grande efficacia."
  },
  {
    "objectID": "doc_sources/introduction.html#footnotes",
    "href": "doc_sources/introduction.html#footnotes",
    "title": "Perch√© √® stato creato Tom√©to Tomato",
    "section": "Note",
    "text": "Note\n\n\nCodes of Italian municipalities‚Ü©Ô∏é"
  },
  {
    "objectID": "doc_sources/how-to.html",
    "href": "doc_sources/how-to.html",
    "title": "Come usare Tom√©to Tomato",
    "section": "",
    "text": "tometo_tomato √® un‚Äôutilit√† a riga di comando progettata per eseguire ‚Äúfuzzy join‚Äù tra due file CSV. In altre parole, permette di collegare due tabelle basandosi su colonne i cui valori non sono identici, ma semplicemente ‚Äúsimili‚Äù, gestendo errori di battitura, abbreviazioni o differenze di formattazione."
  },
  {
    "objectID": "doc_sources/how-to.html#introduzione",
    "href": "doc_sources/how-to.html#introduzione",
    "title": "Come usare Tom√©to Tomato",
    "section": "",
    "text": "tometo_tomato √® un‚Äôutilit√† a riga di comando progettata per eseguire ‚Äúfuzzy join‚Äù tra due file CSV. In altre parole, permette di collegare due tabelle basandosi su colonne i cui valori non sono identici, ma semplicemente ‚Äúsimili‚Äù, gestendo errori di battitura, abbreviazioni o differenze di formattazione."
  },
  {
    "objectID": "doc_sources/how-to.html#il-concetto-di-base",
    "href": "doc_sources/how-to.html#il-concetto-di-base",
    "title": "Come usare Tom√©to Tomato",
    "section": "Il Concetto di Base",
    "text": "Il Concetto di Base\nL‚Äôidea √® semplice: 1. Hai un file di input con dati ‚Äúsporchi‚Äù o incompleti. 2. Hai un file di riferimento (anagrafica) che contiene i dati ‚Äúpuliti‚Äù e corretti. 3. Vuoi arricchire il file di input aggiungendo colonne prese dal file di riferimento, trovando le corrispondenze migliori anche quando non sono perfette."
  },
  {
    "objectID": "doc_sources/how-to.html#i-nostri-file-di-esempio",
    "href": "doc_sources/how-to.html#i-nostri-file-di-esempio",
    "title": "Come usare Tom√©to Tomato",
    "section": "I Nostri File di Esempio",
    "text": "I Nostri File di Esempio\nPer questa guida, useremo due file:\n1. File di input (docs/files/input.csv)\nUn file con un elenco di comuni e regioni, ma con nomi imprecisi e problemi di case.\ncity,region\nCefalu',Sicilia\nReggio Calabria,CALABRIA\nRODENGO-SAIANO,Lombardia\n2. File di riferimento (docs/files/ref.csv)\nUn‚Äôanagrafica pulita che contiene anche il codice ISTAT (city_code) che vogliamo aggiungere al nostro file di input.\ncity,region,city_code\nCefal√π,Sicilia,082027\nReggio di Calabria,Calabria,080063\nRodengo Saiano,Lombardia,017164"
  },
  {
    "objectID": "doc_sources/how-to.html#uso-fondamentale",
    "href": "doc_sources/how-to.html#uso-fondamentale",
    "title": "Come usare Tom√©to Tomato",
    "section": "Uso Fondamentale",
    "text": "Uso Fondamentale\nPer collegare questi due file, useremo il seguente comando:\ntometo_tomato docs/files/input.csv docs/files/ref.csv \\\n  -j \"city,city\" \\\n  -j \"region,region\" \\\n  -a \"city_code\" \\\n  -s \\\n  -o \"output/clean_matches.csv\"\nAnalizziamo il comando pezzo per pezzo:\n\ntometo_tomato docs/files/input.csv docs/files/ref.csv: Indichiamo il programma da eseguire, seguito dal file di input e da quello di riferimento.\n-j \"city,city\": Specifichiamo la prima coppia di colonne da usare per il join. In questo caso i nomi delle colonne sono identici.\n-j \"region,region\": Specifichiamo la seconda coppia di colonne. Il programma calcoler√† un punteggio di somiglianza medio basato su tutte le coppie fornite.\n-a \"city_code\": Indichiamo quale colonna del file di riferimento (ref.csv) vogliamo aggiungere al nostro file di output.\n-s: Chiediamo di includere anche una colonna con il punteggio di somiglianza (avg_score) nell‚Äôoutput.\n-o \"output/clean_matches.csv\": Specifichiamo il percorso del file CSV di output in cui salvare i risultati."
  },
  {
    "objectID": "doc_sources/how-to.html#il-risultato",
    "href": "doc_sources/how-to.html#il-risultato",
    "title": "Come usare Tom√©to Tomato",
    "section": "Il Risultato",
    "text": "Il Risultato\nDopo aver eseguito il comando, il file output/clean_matches.csv conterr√† i dati del file di input, arricchiti con le colonne del file di riferimento per le righe che hanno trovato una corrispondenza con un punteggio sufficientemente alto. Ad esempio, la riga RODENGO-SAIANO,Lombardia verr√† collegata a Rodengo Saiano,Lombardia, e il city_code 017164 verr√† aggiunto alla riga di output."
  },
  {
    "objectID": "doc_sources/how-to.html#opzioni-avanzate-utili",
    "href": "doc_sources/how-to.html#opzioni-avanzate-utili",
    "title": "Come usare Tom√©to Tomato",
    "section": "Opzioni Avanzate Utili",
    "text": "Opzioni Avanzate Utili\nL‚Äôutilit√† offre molte altre opzioni per personalizzare il processo:\n\n--threshold o -t: Permette di modificare la soglia di somiglianza minima (default: 85) per considerare una corrispondenza valida.\n--infer-pairs: Tenta di indovinare automaticamente le coppie di colonne da usare per il join, basandosi sulla somiglianza dei nomi delle colonne.\n--latinize: Normalizza i caratteri, rimuovendo accenti e simboli speciali prima del confronto, come abbiamo visto nei nostri esempi con strip_accents.\n--keep-alphanumeric o -k: Mantiene solo lettere, numeri e spazi nelle colonne di join, rimuovendo punteggiatura e caratteri speciali. Utile per dati con apostrofi, trattini, punti, ecc.\n\nEsempio: bash   tometo_tomato input.csv ref.csv -j \"name,ref_name\" --keep-alphanumeric -o output.csv   # oppure versione breve   tometo_tomato input.csv ref.csv -j \"name,ref_name\" -k -o output.csv Questo permette di far corrispondere, ad esempio, John O'Connor con John OConnor, oppure Anna & Co. con Anna Co.\n\n--raw-case e --raw-whitespace: Disabilitano la normalizzazione di maiuscole/minuscole e degli spazi, per un controllo pi√π fine."
  },
  {
    "objectID": "doc_sources/how-to.html#ambiguit√†-e-comportamento-degli-output",
    "href": "doc_sources/how-to.html#ambiguit√†-e-comportamento-degli-output",
    "title": "Come usare Tom√©to Tomato",
    "section": "Ambiguit√† e comportamento degli output",
    "text": "Ambiguit√† e comportamento degli output\nLo strumento segnala e gestisce in modo esplicito i casi ambigui per evitare di inserire dati potenzialmente errati nell‚Äôoutput ‚Äúpulito‚Äù.\n\nDefinizione di ambiguit√†: una riga di input √® considerata ‚Äúambigua‚Äù quando due o pi√π righe del file di riferimento ottengono lo stesso punteggio massimo medio (avg_score) per quell‚Äôinput, e tale punteggio soddisfa la soglia --threshold.\nComportamento: le righe ambigue NON vengono incluse nell‚Äô--output-clean. In questo modo il file ‚Äúpulito‚Äù contiene solo corrispondenze non ambigue (una sola migliore corrispondenza con punteggio &gt;= soglia).\nEsportazione ambigui: per salvare le righe candidate ambigue, usare l‚Äôopzione --output-ambiguous ambiguous.csv. Il file conterr√† le righe di riferimento che hanno ottenuto il punteggio massimo (uguale) per ciascun input ambiguo.\nAvviso: lo strumento rileva sempre la presenza di record ambigui e mostra un avviso quando ne trova uno; l‚Äôavviso suggerisce di usare --output-ambiguous per salvare i dettagli.\n\nEsempio: se per l‚Äôinput \"Reggio Calabria\" due righe di riferimento diverse ottengono entrambe 95.45 e la soglia √® 85, l‚Äôinput risulter√† ambiguo, non comparir√† in --output-clean e potrai ispezionare entrambe le righe candidate con --output-ambiguous."
  },
  {
    "objectID": "doc_sources/about.html",
    "href": "doc_sources/about.html",
    "title": "About Me",
    "section": "",
    "text": "This is the about page.\nYou can put information about yourself or your project here."
  },
  {
    "objectID": "doc_sources/install.html",
    "href": "doc_sources/install.html",
    "title": "Installazione",
    "section": "",
    "text": "Questo documento spiega come installare il tool tometo_tomato sul tuo sistema."
  },
  {
    "objectID": "doc_sources/install.html#prerequisiti",
    "href": "doc_sources/install.html#prerequisiti",
    "title": "Installazione",
    "section": "Prerequisiti",
    "text": "Prerequisiti\nAssicurati di avere installato Python (versione 3.8 o superiore). Per una gestione efficiente dei pacchetti, raccomandiamo l‚Äôuso di uv, un tool moderno e veloce."
  },
  {
    "objectID": "doc_sources/install.html#metodo-1-installazione-per-sviluppo-da-sorgente",
    "href": "doc_sources/install.html#metodo-1-installazione-per-sviluppo-da-sorgente",
    "title": "Installazione",
    "section": "Metodo 1: Installazione per Sviluppo (da sorgente)",
    "text": "Metodo 1: Installazione per Sviluppo (da sorgente)\nQuesto metodo √® ideale se intendi contribuire al progetto o modificarne il codice.\n\nClona il repository: bash     git clone https://github.com/aborruso/tometo_tomato.git\nNaviga nella directory del progetto: bash     cd tometo_tomato\nInstalla le dipendenze:\n\nCon uv (raccomandato): bash     uv pip install -e . Questo installer√† il pacchetto in modalit√† ‚Äúeditabile‚Äù, permettendoti di vedere immediatamente le modifiche al codice senza dover reinstallare.\nCon pip: bash     pip install -e .\n\nSe non intendi modificare il codice, puoi omettere il flag -e:\n\nuv pip install .\npip install ."
  },
  {
    "objectID": "doc_sources/install.html#metodo-2-installazione-come-tool-globale-con-uv-tool-install",
    "href": "doc_sources/install.html#metodo-2-installazione-come-tool-globale-con-uv-tool-install",
    "title": "Installazione",
    "section": "Metodo 2: Installazione come Tool Globale (con uv tool install)",
    "text": "Metodo 2: Installazione come Tool Globale (con uv tool install)\nSe vuoi semplicemente usare tometo_tomato come un comando da riga di comando senza preoccuparti della gestione del codice sorgente o degli ambienti virtuali, uv tool install √® la soluzione pi√π pulita.\n\nAssicurati di avere uv installato.\nInstalla il tool: bash     uv tool install tometo_tomato Questo comando installer√† tometo_tomato in un ambiente isolato gestito da uv e lo render√† disponibile nel tuo PATH."
  },
  {
    "objectID": "doc_sources/install.html#verifica-dellinstallazione",
    "href": "doc_sources/install.html#verifica-dellinstallazione",
    "title": "Installazione",
    "section": "Verifica dell‚ÄôInstallazione",
    "text": "Verifica dell‚ÄôInstallazione\nDopo aver completato uno dei metodi di installazione, puoi verificare che tometo_tomato sia correttamente installato eseguendo:\ntometo_tomato --version\nDovresti vedere la versione del tool stampata a schermo."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Tom√©to Tomato",
    "section": "",
    "text": "Tired of messy data breaking your joins?\nTom√©to Tomato is your powerful command-line tool for fuzzy data integration. It intelligently connects disparate datasets by similarity, not just exact matches, even with typos or inconsistencies.\nLeveraging DuckDB and rapidfuzz, Tom√©to Tomato makes cleaning and merging your data fast, flexible, and reliable.\nUnlock the true potential of your data. Get started now!\nLet‚Äôs Call the Whole Thing Off!!\n\n\n\nThe raw data\n\n\ncity\nregion\n\n\n\n\nCefalu‚Äô\nSicilia\n\n\nReggio Calabria\nCALABRIA\n\n\nRODENGO-SAIANO\nLombardia\n\n\n\n\n\n\n\nThe reference data\n\n\ncity\nregion\ncity_code\n\n\n\n\nCefal√π\nSicilia\n082027\n\n\nReggio di Calabria\nCalabria\n080063\n\n\nRodengo Saiano\nLombardia\n017164\n\n\n\n\n\n\nExample fuzzy join result\n\n\n\n\n\n\n\n\n\n\ncity (input)\nregion (input)\ncity (ref)\nregion (ref)\ncity_code\navg_score\n\n\n\n\nCefalu‚Äô\nSicilia\nCefal√π\nSicilia\n082027\n100.0\n\n\nRODENGO-SAIANO\nLombardia\nRodengo Saiano\nLombardia\n017164\n98.14\n\n\nReggio Calabria\nCALABRIA\nReggio di Calabria\nCalabria\n080063\n95.45\n\n\n\n Intro üëâ How to use it"
  },
  {
    "objectID": "doc_sources/PRD.html",
    "href": "doc_sources/PRD.html",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "Integrating data from different sources often presents a common challenge: the lack of unique and consistent join keys. Typos, abbreviations, formatting variations, or simple discrepancies (e.g., ‚ÄúReggio di Calabria‚Äù, ‚ÄúReggio Calabria‚Äù, ‚ÄúReggi√≤ Calabria‚Äù) render standard SQL joins (A.key = B.key) ineffective.\nThis document defines the requirements for a ‚Äúfuzzy join‚Äù procedure that allows connecting records between two tables based on the textual similarity of fields, rather than an exact match.\n\n\n\nTo create a robust, configurable, and high-performance process for performing table joins via fuzzy string matching. The system must identify the best possible match for each record, transparently handle ambiguities, and provide clear, analysis-ready outputs.\n\n\n\n\n\n\nThe procedure allows performing a join between table A (left) and table B (right) based on the similarity of one or more pairs of text columns. New Feature: If the columns to compare (--join-pair) are not specified, the system automatically uses all columns with the same name present in both files.\n\n\n\n\nBy default:\n\nColumn matching is case insensitive (comparison ignores upper/lower case).\nMultiple and leading/trailing whitespaces are removed from each field before comparison.\n\nCLI options:\n\n--raw-case: enables case sensitive comparison (disables lower-case conversion)\n--raw-whitespace: preserves whitespaces (disables whitespace normalization)\n--latinize: normalizes accented characters and apostrophes, converting non-latin characters to latin and removing special characters (e.g.¬†‚ÄúCefal√π‚Äù and ‚ÄúCefalu‚Äô‚Äù both become ‚ÄúCefalu‚Äù)\n\nAll options can be combined as needed.\n\n\n\n\nFor each record in table A, the system calculates a similarity score (from 0 to 100) with all records in table B, using the rapidfuzz extension functions for DuckDB.\n\n\n\n\nThe system identifies the ‚Äúbest match‚Äù as the record in table B that obtains the highest similarity score.\nA join is valid only if the score exceeds a configurable minimum threshold (e.g., 85). All matches with a lower score are discarded.\n\n\n\n\nThe procedure supports joining based on multiple column pairs. If not specified, it uses all common columns.\n\n\n\n\nDefinition of ambiguity: Ambiguity occurs when a record in table A obtains the same maximum score for multiple records in table B.\nHandling: In case of ambiguity, record A and all corresponding records in B are excluded from the final join result.\nAmbiguity Output: All records excluded due to ambiguity are saved to a separate file (e.g., ambiguous_log.csv) for manual analysis.\n\n\n\n\nThe procedure produces two main outputs:\n\nClean Join Table: A file with all records from the input table (left join behavior). Records that found a unique match above the threshold will have the corresponding reference data populated; records without a match will have the reference fields empty/null.\nAmbiguity Log File: A file with records discarded due to the reasons described in FR5.\n\n\n\n\nThe system implements a left join approach:\n\nALL records from the input table are included in the clean output, regardless of whether they found a match or not.\nRecords that found a valid match (score &gt;= threshold and unambiguous) will have the reference data populated.\nRecords that did not find any match or found only matches below the threshold will have the reference fields empty/null.\nThis allows users to study and analyze which records were not successfully joined.\n\n\n\n\n\n\n\nThe procedure is optimized to handle large datasets. The use of WHERE score &gt; threshold in DuckDB reduces computational load.\n\n\n\nThe user can easily configure: - Input file paths. - Column names to use for the join (optional; if omitted, common columns are used). - Similarity threshold (number from 0 to 100). - rapidfuzz function to use (e.g., rapidfuzz_ratio, rapidfuzz_token_sort_ratio). - Output file paths.\n\n\n\nThe process produces an execution log with key statistics: number of input records, successful joins, ambiguous cases.\n\n\n\n\n\nProcessing Engine: DuckDB\nFuzzy Matching Library: rapidfuzz extension for DuckDB\nOrchestration: Python script (single-command CLI: tometo_tomato)\n\n\n\n\nThe project documentation will be published using Quarto, leveraging the project‚Äôs docs folder.\n\n\n\nThis use case demonstrates the association of ISTAT codes with an unofficial registry, managing inaccuracies in place names.\nTable A (ref.csv - Official ISTAT Source) Contains official data of Italian municipalities.\n\n\n\nregion\nmunicipality\nmunicipality_code\n\n\n\n\nCalabria\nReggio Calabria\n80065\n\n\nLombardy\nMilan\n015146\n\n\nPiedmont\nTurin\n001272\n\n\nLazio\nRome\n058091\n\n\nCampania\nNaples\n063049\n\n\n\nTable B (input.csv - Unofficial Registry) Contains data with possible typos.\n\n\n\nregion\nmunicipality\n\n\n\n\nCalabria\nReggio Calabr\n\n\nLombardy\nMilan\n\n\nPiedmont\nTorinoo\n\n\nLazio\nRma\n\n\nCampania\nNaples\n\n\n\nObjective Associate the municipality_code from Table A (ref.csv) with records in Table B (input.csv).\nConfiguration (CLI Call Example) The process is executed via the single-command CLI tometo_tomato:\ntometo_tomato input.csv ref.csv --join-pair region,region --join-pair municipality,municipality --add-field municipality_code --threshold 90 --show-score\nOr, if the columns to compare coincide in the two files:\ntometo_tomato input.csv ref.csv --add-field municipality_code --threshold 90 --show-score\nThe process identifies the best match for each row in input.csv within ref.csv and associates the corresponding municipality_code. All input records are included in the output (left join behavior).\nExample of expected matches:\n\ninput.csv (Reggio Calabr, Calabria) -&gt; ref.csv (Reggio Calabria, Calabria) with municipality_code 80065.\ninput.csv (Torinoo, Piedmont) -&gt; ref.csv (Turin, Piedmont) with municipality_code 001272.\ninput.csv (Rma, Lazio) -&gt; ref.csv (Rome, Lazio) with municipality_code 058091.\nRecords with no match or low similarity scores will have empty/null values for municipality_code but will still appear in the output.\nThe final result is a table with all rows from input.csv plus the associated municipality_code (populated only for successful matches)."
  },
  {
    "objectID": "doc_sources/PRD.html#introduction",
    "href": "doc_sources/PRD.html#introduction",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "Integrating data from different sources often presents a common challenge: the lack of unique and consistent join keys. Typos, abbreviations, formatting variations, or simple discrepancies (e.g., ‚ÄúReggio di Calabria‚Äù, ‚ÄúReggio Calabria‚Äù, ‚ÄúReggi√≤ Calabria‚Äù) render standard SQL joins (A.key = B.key) ineffective.\nThis document defines the requirements for a ‚Äúfuzzy join‚Äù procedure that allows connecting records between two tables based on the textual similarity of fields, rather than an exact match.\n\n\n\nTo create a robust, configurable, and high-performance process for performing table joins via fuzzy string matching. The system must identify the best possible match for each record, transparently handle ambiguities, and provide clear, analysis-ready outputs."
  },
  {
    "objectID": "doc_sources/PRD.html#functional-requirements",
    "href": "doc_sources/PRD.html#functional-requirements",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "The procedure allows performing a join between table A (left) and table B (right) based on the similarity of one or more pairs of text columns. New Feature: If the columns to compare (--join-pair) are not specified, the system automatically uses all columns with the same name present in both files.\n\n\n\n\nBy default:\n\nColumn matching is case insensitive (comparison ignores upper/lower case).\nMultiple and leading/trailing whitespaces are removed from each field before comparison.\n\nCLI options:\n\n--raw-case: enables case sensitive comparison (disables lower-case conversion)\n--raw-whitespace: preserves whitespaces (disables whitespace normalization)\n--latinize: normalizes accented characters and apostrophes, converting non-latin characters to latin and removing special characters (e.g.¬†‚ÄúCefal√π‚Äù and ‚ÄúCefalu‚Äô‚Äù both become ‚ÄúCefalu‚Äù)\n\nAll options can be combined as needed.\n\n\n\n\nFor each record in table A, the system calculates a similarity score (from 0 to 100) with all records in table B, using the rapidfuzz extension functions for DuckDB.\n\n\n\n\nThe system identifies the ‚Äúbest match‚Äù as the record in table B that obtains the highest similarity score.\nA join is valid only if the score exceeds a configurable minimum threshold (e.g., 85). All matches with a lower score are discarded.\n\n\n\n\nThe procedure supports joining based on multiple column pairs. If not specified, it uses all common columns.\n\n\n\n\nDefinition of ambiguity: Ambiguity occurs when a record in table A obtains the same maximum score for multiple records in table B.\nHandling: In case of ambiguity, record A and all corresponding records in B are excluded from the final join result.\nAmbiguity Output: All records excluded due to ambiguity are saved to a separate file (e.g., ambiguous_log.csv) for manual analysis.\n\n\n\n\nThe procedure produces two main outputs:\n\nClean Join Table: A file with all records from the input table (left join behavior). Records that found a unique match above the threshold will have the corresponding reference data populated; records without a match will have the reference fields empty/null.\nAmbiguity Log File: A file with records discarded due to the reasons described in FR5.\n\n\n\n\nThe system implements a left join approach:\n\nALL records from the input table are included in the clean output, regardless of whether they found a match or not.\nRecords that found a valid match (score &gt;= threshold and unambiguous) will have the reference data populated.\nRecords that did not find any match or found only matches below the threshold will have the reference fields empty/null.\nThis allows users to study and analyze which records were not successfully joined."
  },
  {
    "objectID": "doc_sources/PRD.html#non-functional-requirements",
    "href": "doc_sources/PRD.html#non-functional-requirements",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "The procedure is optimized to handle large datasets. The use of WHERE score &gt; threshold in DuckDB reduces computational load.\n\n\n\nThe user can easily configure: - Input file paths. - Column names to use for the join (optional; if omitted, common columns are used). - Similarity threshold (number from 0 to 100). - rapidfuzz function to use (e.g., rapidfuzz_ratio, rapidfuzz_token_sort_ratio). - Output file paths.\n\n\n\nThe process produces an execution log with key statistics: number of input records, successful joins, ambiguous cases."
  },
  {
    "objectID": "doc_sources/PRD.html#technology-stack",
    "href": "doc_sources/PRD.html#technology-stack",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "Processing Engine: DuckDB\nFuzzy Matching Library: rapidfuzz extension for DuckDB\nOrchestration: Python script (single-command CLI: tometo_tomato)"
  },
  {
    "objectID": "doc_sources/PRD.html#documentation",
    "href": "doc_sources/PRD.html#documentation",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "The project documentation will be published using Quarto, leveraging the project‚Äôs docs folder."
  },
  {
    "objectID": "doc_sources/PRD.html#example-use-case-istat-code-association",
    "href": "doc_sources/PRD.html#example-use-case-istat-code-association",
    "title": "PRD: Flexible Join Procedure (Fuzzy Join)",
    "section": "",
    "text": "This use case demonstrates the association of ISTAT codes with an unofficial registry, managing inaccuracies in place names.\nTable A (ref.csv - Official ISTAT Source) Contains official data of Italian municipalities.\n\n\n\nregion\nmunicipality\nmunicipality_code\n\n\n\n\nCalabria\nReggio Calabria\n80065\n\n\nLombardy\nMilan\n015146\n\n\nPiedmont\nTurin\n001272\n\n\nLazio\nRome\n058091\n\n\nCampania\nNaples\n063049\n\n\n\nTable B (input.csv - Unofficial Registry) Contains data with possible typos.\n\n\n\nregion\nmunicipality\n\n\n\n\nCalabria\nReggio Calabr\n\n\nLombardy\nMilan\n\n\nPiedmont\nTorinoo\n\n\nLazio\nRma\n\n\nCampania\nNaples\n\n\n\nObjective Associate the municipality_code from Table A (ref.csv) with records in Table B (input.csv).\nConfiguration (CLI Call Example) The process is executed via the single-command CLI tometo_tomato:\ntometo_tomato input.csv ref.csv --join-pair region,region --join-pair municipality,municipality --add-field municipality_code --threshold 90 --show-score\nOr, if the columns to compare coincide in the two files:\ntometo_tomato input.csv ref.csv --add-field municipality_code --threshold 90 --show-score\nThe process identifies the best match for each row in input.csv within ref.csv and associates the corresponding municipality_code. All input records are included in the output (left join behavior).\nExample of expected matches:\n\ninput.csv (Reggio Calabr, Calabria) -&gt; ref.csv (Reggio Calabria, Calabria) with municipality_code 80065.\ninput.csv (Torinoo, Piedmont) -&gt; ref.csv (Turin, Piedmont) with municipality_code 001272.\ninput.csv (Rma, Lazio) -&gt; ref.csv (Rome, Lazio) with municipality_code 058091.\nRecords with no match or low similarity scores will have empty/null values for municipality_code but will still appear in the output.\nThe final result is a table with all rows from input.csv plus the associated municipality_code (populated only for successful matches)."
  }
]