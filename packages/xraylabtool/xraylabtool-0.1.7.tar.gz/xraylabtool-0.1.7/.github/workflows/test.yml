name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macOS-latest, windows-latest]
        python-version: ["3.12", "3.13"]
        
    steps:
    - uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. Using 88 chars to match Black formatting
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
      continue-on-error: true
        
    - name: Test with pytest (without benchmarks)
      run: |
        pytest tests/ -v --cov=xraylabtool --cov-report=xml --cov-report=html
        
    - name: Run benchmarks
      run: |
        pytest tests/test_integration.py::TestPerformanceBenchmarks -v --benchmark-only --benchmark-json=benchmark.json
      continue-on-error: true
        
    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
        
    - name: Upload benchmark results
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark.json

  benchmark-comparison:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for comparison
        
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: 3.12
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        
    - name: Run benchmarks on current branch
      run: |
        pytest tests/test_integration.py::TestPerformanceBenchmarks --benchmark-only --benchmark-json=current-bench.json
      continue-on-error: true
        
    - name: Switch to base branch and run benchmarks
      run: |
        git checkout ${{ github.event.pull_request.base.sha }}
        pytest tests/test_integration.py::TestPerformanceBenchmarks --benchmark-only --benchmark-json=base-bench.json
      continue-on-error: true
        
    - name: Compare benchmarks
      run: |
        echo "## Benchmark Comparison" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f "current-bench.json" ] && [ -f "base-bench.json" ]; then
          echo "Benchmark comparison between base and current branch:" >> $GITHUB_STEP_SUMMARY
          echo "*(Note: Detailed comparison would require additional tooling)*" >> $GITHUB_STEP_SUMMARY
        else
          echo "Could not generate benchmark comparison - benchmark files missing" >> $GITHUB_STEP_SUMMARY
        fi
      continue-on-error: true
