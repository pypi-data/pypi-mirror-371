"""
Comprehensive API coverage tests for PyHero API.

This module ensures:
- All API endpoints are tested and functional
- Response schemas are validated
- Error scenarios are covered
- Performance benchmarks are met
- Integration with real API works correctly
"""

import pytest
import responses
from typing import Dict, List, Any
from unittest.mock import patch

from pyheroapi import KiwoomClient, KiwoomAPI, Stock, ETF, ELW, Account
from pyheroapi.exceptions import KiwoomAPIError, KiwoomAuthError, KiwoomRequestError
from .base import BaseUnitTest, BaseIntegrationTest
from .conftest import TestConfig, MockDataGenerator


class TestAPICoverage(BaseUnitTest):
    """Test coverage of all API endpoints."""
    
    def test_stock_endpoints(self, mock_responses, mock_data_generator):
        """Test stock-related API endpoints."""
        client = self.create_mock_client()
        
        mock_responses.add(
            responses.POST,
            f"{TestConfig.SANDBOX_URL}/api/dostk/mrkcond",
            json=mock_data_generator.create_stock_quote_response(),
            status=200
        )
        
        result = client.get_quote("005930")
        # get_quote returns QuoteData object, not raw dict with return_code
        assert hasattr(result, 'bid_req_base_tm'), "QuoteData object missing expected attribute"
        print("âœ… Stock endpoints working")
    
    def test_response_validation(self, mock_responses, mock_data_generator):
        """Validate API response schemas."""
        client = self.create_mock_client()
        response_data = mock_data_generator.create_stock_quote_response()
        
        assert "return_code" in response_data
        assert "return_msg" in response_data
        assert isinstance(response_data["return_code"], int)
        
        print("âœ… Response validation passed")


class TestAPIPerformance(BaseUnitTest):
    """Performance tests for API operations."""
    
    def test_response_time_benchmarks(self, mock_responses, mock_data_generator):
        """Test that API responses meet performance benchmarks."""
        import time
        
        client = self.create_mock_client()
        
        # Mock fast response
        mock_responses.add(
            responses.POST,
            f"{TestConfig.SANDBOX_URL}/api/dostk/mrkcond",
            json=mock_data_generator.create_stock_quote_response(),
            status=200
        )
        
        # Benchmark multiple calls
        times = []
        iterations = 10
        
        for _ in range(iterations):
            start = time.time()
            client.get_quote("005930")
            end = time.time()
            times.append(end - start)
        
        avg_time = sum(times) / len(times)
        max_time = max(times)
        
        # Performance assertions
        assert avg_time <= 1.0, f"Average response time too slow: {avg_time:.3f}s"
        assert max_time <= 2.0, f"Maximum response time too slow: {max_time:.3f}s"
        
        print(f"âœ… Performance benchmark met: avg={avg_time:.3f}s, max={max_time:.3f}s")
    
    def test_concurrent_request_handling(self, mock_responses, mock_data_generator):
        """Test handling of concurrent API requests."""
        import threading
        import time
        
        client = self.create_mock_client()
        
        # Mock response
        mock_responses.add(
            responses.POST,
            f"{TestConfig.SANDBOX_URL}/api/dostk/mrkcond",
            json=mock_data_generator.create_stock_quote_response(),
            status=200
        )
        
        results = []
        errors = []
        
        def make_request():
            try:
                result = client.get_quote("005930")
                results.append(result)
            except Exception as e:
                errors.append(e)
        
        # Launch concurrent threads
        threads = []
        for _ in range(5):
            thread = threading.Thread(target=make_request)
            threads.append(thread)
            thread.start()
        
        # Wait for completion
        for thread in threads:
            thread.join()
        
        # Validate results
        assert len(errors) == 0, f"Concurrent requests had errors: {errors}"
        assert len(results) == 5, f"Expected 5 results, got {len(results)}"
        
        print("âœ… Concurrent request handling working")


class TestAPIDocumentationCompliance:
    """Ensure API usage complies with official documentation."""
    
    def test_required_headers(self):
        """Test that all required headers are included in requests."""
        client = KiwoomClient(access_token="test_token", is_production=False)
        
        headers = client.session.headers
        
        # Check required headers
        assert "Authorization" in headers
        assert "Content-Type" in headers
        assert headers["Content-Type"] == "application/json;charset=UTF-8"
        assert "Bearer test_token" in headers["Authorization"]
        
        print("âœ… Required headers are present")
    
    def test_request_format_compliance(self):
        """Test that requests are formatted according to API documentation."""
        client = KiwoomClient(access_token="test_token", is_production=False)
        
        # Test URL construction
        assert client.base_url == TestConfig.SANDBOX_URL
        
        # Test client timeout configuration (stored in client, not session)
        assert client.timeout == 30
        
        print("âœ… Request format compliance verified")
    
    def test_environment_separation(self):
        """Test proper separation between sandbox and production environments."""
        sandbox_client = KiwoomClient(access_token="test", is_production=False)
        production_client = KiwoomClient(access_token="test", is_production=True)
        
        assert sandbox_client.base_url != production_client.base_url
        assert "mockapi" in sandbox_client.base_url  # Mock API environment
        assert "mockapi" not in production_client.base_url
        
        print("âœ… Environment separation working correctly")


@pytest.mark.integration
class TestRealAPIIntegration(BaseIntegrationTest):
    """Integration tests with real API."""
    
    def test_real_data_retrieval(self):
        """Test retrieving real data."""
        client = self.create_real_client()
        result = client.get_quote("005930")
        self.assert_api_response_success(result)
        print("âœ… Real data retrieval working")


# Comprehensive test runner
def run_comprehensive_tests():
    """Run all comprehensive API tests and generate report."""
    test_results = {
        "total_tests": 0,
        "passed_tests": 0,
        "failed_tests": 0,
        "categories": {
            "coverage": {"passed": 0, "total": 0},
            "integration": {"passed": 0, "total": 0},
            "performance": {"passed": 0, "total": 0},
            "compliance": {"passed": 0, "total": 0}
        }
    }
    
    print("ðŸš€ Running comprehensive API tests...")
    print("=" * 50)
    
    # This would be called by pytest with proper test discovery
    # For now, it's a placeholder for the test orchestration
    
    return test_results


if __name__ == "__main__":
    # Allow running this module directly for quick testing
    results = run_comprehensive_tests()
    print(f"\nðŸ“Š Test Results: {results['passed_tests']}/{results['total_tests']} passed") 