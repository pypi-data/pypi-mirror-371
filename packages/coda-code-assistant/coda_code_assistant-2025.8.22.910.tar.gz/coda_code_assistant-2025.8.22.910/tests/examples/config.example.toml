# Coda Configuration Example
# Copy this file to ~/.config/coda/config.toml and modify as needed

# Default provider to use
default_provider = "oci_genai"

# Provider configurations
[providers.oci_genai]
# OCI compartment ID (can also be set via OCI_COMPARTMENT_ID env var)
compartment_id = "ocid1.compartment.oc1..example"
# Optional: OCI config profile (defaults to DEFAULT)
config_profile = "DEFAULT"
# Optional: OCI config file location (defaults to ~/.oci/config)
config_file_location = "~/.oci/config"

[providers.litellm]
# Configuration for LiteLLM provider (when implemented)
# api_base = "http://localhost:8000"
# default_model = "gpt-4"

[providers.ollama]
# Configuration for Ollama provider (when implemented)
# host = "http://localhost:11434"
# default_model = "llama3"

# Session settings
[session]
history_file = "~/.local/share/coda/history"
max_history = 1000
autosave = true

# UI settings
[ui]
theme = "default"
show_model_info = true
show_token_usage = false

# Generation parameters
temperature = 0.7
max_tokens = 2000

# Debug mode
debug = false