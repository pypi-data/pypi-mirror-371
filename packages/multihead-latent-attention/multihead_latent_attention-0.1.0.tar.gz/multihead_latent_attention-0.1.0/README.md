# Multi-head Latent Attention (MLA)

## Overview
This repository attempts to implement Multi-head Latent Attention (MLA), a novel attention mechanism introduced by DeepSeek in their paper ["DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model"](https://arxiv.org/abs/2405.04434), which eliminates the bottleneck caused by Key-Value (KV) cache in Multi-Head Attention (MHA).

## Install
```bash
pip install multihead-latent-attention
```

## Contributing
This project is an educational tool and welcomes contributions. To contribute:
- Fork the repository.
- Make your changes.
- Submit a pull request with a description of your updates.

Feel free to open an issue for suggestions or bugs.

## Citation
```bibtex 
@article{deepseek2024,
  title   = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author  = {DeepSeek-AI},
  journal = {arXiv},
  year    = {2024},
  url     = {https://arxiv.org/abs/2405.04434}
}
```

## License
This project is licensed under the [MIT License](LICENSE).
