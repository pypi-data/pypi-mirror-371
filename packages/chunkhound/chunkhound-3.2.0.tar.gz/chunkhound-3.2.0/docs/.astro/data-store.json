[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.13.2","content-config-digest","9a95ec2e8398aaca","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://ofriw.github.io\",\"compressHTML\":true,\"base\":\"/chunkhound\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"where\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[null,[null,{\"experimentalHeadingIdCompat\":false}],null,[null,{\"themes\":[\"github-light\",\"github-dark\"],\"defaultLocale\":\"en\",\"cascadeLayer\":\"starlight.components\",\"styleOverrides\":{\"borderRadius\":\"0px\",\"borderWidth\":\"1px\",\"codePaddingBlock\":\"0.75rem\",\"codePaddingInline\":\"1rem\",\"codeFontFamily\":\"var(--__sl-font-mono)\",\"codeFontSize\":\"var(--sl-text-code)\",\"codeLineHeight\":\"var(--sl-line-height)\",\"uiFontFamily\":\"var(--__sl-font)\",\"textMarkers\":{\"lineDiffIndicatorMarginLeft\":\"0.25rem\",\"defaultChroma\":\"45\",\"backgroundOpacity\":\"60%\"}},\"plugins\":[{\"name\":\"Starlight Plugin\",\"hooks\":{}},{\"name\":\"astro-expressive-code\",\"hooks\":{}}]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false},\"legacy\":{\"collections\":false},\"prefetch\":{\"prefetchAll\":true},\"i18n\":{\"defaultLocale\":\"en\",\"locales\":[\"en\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":false,\"fallbackType\":\"redirect\"}}}","docs",["Map",11,12,36,37,47,48,58,59],"index",{"id":11,"data":13,"body":30,"filePath":31,"assetImports":32,"digest":35,"deferredRender":16},{"title":14,"description":15,"editUrl":16,"head":17,"template":18,"hero":19,"sidebar":27,"pagefind":16,"draft":28},"ChunkHound","Semantic and regex search for codebases via MCP",true,[],"doc",{"title":20,"tagline":21,"image":22,"actions":26},"","Modern RAG for your codebase - Semantic and Regex Search via MCP",{"alt":23,"dark":24,"light":25},"ChunkHound Logo","__ASTRO_IMAGE_../../assets/wordmark-dark.svg","__ASTRO_IMAGE_../../assets/wordmark.svg",[],{"hidden":28,"attrs":29},false,{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\n\n## What it does\n\nLLMs like Claude and GPT don't know your codebase - they only know what they were trained on. Every time they help you code, they need to search your files to understand your project's specific patterns and terminology.\n\nChunkHound integrates with AI assistants via the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) to give them two ways to explore your code:\n- **Semantic search** - Finds code by meaning, so when the AI looks for \"user authentication\" it also finds your `validateLogin()` and `checkCredentials()` functions\n- **Regex search** - Pattern matching for precise code structures\n\nTraditional search was built for humans who know what they're looking for. But AI assistants start with zero knowledge about your codebase. Semantic search bridges this gap by understanding that \"database timeout\" and \"SQL connection lost\" are related concepts, even though they share no keywords.\n\n## Supported Languages\n\nChunkHound supports **22 languages** with structured parsing:\n\n- **Programming** (via [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)): Python, JavaScript, TypeScript, JSX, TSX, Java, Kotlin, Groovy, C, C++, C#, Go, Rust, Bash, MATLAB, Makefile\n- **Configuration** (via Tree-sitter): JSON, YAML, TOML, Markdown\n- **Text-based** (custom parsers): Text files, PDF\n\n## Requirements\n\n- Python 3.10+\n- [uv package manager](https://docs.astral.sh/uv/)\n- API key for semantic search (optional)\n\n## Installation\n\n```bash\n# Install uv package manager\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install ChunkHound\nuv tool install chunkhound\n```\n\n## Configuration\n\nChunkHound works **without configuration** for regex search. For semantic search, create `.chunkhound.json` in your project root:\n\n\u003CTabs syncKey=\"provider\">\n  \u003CTabItem label=\"VoyageAI\">\n    **Recommended:** Fastest, most accurate, and cost effective\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"voyageai\",\n        \"api_key\": \"pa-your-voyage-key\"\n      }\n    }\n    ```\n\n    Get API key from [VoyageAI Console](https://dash.voyageai.com/) | [Documentation](https://docs.voyageai.com/docs/embeddings)\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"OpenAI\">\n    **Best for:** Wide compatibility and ecosystem\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"api_key\": \"sk-your-openai-key\"\n      }\n    }\n    ```\n\n    Get API key from [OpenAI Platform](https://platform.openai.com/api-keys) | [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"Local\">\n    **Best for:** Privacy and offline use\n\n    ```bash\n    # Start Ollama with embedding model\n    ollama pull nomic-embed-text\n    ```\n\n    [Ollama Documentation](https://ollama.ai/) | [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"base_url\": \"http://localhost:11434/v1\",\n        \"model\": \"nomic-embed-text\"\n      }\n    }\n    ```\n\n    No API key required - completely local. Works with any OpenAI compatible endpoint.\n  \u003C/TabItem>\n\u003C/Tabs>\n\n## IDE Setup\n\nConfigure ChunkHound as an MCP server in your AI assistant:\n\n\u003CTabs syncKey=\"ide-setup\">\n  \u003CTabItem label=\"Claude Code\">\n    Add to `~/.claude.json`:\n    ```json\n    {\n      \"mcpServers\": {\n        \"chunkhound\": {\n          \"command\": \"chunkhound\",\n          \"args\": [\"mcp\"]\n        }\n      }\n    }\n    ```\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"VS Code\">\n    Add to `.vscode/mcp.json`:\n    ```json\n    {\n      \"servers\": {\n        \"chunkhound\": {\n          \"type\": \"stdio\",\n          \"command\": \"chunkhound\",\n          \"args\": [\"mcp\", \"/path/to/project\"]\n        }\n      }\n    }\n    ```\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"Cursor\">\n    Add to `.cursor/mcp.json`:\n    ```json\n    {\n      \"mcpServers\": {\n        \"chunkhound\": {\n          \"command\": \"chunkhound\",\n          \"args\": [\"mcp\", \"/path/to/project\"]\n        }\n      }\n    }\n    ```\n  \u003C/TabItem>\n\u003C/Tabs>\n\n## Initial Indexing\n\n```bash\n# Index your codebase (respects .gitignore automatically)\ncd /path/to/project && chunkhound index\n```\n\n## Documentation\n\n- [Tutorial](/chunkhound/tutorial/) - Learn ChunkHound in 5 minutes\n- [Configuration](/chunkhound/configuration/) - Complete configuration reference\n- [Under the Hood](/chunkhound/under-the-hood/) - Technical deep dive into [cAST algorithm](https://arxiv.org/pdf/2506.15655) and architecture\n\n## Links\n\n- [GitHub Repository](https://github.com/ofriw/chunkhound)\n- [Report Issues](https://github.com/ofriw/chunkhound/issues)","src/content/docs/index.mdx",[33,34],"../../assets/wordmark-dark.svg","../../assets/wordmark.svg","054187c9f405ba95","under-the-hood",{"id":36,"data":38,"body":44,"filePath":45,"digest":46,"deferredRender":16},{"title":39,"description":40,"editUrl":16,"head":41,"template":18,"sidebar":42,"pagefind":16,"draft":28},"Under the Hood","Technical deep dive into ChunkHound's architecture, algorithms, and design decisions",[],{"hidden":28,"attrs":43},{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\nimport { Card, CardGrid, Aside } from '@astrojs/starlight/components';\nimport SemanticSearchFlow from '../../components/SemanticSearchFlow.tsx';\nimport MultiHopSearchFlow from '../../components/MultiHopSearchFlow.tsx';\n\n## Architecture Overview\n\nChunkHound uses a local-first architecture with embedded databases and universal code parsing. The system is built around the [cAST (Chunking via Abstract Syntax Trees)](https://arxiv.org/pdf/2506.15655) algorithm for intelligent code segmentation:\n\n\u003CCardGrid>\n  \u003CCard title=\"Database Layer\" icon=\"setting\">\n    **[DuckDB](https://duckdb.org/)** (primary) - OLAP columnar database with HNSW vector indexing\n    **[LanceDB](https://lancedb.github.io/lancedb/)** (experimental) - Purpose-built vector database with [Apache Arrow](https://arrow.apache.org/) format\n  \u003C/Card>\n\n  \u003CCard title=\"Parsing Engine\" icon=\"open-book\">\n    **[Tree-sitter](https://tree-sitter.github.io/tree-sitter/)** - Universal AST parser supporting 20+ languages\n    **Language-agnostic** - Same semantic concepts across all languages\n  \u003C/Card>\n\n  \u003CCard title=\"Flexible Providers\" icon=\"puzzle\">\n    **Pluggable backends** - [OpenAI](https://platform.openai.com/docs/guides/embeddings), [VoyageAI](https://docs.voyageai.com/), [Ollama](https://ollama.ai/)\n    **Cloud & Local** - Run with APIs or fully offline with local models\n  \u003C/Card>\n\n  \u003CCard title=\"Advanced Algorithms\" icon=\"rocket\">\n    **[cAST](https://arxiv.org/pdf/2506.15655)** - Semantic code chunking preserving AST structure\n    **Multi-Hop Search** - Context-aware search with reranking\n  \u003C/Card>\n\u003C/CardGrid>\n\nChunkHound's local-first architecture provides key advantages: **Privacy** - Your code never leaves your machine. **Speed** - No network latency or API rate limits. **Reliability** - Works offline and in air-gapped environments. **Cost** - No per-token charges for indexing large codebases.\n\n## The cAST Algorithm\n\nWhen AI assistants search your codebase, they need code split into \"chunks\" - searchable pieces small enough to understand but large enough to be meaningful. The challenge: how do you split code without breaking its logic?\n\n**Research Foundation**: ChunkHound implements the [cAST (Chunking via Abstract Syntax Trees)](https://arxiv.org/pdf/2506.15655) algorithm developed by researchers at Carnegie Mellon University and Augment Code. This approach demonstrates significant improvements in code retrieval and generation tasks.\n\n### Three Approaches Compared\n\n**1. Naive Fixed-Size Chunking**\n\nSplit every 1000 characters regardless of code structure:\n\n```python\ndef authenticate_user(username, password):\n    if not username or not password:\n        return False\n\n    hashed = hash_password(password)\n    user = database.get_u\n# CHUNK BOUNDARY CUTS HERE ❌\nser(username)\n    return user and user.password_hash == hashed\n```\n\n**Problem**: Functions get cut in half, breaking meaning.\n\n**2. Naive AST Chunking**\n\nSplit only at function/class boundaries:\n\n```python\n# Chunk 1: Tiny function (50 characters)\ndef get_name(self):\n    return self.name\n\n# Chunk 2: Massive function (5000 characters)\ndef process_entire_request(self, request):\n    # ... 200 lines of complex logic ...\n```\n\n**Problem**: Creates chunks that are too big or too small.\n\n**3. Smart cAST Algorithm (ChunkHound's Solution)**\n\nRespects code boundaries AND enforces size limits:\n\n```python\n# Right-sized chunks that preserve meaning\ndef authenticate_user(username, password):    # ✅ Complete function\n    if not username or not password:          #    fits in one chunk\n        return False\n    hashed = hash_password(password)\n    user = database.get_user(username)\n    return user and user.password_hash == hashed\n\ndef hash_password(password):                  # ✅ Small adjacent functions\ndef validate_email(email):                   #    merged together\ndef sanitize_input(data):\n    # All fit together in one chunk\n```\n\n### How cAST Works\n\nThe algorithm is surprisingly simple:\n\n1. **Parse** code into a syntax tree (AST) using [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)\n2. **Walk** the tree top-down (classes → functions → statements)\n3. **For each piece**:\n   - If it fits in size limit (1200 chars) → make it a chunk\n   - If too big → split at smart boundaries (`;`, `}`, line breaks)\n   - If too small → merge with neighboring pieces\n4. **Result**: Every chunk is meaningful code that fits in context window\n\n**Performance**: The [research paper](https://arxiv.org/pdf/2506.15655) shows cAST provides 4.3 point gain in Recall@5 on RepoEval retrieval and 2.67 point gain in Pass@1 on SWE-bench generation tasks.\n\n\u003CAside type=\"tip\">\n**Think of code like paragraphs in an essay**. You wouldn't split a paragraph mid-sentence - cAST doesn't split code mid-statement. It keeps related logic together while respecting size limits.\n\u003C/Aside>\n\n### Why This Matters for AI\n\n- **Better Search**: Find complete functions, not fragments\n- **Better Context**: AI sees full logic flow, not half-statements\n- **Better Results**: AI gives accurate suggestions based on complete code understanding\n- **Research-Backed**: [Peer-reviewed algorithm](https://arxiv.org/pdf/2506.15655) with proven performance gains\n\nTraditional chunking gives AI puzzle pieces. cAST gives it complete pictures.\n\n**Learn More**: Read the full [cAST research paper](https://arxiv.org/pdf/2506.15655) for implementation details and benchmarks.\n\n## Semantic Search Architecture\n\nChunkHound provides two search modes depending on your embedding provider's capabilities. The system uses vector embeddings from providers like [OpenAI](https://platform.openai.com/docs/guides/embeddings), [VoyageAI](https://docs.voyageai.com/docs/embeddings), or [local models via Ollama](https://ollama.ai/).\n\n### Regular Semantic Search\n\nThe standard approach used by most embedding providers:\n\n\u003CSemanticSearchFlow client:load />\n\n**How it works**:\n1. Convert query to embedding vector\n2. Search the vector index for nearest neighbors\n3. Return top-k most similar code chunks\n\n### Multi-Hop Semantic Search\n\nTraditional semantic search finds code that directly matches your query, but real codebases are interconnected webs of relationships. When you search for \"authentication,\" you don't just want the login function—you want the password hashing, token validation, session management, and security logging that work together to make authentication complete.\n\nMulti-hop search addresses this by following semantic relationships. It starts with direct matches, then identifies similar code to expand the result set. Through iterative expansion rounds, it discovers related functionality across architectural boundaries.\n\n\u003CMultiHopSearchFlow client:load />\n\nThe process resembles following references in technical documentation. Starting with \"authentication,\" you might discover \"cryptographic hash,\" then \"salt generation,\" then \"timing attack prevention.\" Each step reveals related concepts that share semantic similarity with your original query.\n\nThe algorithm maintains focus throughout exploration by continuously reranking all discovered code against the original query. This prevents semantic drift, ensuring that expansion doesn't compromise relevance.\n\nConsider how ChunkHound discovers these semantic chains in its own codebase: a search for \"HNSW optimization\" finds the initial embedding repository code, expands to discover the DuckDB provider optimizations, then the search service coordination, and finally the indexing orchestration—a complete end-to-end picture of how vector indexing works across architectural layers.\n\n### How Multi-Hop Search Works\n\nMulti-hop search begins by retrieving more initial candidates than standard semantic search. Instead of returning just the requested number of results, it retrieves three times that amount (up to 100 total) of the top-ranked matches. This provides the reranking algorithm with more high-quality candidates to evaluate. These expanded initial results undergo immediate reranking against the original query, establishing a relevance baseline for subsequent expansion rounds.\n\nThe expansion phase takes the highest-scoring chunks as seeds to discover semantic neighbors—code that shares similar patterns, concepts, or functionality. This creates the \"hops\": from query to initial matches, then from those matches to their related code, forming chains of semantic relationships across the codebase.\n\nAfter each expansion round, the algorithm maintains focus by reranking all discovered code against the original query. This continuous relevance assessment prevents semantic drift, ensuring that multi-hop exploration doesn't compromise result quality.\n\nThe process continues iteratively until convergence detection triggers termination. Multi-hop search monitors its progress through rate-of-change analysis, ending exploration when score improvements diminish below the threshold, when computational limits are reached, or when insufficient expansion candidates remain.\n\n### Convergence Detection\n\nMulti-hop search implements several termination criteria to balance comprehensive discovery with computational efficiency. Left unchecked, semantic expansion could theoretically connect any piece of code to any other piece through enough intermediate hops—most codebases are more interconnected than they appear. The algorithm uses gradient-based convergence detection to recognize when exploration should cease.\n\nThe system monitors three key signals for termination. First, it employs **rate-of-change monitoring** similar to early stopping in machine learning: when reranking scores degrade by more than 0.15 between iterations, indicating diminishing relevance returns. This derivative-based stopping criterion is common in optimization algorithms, effectively measuring the \"convergence velocity\" of score improvements. Second, it respects computational boundaries—both execution time (5 seconds maximum) and result volume (500 candidates maximum). Third, it detects resource exhaustion when fewer than 5 high-scoring candidates remain for productive expansion.\n\nThis convergence detection creates a practical balance. The algorithm explores broadly enough to discover cross-domain relationships while terminating before semantic drift compromises result quality.\n\n\u003CAside type=\"tip\" title=\"When Multi-Hop Activates\">\nMulti-hop search automatically activates when you use providers with reranking support:\n- **[VoyageAI](https://docs.voyageai.com/docs/reranking)**: Built-in `rerank-lite-1` model\n- **Custom servers**: With reranking endpoints following [OpenAI format](https://platform.openai.com/docs/api-reference)\n- **[OpenAI](https://platform.openai.com/docs/guides/embeddings)**: Falls back to regular search (no reranking)\n\nSee [Configuration](/chunkhound/configuration/) for provider setup details.\n\u003C/Aside>\n\n## Learn More\n\n### Research & Documentation\n- **[cAST Paper](https://arxiv.org/pdf/2506.15655)** - Original research on structure-aware code chunking\n- **[MCP Specification](https://spec.modelcontextprotocol.io/)** - Protocol for AI assistant integration\n- **[Tree-sitter Documentation](https://tree-sitter.github.io/tree-sitter/)** - Universal code parsing\n\n### Database Technologies\n- **[DuckDB Documentation](https://duckdb.org/docs/)** - High-performance analytical database\n- **[LanceDB Documentation](https://lancedb.github.io/lancedb/)** - Vector database for AI applications\n- **[Apache Arrow](https://arrow.apache.org/)** - Columnar data format and interoperability\n\n### Embedding Providers\n- **[OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)** - Industry-standard embedding API\n- **[VoyageAI Documentation](https://docs.voyageai.com/)** - Code-optimized embeddings and reranking\n- **[Ollama](https://ollama.ai/)** - Local model deployment and management","src/content/docs/under-the-hood.mdx","9d8d070621492a8a","tutorial",{"id":47,"data":49,"body":55,"filePath":56,"digest":57,"deferredRender":16},{"title":50,"description":51,"editUrl":16,"head":52,"template":18,"sidebar":53,"pagefind":16,"draft":28},"ChunkHound Tutorial","Learn ChunkHound in 5 minutes - from installation to advanced search",[],{"hidden":28,"attrs":54},{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\nimport { Card, CardGrid, Aside } from '@astrojs/starlight/components';\n\n## Understanding ChunkHound\n\nChunkHound transforms your codebase into a searchable knowledge base for AI assistants. It provides two powerful search methods:\n- **Semantic search** - Natural language queries that understand meaning and context\n- **Regex search** - Precise pattern matching for exact code structures\n\n## The Index Command - Your Knowledge Base\n\n### Why Index Separately?\n\nFor large codebases, indexing is a separate step that provides significant benefits:\n\n\u003CCardGrid>\n  \u003CCard title=\"Performance\" icon=\"rocket\">\n    **Index once, search many times**\n    Initial indexing takes time, but subsequent searches are instant\n  \u003C/Card>\n\n  \u003CCard title=\"Smart Diffing\" icon=\"puzzle\">\n    **Only processes changed files**\n    Preserves embeddings for unchanged code\n  \u003C/Card>\n\n  \u003CCard title=\"Fix Command\" icon=\"setting\">\n    **Repairs inconsistencies**\n    `chunkhound index` detects and fixes database drift\n  \u003C/Card>\n\n  \u003CCard title=\"Enterprise Ready\" icon=\"approve-check\">\n    **Battle-tested scaling**\n    Used on codebases with 75k+ LOC\n  \u003C/Card>\n\u003C/CardGrid>\n\n### Example: Initial Index\n\n```bash\n$ chunkhound index /path/to/large-codebase\nScanning 10,000 files...\nProcessing 8,234 Python files, 1,766 TypeScript files...\n✓ 45,000 chunks indexed\n✓ Embeddings: 45,000 generated\n⏱️  Time: 34m 30s\n```\n\n### Example: Incremental Update\n\n```bash\n$ chunkhound index  # After editing 3 files\nDetecting changes...\n✓ 3 files modified, 8,234 files unchanged\n✓ 150 chunks updated\n✓ Embeddings: 150 generated, 45,000 reused\n⏱️  Time: 18 seconds\n```\n\n\u003CAside type=\"tip\" title=\"Performance Tip\">\nYou can re-index frequently without penalty. Run `chunkhound index` after major code changes.\n\u003C/Aside>\n\n## Choosing Your Server Mode\n\n| **Use Case** | **Mode** | **Command** |\n|--------------|----------|-------------|\n| Personal development | stdio | `chunkhound mcp` |\n| Team/production use | HTTP | `chunkhound mcp --http` |\n\n### stdio Mode - Let Your IDE Handle It\n\nYour IDE starts/stops the server automatically. The index stays in memory for instant searches. Perfect for personal development with a single IDE.\n\n```bash\nchunkhound mcp /path/to/project\n```\n\n### HTTP Mode - Shared Server\n\nYou start the server once, multiple IDEs can connect. Ideal for teams or when switching between multiple git worktrees.\n\n```bash\nchunkhound mcp /path/to/project --http --port 8000\n# Connect IDEs to http://localhost:8000\n```\n\n\u003CAside type=\"tip\">\n**Quick rule**: Use stdio for personal work, HTTP for everything else.\n\u003C/Aside>\n\n## Production Usage\n\nChunkHound is production-ready and actively tested. For detailed configuration options, see the [Configuration Guide](/chunkhound/configuration/):\n\n\u003CCardGrid>\n  \u003CCard title=\"Enterprise Scale\" icon=\"star\">\n    **75k+ LOC indexed**\n\n    Proven on massive monorepos with complex dependency graphs\n  \u003C/Card>\n\n  \u003CCard title=\"Real-World Testing\" icon=\"laptop\">\n    **Enterprise Validated**\n\n    Tested on multiple enterprise projects and [GoatDB's](https://goatdb.dev) TypeScript codebase. See [Configuration](/chunkhound/configuration/) for production setup.\n  \u003C/Card>\n\n  \u003CCard title=\"Multi-Language Support\" icon=\"translate\">\n    **20+ Languages**\n\n    Python, TypeScript, Go, Rust, Java, C++, and more via [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)\n  \u003C/Card>\n\n  \u003CCard title=\"AI-Built Architecture\" icon=\"rocket\">\n    **100% AI-Generated**\n\n    Entire codebase written by AI agents, using [cAST algorithm](https://arxiv.org/pdf/2506.15655) for intelligent code chunking\n  \u003C/Card>\n\u003C/CardGrid>\n\n## Next Steps\n\nNow that you understand ChunkHound's core concepts:\n\n1. **Start using it** - Index your codebase and connect your AI assistant\n2. **[Advanced configuration](/configuration/)** - Advanced configuration options\n3. **[Technical deep dive](/under-the-hood/)** - Understand the architecture\n\n\u003CAside type=\"note\" title=\"Questions or Issues?\">\nChunkHound is actively developed. Report issues or ask questions on [GitHub](https://github.com/ofriw/chunkhound/issues).\n\u003C/Aside>","src/content/docs/tutorial.mdx","6ecda0abbe86789a","configuration",{"id":58,"data":60,"body":66,"filePath":67,"digest":68,"deferredRender":16},{"title":61,"description":62,"editUrl":16,"head":63,"template":18,"sidebar":64,"pagefind":16,"draft":28},"Configuration","Complete reference for all ChunkHound configuration options",[],{"hidden":28,"attrs":65},{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\nimport { Card, CardGrid, Aside } from '@astrojs/starlight/components';\n\n## Configuration Sources\n\nChunkHound uses a 5-level configuration hierarchy. Each source can override the previous ones:\n\n1. **CLI arguments** (highest priority) - `--api-key`, `--model`, `--debug`\n2. **Local `.chunkhound.json`** - Project-specific config in target directory\n3. **Config file** - Specified via `--config` path or `CHUNKHOUND_CONFIG_FILE`\n4. **Environment variables** - `CHUNKHOUND_*` prefixed variables\n5. **Default values** (lowest priority) - Built-in defaults\n\n\u003CAside type=\"tip\" title=\"Configuration Discovery\">\nChunkHound automatically looks for `.chunkhound.json` in your project directory. No need to specify paths manually!\n\u003C/Aside>\n\n## Complete Configuration Schema\n\n### Full JSON Configuration\n\n```json\n{\n  \"database\": {\n    \"provider\": \"duckdb\",\n    \"path\": \"/path/to/database\"\n  },\n  \"embedding\": {\n    \"provider\": \"voyageai\",\n    \"model\": \"voyage-3.5\",\n    \"api_key\": \"pa-your-key\",\n    \"base_url\": \"https://api.voyageai.com/v1\",\n    \"rerank_model\": \"rerank-lite-1\",\n    \"rerank_url\": \"/rerank\"\n  },\n  \"indexing\": {\n    \"include\": [\"**/*.py\", \"**/*.js\", \"**/*.ts\"],\n    \"exclude\": [\"**/node_modules/**\", \"**/__pycache__/**\"]\n  },\n  \"mcp\": {\n    \"transport\": \"stdio\",\n    \"host\": \"0.0.0.0\",\n    \"port\": 3000\n  },\n  \"debug\": false\n}\n```\n\n## Database Configuration\n\n\u003CCardGrid>\n  \u003CCard title=\"DuckDB (Default)\" icon=\"database\">\n    **File**: Single `.db` file\n    **Performance**: Excellent for code search\n    **Storage**: Efficient columnar format\n    **Setup**: Zero configuration required\n  \u003C/Card>\n\n  \u003CCard title=\"LanceDB (Alternative)\" icon=\"database\">\n    **File**: Directory with multiple files\n    **Performance**: Optimized for vector operations\n    **Storage**: Native vector format\n    **Setup**: Set `\"provider\": \"lancedb\"`\n  \u003C/Card>\n\u003C/CardGrid>\n\n### Database Options\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `provider` | `\"duckdb\"` \\| `\"lancedb\"` | `\"duckdb\"` | Database engine |\n| `path` | `string` | `.chunkhound` | Database directory path |\n\n**Environment Variables**:\n- `CHUNKHOUND_DATABASE__PROVIDER` - Database provider\n- `CHUNKHOUND_DATABASE__PATH` - Database directory path\n\n**CLI Arguments**:\n- `--database-provider` - Choose database provider\n- `--db`, `--database-path` - Set database path\n\n## Embedding Configuration\n\n\u003CTabs syncKey=\"embedding-provider\">\n  \u003CTabItem label=\"VoyageAI\">\n    **Best for**: Accuracy, cost efficiency, code understanding\n\n    [VoyageAI Documentation](https://docs.voyageai.com/) | [API Reference](https://docs.voyageai.com/docs/embeddings)\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"voyageai\",\n        \"api_key\": \"pa-your-voyage-key\",\n        \"model\": \"voyage-3.5\",\n        \"rerank_model\": \"rerank-lite-1\"\n      }\n    }\n    ```\n\n    **Available Models** ([full list](https://docs.voyageai.com/docs/embeddings)):\n    - `voyage-3.5` (default) - General purpose, 1024 dimensions\n    - `voyage-code-3` - Optimized for code, 1024 dimensions\n    - `voyage-3-large` - Higher accuracy, 1024 dimensions\n    - `voyage-law-2` - Legal documents, 1024 dimensions\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"OpenAI\">\n    **Best for**: Wide compatibility and ecosystem support\n\n    [OpenAI Documentation](https://platform.openai.com/docs/) | [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"api_key\": \"sk-your-openai-key\",\n        \"model\": \"text-embedding-3-small\"\n      }\n    }\n    ```\n\n    **Available Models** ([pricing](https://openai.com/api/pricing/)):\n    - `text-embedding-3-small` (default) - Fast, 1536 dimensions\n    - `text-embedding-3-large` - Higher accuracy, 3072 dimensions\n    - `text-embedding-ada-002` - Legacy model, 1536 dimensions\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"Custom/Local\">\n    **Best for**: Privacy, custom models, local deployment\n\n    Uses OpenAI-compatible API format for maximum compatibility.\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"base_url\": \"http://localhost:11434/v1\",\n        \"model\": \"nomic-embed-text\"\n      }\n    }\n    ```\n\n    **Compatible Servers**:\n    - **[Ollama](https://ollama.ai/)** - `http://localhost:11434/v1` ([API docs](https://github.com/ollama/ollama/blob/main/docs/api.md))\n    - **[LocalAI](https://localai.io/)** - `http://localhost:8080/v1` ([setup guide](https://localai.io/basics/getting_started/))\n    - **[LM Studio](https://lmstudio.ai/)** - `http://localhost:1234/v1` ([local server docs](https://lmstudio.ai/docs/local-server))\n    - **Custom OpenAI-compatible APIs**\n  \u003C/TabItem>\n\u003C/Tabs>\n\n### Embedding Options\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `provider` | `\"openai\"` \\| `\"voyageai\"` | None | Embedding provider |\n| `model` | `string` | Provider default | Model name |\n| `api_key` | `string` | None | API key for authentication |\n| `base_url` | `string` | Provider default | Custom API base URL |\n| `rerank_model` | `string` | None | Reranking model |\n| `rerank_url` | `string` | `\"/rerank\"` | Rerank endpoint path |\n\n## Indexing Configuration\n\n### File Discovery\n\nChunkHound automatically respects `.gitignore` files and includes comprehensive defaults. File discovery uses [Tree-sitter](https://tree-sitter.github.io/tree-sitter/) for language detection:\n\n**Default Include Patterns**:\n```json\n[\n  \"**/*.py\", \"**/*.js\", \"**/*.ts\", \"**/*.tsx\", \"**/*.jsx\",\n  \"**/*.go\", \"**/*.rs\", \"**/*.java\", \"**/*.c\", \"**/*.cpp\",\n  \"**/*.h\", \"**/*.hpp\", \"**/*.cs\", \"**/*.php\", \"**/*.rb\",\n  \"**/*.swift\", \"**/*.kt\", \"**/*.scala\", \"**/*.clj\",\n  \"**/*.sh\", \"**/*.bash\", \"**/*.zsh\", \"**/*.fish\",\n  \"**/*.sql\", \"**/*.json\", \"**/*.yaml\", \"**/*.yml\",\n  \"**/*.toml\", \"**/*.xml\", \"**/*.html\", \"**/*.css\",\n  \"**/*.scss\", \"**/*.sass\", \"**/*.less\", \"**/*.md\",\n  \"**/*.rst\", \"**/*.txt\", \"**/*.dockerfile\",\n  \"**/Dockerfile*\", \"**/Makefile*\", \"**/*.mk\"\n]\n```\n\n**Default Exclude Patterns**:\n```json\n[\n  \"**/node_modules/**\", \"**/.git/**\", \"**/__pycache__/**\",\n  \"**/venv/**\", \"**/.venv/**\", \"**/dist/**\", \"**/build/**\",\n  \"**/target/**\", \"**/.vscode/**\", \"**/.idea/**\",\n  \"**/*.tmp*\", \"**/*.swp\", \"**/*.swo\", \"**/*.min.js\",\n  \"**/*.min.css\", \"**/package-lock.json\", \"**/yarn.lock\"\n]\n```\n\n### Indexing Options\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `include` | `string[]` | Comprehensive list | File patterns to include |\n| `exclude` | `string[]` | Comprehensive list | File patterns to exclude |\n\n**Environment Variables**:\n- `CHUNKHOUND_INDEXING__INCLUDE` - Comma-separated include patterns\n- `CHUNKHOUND_INDEXING__EXCLUDE` - Comma-separated exclude patterns\n\n**CLI Arguments**:\n- `--force-reindex` - Force reindexing all files\n- `--include PATTERN` - Add include pattern (can be used multiple times)\n- `--exclude PATTERN` - Add exclude pattern (can be used multiple times)\n\n## MCP Server Configuration\n\nMCP transport mode is controlled via CLI arguments when starting the server, not through configuration files.\n\n\u003CTabs syncKey=\"mcp-transport\">\n  \u003CTabItem label=\"stdio (Default)\">\n    **Best for**: IDE integrations ([Claude Desktop](https://claude.ai/), [Claude Code](https://docs.anthropic.com/claude/docs/claude-code), [Cursor](https://cursor.com/), [VS Code](https://code.visualstudio.com/))\n\n    Follows [MCP specification](https://spec.modelcontextprotocol.io/) for standard I/O transport.\n\n    ```bash\n    # Default stdio mode\n    chunkhound mcp\n\n    # Explicit stdio mode\n    chunkhound mcp --stdio\n    ```\n\n    Uses standard input/output for communication. Most IDE integrations expect this mode.\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"HTTP\">\n    **Best for**: Web applications, [VS Code extensions](https://marketplace.visualstudio.com/), debugging\n\n    Uses [MCP over HTTP](https://spec.modelcontextprotocol.io/specification/basic/transports/) transport.\n\n    ```bash\n    # HTTP mode with default port (3000)\n    chunkhound mcp --http\n\n    # HTTP mode with custom port and host\n    chunkhound mcp --http --port 8000 --host 127.0.0.1\n    ```\n\n    Runs an HTTP server for MCP communication. Easier to debug and test.\n  \u003C/TabItem>\n\u003C/Tabs>\n\n### CLI Arguments for MCP Server\n\n| Argument | Description | Example |\n|----------|-------------|---------|\n| `--stdio` | Use stdio transport (default) | `chunkhound mcp --stdio` |\n| `--http` | Use HTTP transport | `chunkhound mcp --http` |\n| `--host HOST` | Set HTTP server host | `chunkhound mcp --http --host localhost` |\n| `--port PORT` | Set HTTP server port | `chunkhound mcp --http --port 8000` |\n\n**Environment Variables** (for HTTP mode):\n- `CHUNKHOUND_MCP__HOST` - Default HTTP server host\n- `CHUNKHOUND_MCP__PORT` - Default HTTP server port\n\n## Environment Variables Reference\n\n### Naming Convention\n\nChunkHound uses a standardized naming pattern:\n- **Prefix**: `CHUNKHOUND_`\n- **Sections**: Separated by `__` (double underscore)\n- **Example**: `CHUNKHOUND_EMBEDDING__API_KEY`\n\n### Complete Environment Variables List\n\n```bash\n# Main Configuration\nCHUNKHOUND_DEBUG=true                           # Enable debug mode\nCHUNKHOUND_CONFIG_FILE=/path/to/config.json     # Config file path\n\n# Database Configuration\nCHUNKHOUND_DATABASE__PROVIDER=duckdb            # Database provider\nCHUNKHOUND_DATABASE__PATH=/custom/db/path       # Database directory\n\n# Embedding Configuration\nCHUNKHOUND_EMBEDDING__PROVIDER=voyageai         # Embedding provider\nCHUNKHOUND_EMBEDDING__API_KEY=pa-your-key       # API key\nCHUNKHOUND_EMBEDDING__BASE_URL=https://api...   # Custom base URL\nCHUNKHOUND_EMBEDDING__MODEL=voyage-3.5          # Model name\n\n# Indexing Configuration\nCHUNKHOUND_INDEXING__INCLUDE=\"*.py,*.js\"       # Include patterns\nCHUNKHOUND_INDEXING__EXCLUDE=\"*/tests/*\"       # Exclude patterns\n\n# MCP Configuration (HTTP mode only)\nCHUNKHOUND_MCP__HOST=localhost                  # Default HTTP server host\nCHUNKHOUND_MCP__PORT=8080                       # Default HTTP server port\n\n# Provider Fallback Variables\nOPENAI_API_KEY=sk-your-key                      # OpenAI API key fallback\nOPENAI_BASE_URL=https://api.openai.com/v1       # OpenAI base URL fallback\nVOYAGE_API_KEY=pa-your-key                      # VoyageAI API key fallback\n```","src/content/docs/configuration.mdx","a00defd52cd0696c"]