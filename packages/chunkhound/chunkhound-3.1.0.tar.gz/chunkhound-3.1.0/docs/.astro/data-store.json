[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.13.2","content-config-digest","9a95ec2e8398aaca","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://ofriw.github.io\",\"compressHTML\":true,\"base\":\"/chunkhound\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"where\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":false,\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null,null,null],\"rehypePlugins\":[null,[null,{\"experimentalHeadingIdCompat\":false}],null,[null,{\"themes\":[\"github-light\",\"github-dark\"],\"defaultLocale\":\"en\",\"cascadeLayer\":\"starlight.components\",\"styleOverrides\":{\"borderRadius\":\"0px\",\"borderWidth\":\"1px\",\"codePaddingBlock\":\"0.75rem\",\"codePaddingInline\":\"1rem\",\"codeFontFamily\":\"var(--__sl-font-mono)\",\"codeFontSize\":\"var(--sl-text-code)\",\"codeLineHeight\":\"var(--sl-line-height)\",\"uiFontFamily\":\"var(--__sl-font)\",\"textMarkers\":{\"lineDiffIndicatorMarginLeft\":\"0.25rem\",\"defaultChroma\":\"45\",\"backgroundOpacity\":\"60%\"}},\"plugins\":[{\"name\":\"Starlight Plugin\",\"hooks\":{}},{\"name\":\"astro-expressive-code\",\"hooks\":{}}]}]],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false},\"legacy\":{\"collections\":false},\"prefetch\":{\"prefetchAll\":true},\"i18n\":{\"defaultLocale\":\"en\",\"locales\":[\"en\"],\"routing\":{\"prefixDefaultLocale\":false,\"redirectToDefaultLocale\":false,\"fallbackType\":\"redirect\"}}}","docs",["Map",11,12,36,37,47,48,58,59],"index",{"id":11,"data":13,"body":30,"filePath":31,"assetImports":32,"digest":35,"deferredRender":16},{"title":14,"description":15,"editUrl":16,"head":17,"template":18,"hero":19,"sidebar":27,"pagefind":16,"draft":28},"ChunkHound","Semantic and regex search for codebases via MCP",true,[],"doc",{"title":20,"tagline":21,"image":22,"actions":26},"","Modern RAG for your codebase - Semantic and Regex Search via MCP",{"alt":23,"dark":24,"light":25},"ChunkHound Logo","__ASTRO_IMAGE_../../assets/wordmark-dark.svg","__ASTRO_IMAGE_../../assets/wordmark.svg",[],{"hidden":28,"attrs":29},false,{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\n\n## What it does\n\nLLMs like Claude and GPT don't know your codebase - they only know what they were trained on. Every time they help you code, they need to search your files to understand your project's specific patterns and terminology.\n\nChunkHound integrates with AI assistants via the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) to give them two ways to explore your code:\n- **Semantic search** - Finds code by meaning, so when the AI looks for \"user authentication\" it also finds your `validateLogin()` and `checkCredentials()` functions\n- **Regex search** - Pattern matching for precise code structures\n\nTraditional search was built for humans who know what they're looking for. But AI assistants start with zero knowledge about your codebase. Semantic search bridges this gap by understanding that \"database timeout\" and \"SQL connection lost\" are related concepts, even though they share no keywords.\n\n## Supported Languages\n\nChunkHound supports **22 languages** with structured parsing:\n\n- **Programming** (via [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)): Python, JavaScript, TypeScript, JSX, TSX, Java, Kotlin, Groovy, C, C++, C#, Go, Rust, Bash, MATLAB, Makefile\n- **Configuration** (via Tree-sitter): JSON, YAML, TOML, Markdown\n- **Text-based** (custom parsers): Text files, PDF\n\n## Requirements\n\n- Python 3.10+\n- [uv package manager](https://docs.astral.sh/uv/)\n- API key for semantic search (optional)\n\n## Installation\n\n```bash\n# Install uv package manager\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install ChunkHound\nuv tool install chunkhound\n```\n\n## Configuration\n\nChunkHound works **without configuration** for regex search. For semantic search, create `.chunkhound.json` in your project root:\n\n\u003CTabs syncKey=\"provider\">\n  \u003CTabItem label=\"VoyageAI\">\n    **Recommended:** Fastest, most accurate, and cost effective\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"voyageai\",\n        \"api_key\": \"pa-your-voyage-key\"\n      }\n    }\n    ```\n\n    Get API key from [VoyageAI Console](https://dash.voyageai.com/) | [Documentation](https://docs.voyageai.com/docs/embeddings)\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"OpenAI\">\n    **Best for:** Wide compatibility and ecosystem\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"api_key\": \"sk-your-openai-key\"\n      }\n    }\n    ```\n\n    Get API key from [OpenAI Platform](https://platform.openai.com/api-keys) | [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"Local\">\n    **Best for:** Privacy and offline use\n\n    ```bash\n    # Start Ollama with embedding model\n    ollama pull nomic-embed-text\n    ```\n\n    [Ollama Documentation](https://ollama.ai/) | [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"base_url\": \"http://localhost:11434/v1\",\n        \"model\": \"nomic-embed-text\"\n      }\n    }\n    ```\n\n    No API key required - completely local. Works with any OpenAI compatible endpoint.\n  \u003C/TabItem>\n\u003C/Tabs>\n\n## IDE Setup\n\nConfigure ChunkHound as an MCP server in your AI assistant:\n\n\u003CTabs syncKey=\"ide-setup\">\n  \u003CTabItem label=\"Claude Code\">\n    Add to `~/.claude.json`:\n    ```json\n    {\n      \"mcpServers\": {\n        \"chunkhound\": {\n          \"command\": \"chunkhound\",\n          \"args\": [\"mcp\"]\n        }\n      }\n    }\n    ```\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"VS Code\">\n    Add to `.vscode/mcp.json`:\n    ```json\n    {\n      \"servers\": {\n        \"chunkhound\": {\n          \"type\": \"stdio\",\n          \"command\": \"chunkhound\",\n          \"args\": [\"mcp\", \"/path/to/project\"]\n        }\n      }\n    }\n    ```\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"Cursor\">\n    Add to `.cursor/mcp.json`:\n    ```json\n    {\n      \"mcpServers\": {\n        \"chunkhound\": {\n          \"command\": \"chunkhound\",\n          \"args\": [\"mcp\", \"/path/to/project\"]\n        }\n      }\n    }\n    ```\n  \u003C/TabItem>\n\u003C/Tabs>\n\n## Initial Indexing\n\n```bash\n# Index your codebase (respects .gitignore automatically)\ncd /path/to/project && chunkhound index\n```\n\n## Documentation\n\n- [Tutorial](/chunkhound/tutorial/) - Learn ChunkHound in 5 minutes\n- [Configuration](/chunkhound/configuration/) - Complete configuration reference\n- [Under the Hood](/chunkhound/under-the-hood/) - Technical deep dive into [cAST algorithm](https://arxiv.org/pdf/2506.15655) and architecture\n\n## Links\n\n- [GitHub Repository](https://github.com/ofriw/chunkhound)\n- [Report Issues](https://github.com/ofriw/chunkhound/issues)","src/content/docs/index.mdx",[33,34],"../../assets/wordmark-dark.svg","../../assets/wordmark.svg","054187c9f405ba95","under-the-hood",{"id":36,"data":38,"body":44,"filePath":45,"digest":46,"deferredRender":16},{"title":39,"description":40,"editUrl":16,"head":41,"template":18,"sidebar":42,"pagefind":16,"draft":28},"Under the Hood","Technical deep dive into ChunkHound's architecture, algorithms, and design decisions",[],{"hidden":28,"attrs":43},{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\nimport { Card, CardGrid, Aside } from '@astrojs/starlight/components';\nimport SemanticSearchFlow from '../../components/SemanticSearchFlow.tsx';\nimport TwoHopSearchFlow from '../../components/TwoHopSearchFlow.tsx';\n\n## Architecture Overview\n\nChunkHound uses a local-first architecture with embedded databases and universal code parsing. The system is built around the [cAST (Chunking via Abstract Syntax Trees)](https://arxiv.org/pdf/2506.15655) algorithm for intelligent code segmentation:\n\n\u003CCardGrid>\n  \u003CCard title=\"Database Layer\" icon=\"setting\">\n    **[DuckDB](https://duckdb.org/)** (primary) - OLAP columnar database with HNSW vector indexing\n    **[LanceDB](https://lancedb.github.io/lancedb/)** (experimental) - Purpose-built vector database with [Apache Arrow](https://arrow.apache.org/) format\n  \u003C/Card>\n\n  \u003CCard title=\"Parsing Engine\" icon=\"open-book\">\n    **[Tree-sitter](https://tree-sitter.github.io/tree-sitter/)** - Universal AST parser supporting 20+ languages\n    **Language-agnostic** - Same semantic concepts across all languages\n  \u003C/Card>\n\n  \u003CCard title=\"Flexible Providers\" icon=\"puzzle\">\n    **Pluggable backends** - [OpenAI](https://platform.openai.com/docs/guides/embeddings), [VoyageAI](https://docs.voyageai.com/), [Ollama](https://ollama.ai/)\n    **Cloud & Local** - Run with APIs or fully offline with local models\n  \u003C/Card>\n\n  \u003CCard title=\"Advanced Algorithms\" icon=\"rocket\">\n    **[cAST](https://arxiv.org/pdf/2506.15655)** - Semantic code chunking preserving AST structure\n    **Two-Hop Search** - Context-aware search with reranking\n  \u003C/Card>\n\u003C/CardGrid>\n\nChunkHound's local-first architecture provides key advantages: **Privacy** - Your code never leaves your machine. **Speed** - No network latency or API rate limits. **Reliability** - Works offline and in air-gapped environments. **Cost** - No per-token charges for indexing large codebases.\n\n## The cAST Algorithm\n\nWhen AI assistants search your codebase, they need code split into \"chunks\" - searchable pieces small enough to understand but large enough to be meaningful. The challenge: how do you split code without breaking its logic?\n\n**Research Foundation**: ChunkHound implements the [cAST (Chunking via Abstract Syntax Trees)](https://arxiv.org/pdf/2506.15655) algorithm developed by researchers at Carnegie Mellon University and Augment Code. This approach demonstrates significant improvements in code retrieval and generation tasks.\n\n### Three Approaches Compared\n\n**1. Naive Fixed-Size Chunking**\n\nSplit every 1000 characters regardless of code structure:\n\n```python\ndef authenticate_user(username, password):\n    if not username or not password:\n        return False\n\n    hashed = hash_password(password)\n    user = database.get_u\n# CHUNK BOUNDARY CUTS HERE ❌\nser(username)\n    return user and user.password_hash == hashed\n```\n\n**Problem**: Functions get cut in half, breaking meaning.\n\n**2. Naive AST Chunking**\n\nSplit only at function/class boundaries:\n\n```python\n# Chunk 1: Tiny function (50 characters)\ndef get_name(self):\n    return self.name\n\n# Chunk 2: Massive function (5000 characters)\ndef process_entire_request(self, request):\n    # ... 200 lines of complex logic ...\n```\n\n**Problem**: Creates chunks that are too big or too small.\n\n**3. Smart cAST Algorithm (ChunkHound's Solution)**\n\nRespects code boundaries AND enforces size limits:\n\n```python\n# Right-sized chunks that preserve meaning\ndef authenticate_user(username, password):    # ✅ Complete function\n    if not username or not password:          #    fits in one chunk\n        return False\n    hashed = hash_password(password)\n    user = database.get_user(username)\n    return user and user.password_hash == hashed\n\ndef hash_password(password):                  # ✅ Small adjacent functions\ndef validate_email(email):                   #    merged together\ndef sanitize_input(data):\n    # All fit together in one chunk\n```\n\n### How cAST Works\n\nThe algorithm is surprisingly simple:\n\n1. **Parse** code into a syntax tree (AST) using [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)\n2. **Walk** the tree top-down (classes → functions → statements)\n3. **For each piece**:\n   - If it fits in size limit (1200 chars) → make it a chunk\n   - If too big → split at smart boundaries (`;`, `}`, line breaks)\n   - If too small → merge with neighboring pieces\n4. **Result**: Every chunk is meaningful code that fits in context window\n\n**Performance**: The [research paper](https://arxiv.org/pdf/2506.15655) shows cAST provides 4.3 point gain in Recall@5 on RepoEval retrieval and 2.67 point gain in Pass@1 on SWE-bench generation tasks.\n\n\u003CAside type=\"tip\">\n**Think of code like paragraphs in an essay**. You wouldn't split a paragraph mid-sentence - cAST doesn't split code mid-statement. It keeps related logic together while respecting size limits.\n\u003C/Aside>\n\n### Why This Matters for AI\n\n- **Better Search**: Find complete functions, not fragments\n- **Better Context**: AI sees full logic flow, not half-statements\n- **Better Results**: AI gives accurate suggestions based on complete code understanding\n- **Research-Backed**: [Peer-reviewed algorithm](https://arxiv.org/pdf/2506.15655) with proven performance gains\n\nTraditional chunking gives AI puzzle pieces. cAST gives it complete pictures.\n\n**Learn More**: Read the full [cAST research paper](https://arxiv.org/pdf/2506.15655) for implementation details and benchmarks.\n\n## Semantic Search Architecture\n\nChunkHound provides two search modes depending on your embedding provider's capabilities. The system uses vector embeddings from providers like [OpenAI](https://platform.openai.com/docs/guides/embeddings), [VoyageAI](https://docs.voyageai.com/docs/embeddings), or [local models via Ollama](https://ollama.ai/).\n\n### Regular Semantic Search\n\nThe standard approach used by most embedding providers:\n\n\u003CSemanticSearchFlow client:load />\n\n**How it works**:\n1. Convert query to embedding vector\n2. Search the vector index for nearest neighbors\n3. Return top-k most similar code chunks\n\n### Two-Hop Semantic Search\n\nAdvanced search for providers with reranking (VoyageAI, custom servers):\n\n\u003CTwoHopSearchFlow client:load />\n\n**Why it's better**:\n- **Semantic bridging**: Discovers related concepts through intermediate connections\n- **Example**: Search \"authentication\" → finds `validateLogin()` → discovers related `hashPassword()` through semantic similarity\n- **Context expansion**: Finds supporting functions you might not think to search for\n- **Research foundation**: Based on advanced retrieval techniques in RAG systems\n\n\u003CAside type=\"tip\" title=\"When Two-Hop Activates\">\nTwo-hop search automatically activates when you use providers with reranking support:\n- **[VoyageAI](https://docs.voyageai.com/docs/reranking)**: Built-in `rerank-lite-1` model\n- **Custom servers**: With reranking endpoints following [OpenAI format](https://platform.openai.com/docs/api-reference)\n- **[OpenAI](https://platform.openai.com/docs/guides/embeddings)**: Falls back to regular search (no reranking)\n\nSee [Configuration](/chunkhound/configuration/) for provider setup details.\n\u003C/Aside>\n\n## Learn More\n\n### Research & Documentation\n- **[cAST Paper](https://arxiv.org/pdf/2506.15655)** - Original research on structure-aware code chunking\n- **[MCP Specification](https://spec.modelcontextprotocol.io/)** - Protocol for AI assistant integration\n- **[Tree-sitter Documentation](https://tree-sitter.github.io/tree-sitter/)** - Universal code parsing\n\n### Database Technologies\n- **[DuckDB Documentation](https://duckdb.org/docs/)** - High-performance analytical database\n- **[LanceDB Documentation](https://lancedb.github.io/lancedb/)** - Vector database for AI applications\n- **[Apache Arrow](https://arrow.apache.org/)** - Columnar data format and interoperability\n\n### Embedding Providers\n- **[OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)** - Industry-standard embedding API\n- **[VoyageAI Documentation](https://docs.voyageai.com/)** - Code-optimized embeddings and reranking\n- **[Ollama](https://ollama.ai/)** - Local model deployment and management","src/content/docs/under-the-hood.mdx","07c96124ce66eaf8","tutorial",{"id":47,"data":49,"body":55,"filePath":56,"digest":57,"deferredRender":16},{"title":50,"description":51,"editUrl":16,"head":52,"template":18,"sidebar":53,"pagefind":16,"draft":28},"ChunkHound Tutorial","Learn ChunkHound in 5 minutes - from installation to advanced search",[],{"hidden":28,"attrs":54},{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\nimport { Card, CardGrid, Aside } from '@astrojs/starlight/components';\n\n## Understanding ChunkHound\n\nChunkHound transforms your codebase into a searchable knowledge base for AI assistants. It provides two powerful search methods:\n- **Semantic search** - Natural language queries that understand meaning and context\n- **Regex search** - Precise pattern matching for exact code structures\n\n## The Index Command - Your Knowledge Base\n\n### Why Index Separately?\n\nFor large codebases, indexing is a separate step that provides significant benefits:\n\n\u003CCardGrid>\n  \u003CCard title=\"Performance\" icon=\"rocket\">\n    **Index once, search many times**\n    Initial indexing takes time, but subsequent searches are instant\n  \u003C/Card>\n\n  \u003CCard title=\"Smart Diffing\" icon=\"puzzle\">\n    **Only processes changed files**\n    Preserves embeddings for unchanged code\n  \u003C/Card>\n\n  \u003CCard title=\"Fix Command\" icon=\"setting\">\n    **Repairs inconsistencies**\n    `chunkhound index` detects and fixes database drift\n  \u003C/Card>\n\n  \u003CCard title=\"Enterprise Ready\" icon=\"approve-check\">\n    **Battle-tested scaling**\n    Used on codebases with 75k+ LOC\n  \u003C/Card>\n\u003C/CardGrid>\n\n### Example: Initial Index\n\n```bash\n$ chunkhound index /path/to/large-codebase\nScanning 10,000 files...\nProcessing 8,234 Python files, 1,766 TypeScript files...\n✓ 45,000 chunks indexed\n✓ Embeddings: 45,000 generated\n⏱️  Time: 34m 30s\n```\n\n### Example: Incremental Update\n\n```bash\n$ chunkhound index  # After editing 3 files\nDetecting changes...\n✓ 3 files modified, 8,234 files unchanged\n✓ 150 chunks updated\n✓ Embeddings: 150 generated, 45,000 reused\n⏱️  Time: 18 seconds\n```\n\n\u003CAside type=\"tip\" title=\"Performance Tip\">\nYou can re-index frequently without penalty. Run `chunkhound index` after major code changes.\n\u003C/Aside>\n\n## Choosing Your Server Mode\n\n| **Use Case** | **Mode** | **Command** |\n|--------------|----------|-------------|\n| Personal development | stdio | `chunkhound mcp` |\n| Team/production use | HTTP | `chunkhound mcp --http` |\n\n### stdio Mode - Let Your IDE Handle It\n\nYour IDE starts/stops the server automatically. The index stays in memory for instant searches. Perfect for personal development with a single IDE.\n\n```bash\nchunkhound mcp /path/to/project\n```\n\n### HTTP Mode - Shared Server\n\nYou start the server once, multiple IDEs can connect. Ideal for teams or when switching between multiple git worktrees.\n\n```bash\nchunkhound mcp /path/to/project --http --port 8000\n# Connect IDEs to http://localhost:8000\n```\n\n\u003CAside type=\"tip\">\n**Quick rule**: Use stdio for personal work, HTTP for everything else.\n\u003C/Aside>\n\n## Production Usage\n\nChunkHound is production-ready and actively tested. For detailed configuration options, see the [Configuration Guide](/chunkhound/configuration/):\n\n\u003CCardGrid>\n  \u003CCard title=\"Enterprise Scale\" icon=\"star\">\n    **75k+ LOC indexed**\n\n    Proven on massive monorepos with complex dependency graphs\n  \u003C/Card>\n\n  \u003CCard title=\"Real-World Testing\" icon=\"laptop\">\n    **Enterprise Validated**\n\n    Tested on multiple enterprise projects and [GoatDB's](https://goatdb.dev) TypeScript codebase. See [Configuration](/chunkhound/configuration/) for production setup.\n  \u003C/Card>\n\n  \u003CCard title=\"Multi-Language Support\" icon=\"translate\">\n    **20+ Languages**\n\n    Python, TypeScript, Go, Rust, Java, C++, and more via [Tree-sitter](https://tree-sitter.github.io/tree-sitter/)\n  \u003C/Card>\n\n  \u003CCard title=\"AI-Built Architecture\" icon=\"rocket\">\n    **100% AI-Generated**\n\n    Entire codebase written by AI agents, using [cAST algorithm](https://arxiv.org/pdf/2506.15655) for intelligent code chunking\n  \u003C/Card>\n\u003C/CardGrid>\n\n## Next Steps\n\nNow that you understand ChunkHound's core concepts:\n\n1. **Start using it** - Index your codebase and connect your AI assistant\n2. **[Advanced configuration](/configuration/)** - Advanced configuration options\n3. **[Technical deep dive](/under-the-hood/)** - Understand the architecture\n\n\u003CAside type=\"note\" title=\"Questions or Issues?\">\nChunkHound is actively developed. Report issues or ask questions on [GitHub](https://github.com/ofriw/chunkhound/issues).\n\u003C/Aside>","src/content/docs/tutorial.mdx","6ecda0abbe86789a","configuration",{"id":58,"data":60,"body":66,"filePath":67,"digest":68,"deferredRender":16},{"title":61,"description":62,"editUrl":16,"head":63,"template":18,"sidebar":64,"pagefind":16,"draft":28},"Configuration","Complete reference for all ChunkHound configuration options",[],{"hidden":28,"attrs":65},{},"import { Tabs, TabItem } from '@astrojs/starlight/components';\nimport { Card, CardGrid, Aside } from '@astrojs/starlight/components';\n\n## Configuration Sources\n\nChunkHound uses a 5-level configuration hierarchy. Each source can override the previous ones:\n\n1. **CLI arguments** (highest priority) - `--api-key`, `--model`, `--debug`\n2. **Local `.chunkhound.json`** - Project-specific config in target directory\n3. **Config file** - Specified via `--config` path or `CHUNKHOUND_CONFIG_FILE`\n4. **Environment variables** - `CHUNKHOUND_*` prefixed variables\n5. **Default values** (lowest priority) - Built-in defaults\n\n\u003CAside type=\"tip\" title=\"Configuration Discovery\">\nChunkHound automatically looks for `.chunkhound.json` in your project directory. No need to specify paths manually!\n\u003C/Aside>\n\n## Complete Configuration Schema\n\n### Full JSON Configuration\n\n```json\n{\n  \"database\": {\n    \"provider\": \"duckdb\",\n    \"path\": \"/path/to/database\"\n  },\n  \"embedding\": {\n    \"provider\": \"voyageai\",\n    \"model\": \"voyage-3.5\",\n    \"api_key\": \"pa-your-key\",\n    \"base_url\": \"https://api.voyageai.com/v1\",\n    \"rerank_model\": \"rerank-lite-1\",\n    \"rerank_url\": \"/rerank\"\n  },\n  \"indexing\": {\n    \"include\": [\"**/*.py\", \"**/*.js\", \"**/*.ts\"],\n    \"exclude\": [\"**/node_modules/**\", \"**/__pycache__/**\"]\n  },\n  \"mcp\": {\n    \"transport\": \"stdio\",\n    \"host\": \"0.0.0.0\",\n    \"port\": 3000\n  },\n  \"debug\": false\n}\n```\n\n## Database Configuration\n\n\u003CCardGrid>\n  \u003CCard title=\"DuckDB (Default)\" icon=\"database\">\n    **File**: Single `.db` file\n    **Performance**: Excellent for code search\n    **Storage**: Efficient columnar format\n    **Setup**: Zero configuration required\n  \u003C/Card>\n\n  \u003CCard title=\"LanceDB (Alternative)\" icon=\"database\">\n    **File**: Directory with multiple files\n    **Performance**: Optimized for vector operations\n    **Storage**: Native vector format\n    **Setup**: Set `\"provider\": \"lancedb\"`\n  \u003C/Card>\n\u003C/CardGrid>\n\n### Database Options\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `provider` | `\"duckdb\"` \\| `\"lancedb\"` | `\"duckdb\"` | Database engine |\n| `path` | `string` | `.chunkhound` | Database directory path |\n\n**Environment Variables**:\n- `CHUNKHOUND_DATABASE__PROVIDER` - Database provider\n- `CHUNKHOUND_DATABASE__PATH` - Database directory path\n\n**CLI Arguments**:\n- `--database-provider` - Choose database provider\n- `--db`, `--database-path` - Set database path\n\n## Embedding Configuration\n\n\u003CTabs syncKey=\"embedding-provider\">\n  \u003CTabItem label=\"VoyageAI\">\n    **Best for**: Accuracy, cost efficiency, code understanding\n\n    [VoyageAI Documentation](https://docs.voyageai.com/) | [API Reference](https://docs.voyageai.com/docs/embeddings)\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"voyageai\",\n        \"api_key\": \"pa-your-voyage-key\",\n        \"model\": \"voyage-3.5\",\n        \"rerank_model\": \"rerank-lite-1\"\n      }\n    }\n    ```\n\n    **Available Models** ([full list](https://docs.voyageai.com/docs/embeddings)):\n    - `voyage-3.5` (default) - General purpose, 1024 dimensions\n    - `voyage-code-3` - Optimized for code, 1024 dimensions\n    - `voyage-3-large` - Higher accuracy, 1024 dimensions\n    - `voyage-law-2` - Legal documents, 1024 dimensions\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"OpenAI\">\n    **Best for**: Wide compatibility and ecosystem support\n\n    [OpenAI Documentation](https://platform.openai.com/docs/) | [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"api_key\": \"sk-your-openai-key\",\n        \"model\": \"text-embedding-3-small\"\n      }\n    }\n    ```\n\n    **Available Models** ([pricing](https://openai.com/api/pricing/)):\n    - `text-embedding-3-small` (default) - Fast, 1536 dimensions\n    - `text-embedding-3-large` - Higher accuracy, 3072 dimensions\n    - `text-embedding-ada-002` - Legacy model, 1536 dimensions\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"Custom/Local\">\n    **Best for**: Privacy, custom models, local deployment\n\n    Uses OpenAI-compatible API format for maximum compatibility.\n\n    ```json\n    {\n      \"embedding\": {\n        \"provider\": \"openai\",\n        \"base_url\": \"http://localhost:11434/v1\",\n        \"model\": \"nomic-embed-text\"\n      }\n    }\n    ```\n\n    **Compatible Servers**:\n    - **[Ollama](https://ollama.ai/)** - `http://localhost:11434/v1` ([API docs](https://github.com/ollama/ollama/blob/main/docs/api.md))\n    - **[LocalAI](https://localai.io/)** - `http://localhost:8080/v1` ([setup guide](https://localai.io/basics/getting_started/))\n    - **[LM Studio](https://lmstudio.ai/)** - `http://localhost:1234/v1` ([local server docs](https://lmstudio.ai/docs/local-server))\n    - **Custom OpenAI-compatible APIs**\n  \u003C/TabItem>\n\u003C/Tabs>\n\n### Embedding Options\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `provider` | `\"openai\"` \\| `\"voyageai\"` | None | Embedding provider |\n| `model` | `string` | Provider default | Model name |\n| `api_key` | `string` | None | API key for authentication |\n| `base_url` | `string` | Provider default | Custom API base URL |\n| `rerank_model` | `string` | None | Reranking model |\n| `rerank_url` | `string` | `\"/rerank\"` | Rerank endpoint path |\n\n## Indexing Configuration\n\n### File Discovery\n\nChunkHound automatically respects `.gitignore` files and includes comprehensive defaults. File discovery uses [Tree-sitter](https://tree-sitter.github.io/tree-sitter/) for language detection:\n\n**Default Include Patterns**:\n```json\n[\n  \"**/*.py\", \"**/*.js\", \"**/*.ts\", \"**/*.tsx\", \"**/*.jsx\",\n  \"**/*.go\", \"**/*.rs\", \"**/*.java\", \"**/*.c\", \"**/*.cpp\",\n  \"**/*.h\", \"**/*.hpp\", \"**/*.cs\", \"**/*.php\", \"**/*.rb\",\n  \"**/*.swift\", \"**/*.kt\", \"**/*.scala\", \"**/*.clj\",\n  \"**/*.sh\", \"**/*.bash\", \"**/*.zsh\", \"**/*.fish\",\n  \"**/*.sql\", \"**/*.json\", \"**/*.yaml\", \"**/*.yml\",\n  \"**/*.toml\", \"**/*.xml\", \"**/*.html\", \"**/*.css\",\n  \"**/*.scss\", \"**/*.sass\", \"**/*.less\", \"**/*.md\",\n  \"**/*.rst\", \"**/*.txt\", \"**/*.dockerfile\",\n  \"**/Dockerfile*\", \"**/Makefile*\", \"**/*.mk\"\n]\n```\n\n**Default Exclude Patterns**:\n```json\n[\n  \"**/node_modules/**\", \"**/.git/**\", \"**/__pycache__/**\",\n  \"**/venv/**\", \"**/.venv/**\", \"**/dist/**\", \"**/build/**\",\n  \"**/target/**\", \"**/.vscode/**\", \"**/.idea/**\",\n  \"**/*.tmp*\", \"**/*.swp\", \"**/*.swo\", \"**/*.min.js\",\n  \"**/*.min.css\", \"**/package-lock.json\", \"**/yarn.lock\"\n]\n```\n\n### Indexing Options\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| `include` | `string[]` | Comprehensive list | File patterns to include |\n| `exclude` | `string[]` | Comprehensive list | File patterns to exclude |\n\n**Environment Variables**:\n- `CHUNKHOUND_INDEXING__INCLUDE` - Comma-separated include patterns\n- `CHUNKHOUND_INDEXING__EXCLUDE` - Comma-separated exclude patterns\n\n**CLI Arguments**:\n- `--force-reindex` - Force reindexing all files\n- `--include PATTERN` - Add include pattern (can be used multiple times)\n- `--exclude PATTERN` - Add exclude pattern (can be used multiple times)\n\n## MCP Server Configuration\n\nMCP transport mode is controlled via CLI arguments when starting the server, not through configuration files.\n\n\u003CTabs syncKey=\"mcp-transport\">\n  \u003CTabItem label=\"stdio (Default)\">\n    **Best for**: IDE integrations ([Claude Desktop](https://claude.ai/), [Claude Code](https://docs.anthropic.com/claude/docs/claude-code), [Cursor](https://cursor.com/), [VS Code](https://code.visualstudio.com/))\n\n    Follows [MCP specification](https://spec.modelcontextprotocol.io/) for standard I/O transport.\n\n    ```bash\n    # Default stdio mode\n    chunkhound mcp\n\n    # Explicit stdio mode\n    chunkhound mcp --stdio\n    ```\n\n    Uses standard input/output for communication. Most IDE integrations expect this mode.\n  \u003C/TabItem>\n\n  \u003CTabItem label=\"HTTP\">\n    **Best for**: Web applications, [VS Code extensions](https://marketplace.visualstudio.com/), debugging\n\n    Uses [MCP over HTTP](https://spec.modelcontextprotocol.io/specification/basic/transports/) transport.\n\n    ```bash\n    # HTTP mode with default port (3000)\n    chunkhound mcp --http\n\n    # HTTP mode with custom port and host\n    chunkhound mcp --http --port 8000 --host 127.0.0.1\n    ```\n\n    Runs an HTTP server for MCP communication. Easier to debug and test.\n  \u003C/TabItem>\n\u003C/Tabs>\n\n### CLI Arguments for MCP Server\n\n| Argument | Description | Example |\n|----------|-------------|---------|\n| `--stdio` | Use stdio transport (default) | `chunkhound mcp --stdio` |\n| `--http` | Use HTTP transport | `chunkhound mcp --http` |\n| `--host HOST` | Set HTTP server host | `chunkhound mcp --http --host localhost` |\n| `--port PORT` | Set HTTP server port | `chunkhound mcp --http --port 8000` |\n\n**Environment Variables** (for HTTP mode):\n- `CHUNKHOUND_MCP__HOST` - Default HTTP server host\n- `CHUNKHOUND_MCP__PORT` - Default HTTP server port\n\n## Environment Variables Reference\n\n### Naming Convention\n\nChunkHound uses a standardized naming pattern:\n- **Prefix**: `CHUNKHOUND_`\n- **Sections**: Separated by `__` (double underscore)\n- **Example**: `CHUNKHOUND_EMBEDDING__API_KEY`\n\n### Complete Environment Variables List\n\n```bash\n# Main Configuration\nCHUNKHOUND_DEBUG=true                           # Enable debug mode\nCHUNKHOUND_CONFIG_FILE=/path/to/config.json     # Config file path\n\n# Database Configuration\nCHUNKHOUND_DATABASE__PROVIDER=duckdb            # Database provider\nCHUNKHOUND_DATABASE__PATH=/custom/db/path       # Database directory\n\n# Embedding Configuration\nCHUNKHOUND_EMBEDDING__PROVIDER=voyageai         # Embedding provider\nCHUNKHOUND_EMBEDDING__API_KEY=pa-your-key       # API key\nCHUNKHOUND_EMBEDDING__BASE_URL=https://api...   # Custom base URL\nCHUNKHOUND_EMBEDDING__MODEL=voyage-3.5          # Model name\n\n# Indexing Configuration\nCHUNKHOUND_INDEXING__INCLUDE=\"*.py,*.js\"       # Include patterns\nCHUNKHOUND_INDEXING__EXCLUDE=\"*/tests/*\"       # Exclude patterns\n\n# MCP Configuration (HTTP mode only)\nCHUNKHOUND_MCP__HOST=localhost                  # Default HTTP server host\nCHUNKHOUND_MCP__PORT=8080                       # Default HTTP server port\n\n# Provider Fallback Variables\nOPENAI_API_KEY=sk-your-key                      # OpenAI API key fallback\nOPENAI_BASE_URL=https://api.openai.com/v1       # OpenAI base URL fallback\nVOYAGE_API_KEY=pa-your-key                      # VoyageAI API key fallback\n```","src/content/docs/configuration.mdx","a00defd52cd0696c"]