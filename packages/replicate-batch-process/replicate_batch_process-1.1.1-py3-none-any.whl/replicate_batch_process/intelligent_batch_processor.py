#!/usr/bin/env python3
"""
æ™ºèƒ½æ‰¹å¤„ç†å™¨ - åˆå¹¶ç‰ˆæœ¬
æ•´åˆäº†åŠ¨æ€é˜Ÿåˆ—å’Œæ™ºèƒ½ç­–ç•¥çš„æœ€ä½³åŠŸèƒ½
"""

import asyncio
import time
import math
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from collections import deque

from .main import replicate_model_calling


@dataclass
class BatchRequest:
    """æ‰¹å¤„ç†è¯·æ±‚"""
    prompt: str
    model_name: str
    output_filepath: Optional[str] = None
    kwargs: Dict[str, Any] = None
    request_id: Optional[str] = None
    
    def __post_init__(self):
        if self.kwargs is None:
            self.kwargs = {}
        if self.request_id is None:
            import time
            self.request_id = f"req_{int(time.time()*1000)}"


@dataclass
class BatchResult:
    """æ‰¹å¤„ç†ç»“æœ"""
    request_id: str
    prompt: str
    model_name: str
    success: bool
    file_paths: Optional[List[str]] = None
    error: Optional[str] = None
    duration: Optional[float] = None


@dataclass
class QueuedTask:
    """é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
    request: BatchRequest
    future: asyncio.Future
    created_at: float = field(default_factory=time.time)
    started_at: Optional[float] = None
    attempts: int = 0


class IntelligentRateLimiter:
    """æ™ºèƒ½é€Ÿç‡é™åˆ¶å™¨ - ç²¾ç¡®çš„åŠ¨æ€é…é¢ç®¡ç†"""
    
    def __init__(self, max_requests: int = 600, window_seconds: int = 60):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.request_timestamps = deque()  # å­˜å‚¨è¯·æ±‚æ—¶é—´æˆ³
        self.lock = asyncio.Lock()
        
        print(f"ğŸ§  æ™ºèƒ½é€Ÿç‡é™åˆ¶å™¨: {max_requests}è¯·æ±‚/{window_seconds}ç§’")
    
    async def get_available_quota(self) -> int:
        """è·å–å½“å‰å¯ç”¨é…é¢"""
        async with self.lock:
            now = time.time()
            
            # æ¸…ç†è¶…å‡ºæ—¶é—´çª—å£çš„è¯·æ±‚
            while self.request_timestamps and now - self.request_timestamps[0] > self.window_seconds:
                self.request_timestamps.popleft()
            
            used_quota = len(self.request_timestamps)
            available_quota = self.max_requests - used_quota
            
            return max(0, available_quota)
    
    async def can_make_request(self) -> tuple[bool, float]:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å‘èµ·è¯·æ±‚ï¼Œè¿”å›(å¯ä»¥å‘èµ·, éœ€è¦ç­‰å¾…çš„ç§’æ•°)"""
        async with self.lock:
            now = time.time()
            
            # æ¸…ç†è¶…å‡ºçª—å£çš„æ—§è¯·æ±‚
            while self.request_timestamps and now - self.request_timestamps[0] > self.window_seconds:
                self.request_timestamps.popleft()
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é…é¢
            if len(self.request_timestamps) < self.max_requests:
                return True, 0.0
            
            # è®¡ç®—éœ€è¦ç­‰å¾…å¤šé•¿æ—¶é—´
            oldest_request = self.request_timestamps[0]
            wait_time = self.window_seconds - (now - oldest_request) + 0.1  # é¢å¤–0.1ç§’ç¼“å†²
            return False, max(0, wait_time)
    
    async def reserve_quota(self, count: int) -> bool:
        """é¢„ç•™é…é¢"""
        async with self.lock:
            now = time.time()
            
            # æ¸…ç†è¶…å‡ºæ—¶é—´çª—å£çš„è¯·æ±‚ï¼ˆå¤åˆ¶ get_available_quota çš„é€»è¾‘ï¼Œé¿å…æ­»é”ï¼‰
            while self.request_timestamps and now - self.request_timestamps[0] > self.window_seconds:
                self.request_timestamps.popleft()
            
            used_quota = len(self.request_timestamps)
            available_quota = self.max_requests - used_quota
            available = max(0, available_quota)
            
            if available >= count:
                for _ in range(count):
                    self.request_timestamps.append(now)
                return True
            return False
    
    async def record_request(self):
        """è®°å½•ä¸€ä¸ªæ–°è¯·æ±‚"""
        async with self.lock:
            self.request_timestamps.append(time.time())
    
    async def wait_for_quota(self, needed: int) -> float:
        """ç­‰å¾…é…é¢å¯ç”¨ï¼Œè¿”å›éœ€è¦ç­‰å¾…çš„ç§’æ•°"""
        available = await self.get_available_quota()
        
        if available >= needed:
            return 0.0
        
        # è®¡ç®—éœ€è¦ç­‰å¾…å¤šé•¿æ—¶é—´
        now = time.time()
        
        # æ‰¾åˆ°æœ€æ—©çš„è¯·æ±‚æ—¶é—´ï¼Œè®¡ç®—ä½•æ—¶ä¼šæœ‰è¶³å¤Ÿé…é¢
        requests_to_expire = needed - available
        if len(self.request_timestamps) >= requests_to_expire:
            earliest_expire_time = self.request_timestamps[requests_to_expire - 1] + self.window_seconds
            wait_time = max(0, earliest_expire_time - now + 0.1)  # é¢å¤–0.1ç§’ç¼“å†²
            return wait_time
        
        return 0.0
    
    def get_status(self) -> dict:
        """è·å–å½“å‰çŠ¶æ€"""
        now = time.time()
        recent_requests = [t for t in self.request_timestamps if now - t <= self.window_seconds]
        
        return {
            'used_quota': len(recent_requests),
            'available_quota': self.max_requests - len(recent_requests),
            'usage_percentage': (len(recent_requests) / self.max_requests) * 100,
            'window_seconds': self.window_seconds,
            'current_requests': len(recent_requests),
            'max_requests': self.max_requests
        }


class IntelligentBatchProcessor:
    """æ™ºèƒ½æ‰¹å¤„ç†å™¨ - åˆå¹¶æœ€ä½³åŠŸèƒ½"""
    
    def __init__(self, 
                 max_concurrent: int = 8,
                 rate_limit_per_minute: int = 600,
                 max_retries: int = 3,
                 retry_delay: float = 2.0,
                 progress_callback: Optional[Callable] = None):
        
        self.max_concurrent = max_concurrent
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.progress_callback = progress_callback
        
        # æ™ºèƒ½é€Ÿç‡é™åˆ¶å™¨
        self.rate_limiter = IntelligentRateLimiter(rate_limit_per_minute, 60)
        
        # é˜Ÿåˆ—å’Œä»»åŠ¡ç®¡ç†
        self.pending_queue = asyncio.Queue()  # å¾…å¤„ç†ä»»åŠ¡é˜Ÿåˆ—
        self.active_tasks = {}  # æ­£åœ¨å¤„ç†çš„ä»»åŠ¡
        self.completed_results = []
        self.failed_tasks = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_submitted': 0,
            'total_completed': 0,
            'total_failed': 0,
            'total_retried': 0,
            'start_time': None,
            'end_time': None
        }
        
        print(f"ğŸ§  æ™ºèƒ½æ‰¹å¤„ç†å™¨åˆå§‹åŒ–:")
        print(f"   æœ€å¤§å¹¶å‘: {max_concurrent}")
        print(f"   é€Ÿç‡é™åˆ¶: {rate_limit_per_minute}/åˆ†é’Ÿ")
        print(f"   æœ€å¤§é‡è¯•: {max_retries}")
    
    async def analyze_batch_strategy(self, requests: List[BatchRequest]) -> dict:
        """åˆ†ææ‰¹å¤„ç†ç­–ç•¥"""
        total_requests = len(requests)
        current_quota = await self.rate_limiter.get_available_quota()
        
        strategy = {
            'total_requests': total_requests,
            'current_quota': current_quota,
            'can_process_all_immediately': total_requests <= current_quota,
            'recommended_approach': '',
            'estimated_completion_time': 0,
            'batches_needed': 1
        }
        
        if total_requests <= current_quota:
            # ç­–ç•¥1: å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰ä»»åŠ¡
            strategy['recommended_approach'] = 'immediate_full_batch'
            strategy['estimated_completion_time'] = max(total_requests / self.max_concurrent * 25, 60)
            print(f"ğŸ’¡ ç­–ç•¥åˆ†æ: ä»»åŠ¡é‡({total_requests}) <= å½“å‰é…é¢({current_quota})")
            print(f"   æ¨è: ä¸€æ¬¡æ€§å…¨éƒ¨å¤„ç†")
            
        elif total_requests <= self.rate_limiter.max_requests:
            # ç­–ç•¥2: å•ä¸ªæ—¶é—´çª—å£å†…å¯ä»¥å¤„ç†
            strategy['recommended_approach'] = 'single_window_batch'
            wait_time = await self.rate_limiter.wait_for_quota(total_requests)
            strategy['estimated_completion_time'] = wait_time + (total_requests / self.max_concurrent * 25)
            print(f"ğŸ’¡ ç­–ç•¥åˆ†æ: ä»»åŠ¡é‡({total_requests}) <= çª—å£é…é¢({self.rate_limiter.max_requests})")
            print(f"   æ¨è: å•çª—å£æ‰¹å¤„ç†ï¼Œç­‰å¾… {wait_time:.1f} ç§’")
            
        else:
            # ç­–ç•¥3: éœ€è¦å¤šä¸ªæ—¶é—´çª—å£ + åŠ¨æ€é˜Ÿåˆ—
            strategy['recommended_approach'] = 'dynamic_queue_batch'
            strategy['batches_needed'] = math.ceil(total_requests / self.rate_limiter.max_requests)
            strategy['estimated_completion_time'] = strategy['batches_needed'] * 70  # é¢„ä¼°æ¯ä¸ªçª—å£70ç§’
            print(f"ğŸ’¡ ç­–ç•¥åˆ†æ: ä»»åŠ¡é‡({total_requests}) > çª—å£é…é¢({self.rate_limiter.max_requests})")
            print(f"   æ¨è: åŠ¨æ€é˜Ÿåˆ—å¤„ç†ï¼Œé¢„è®¡éœ€è¦ {strategy['batches_needed']} ä¸ªçª—å£")
        
        return strategy
    
    async def _process_single_task(self, queued_task: QueuedTask) -> BatchResult:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_id = queued_task.request.request_id
        
        try:
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            can_proceed, wait_time = await self.rate_limiter.can_make_request()
            if not can_proceed:
                print(f"â³ ä»»åŠ¡ {task_id} ç­‰å¾…é€Ÿç‡é™åˆ¶: {wait_time:.1f}ç§’")
                await asyncio.sleep(wait_time)
            
            # è®°å½•è¯·æ±‚
            await self.rate_limiter.record_request()
            queued_task.started_at = time.time()
            
            # å¤„ç†è¯·æ±‚
            start_time = time.time()
            
            # è°ƒç”¨ Replicate API
            file_paths = await asyncio.to_thread(
                replicate_model_calling,
                queued_task.request.prompt,
                queued_task.request.model_name,
                output_filepath=queued_task.request.output_filepath,
                **queued_task.request.kwargs
            )
            
            duration = time.time() - start_time
            
            return BatchResult(
                request_id=task_id,
                prompt=queued_task.request.prompt,
                model_name=queued_task.request.model_name,
                success=True,
                file_paths=file_paths,
                duration=duration
            )
            
        except Exception as e:
            duration = time.time() - start_time if 'start_time' in locals() else 0
            
            return BatchResult(
                request_id=task_id,
                prompt=queued_task.request.prompt,
                model_name=queued_task.request.model_name,
                success=False,
                error=str(e),
                duration=duration
            )
    
    async def _process_task_simple(self, request: BatchRequest) -> BatchResult:
        """å¤„ç†å•ä¸ªä»»åŠ¡ï¼ˆç®€å•ç‰ˆæœ¬ï¼Œç”¨äºæ‰¹é‡å¤„ç†ï¼‰"""
        start_time = time.time()
        
        try:
            # è°ƒç”¨ Replicate API
            file_paths = await asyncio.to_thread(
                replicate_model_calling,
                request.prompt,
                request.model_name,
                output_filepath=request.output_filepath,
                **request.kwargs
            )
            
            duration = time.time() - start_time
            return BatchResult(
                request_id=request.request_id,
                prompt=request.prompt,
                model_name=request.model_name,
                success=True,
                file_paths=file_paths,
                duration=duration
            )
            
        except Exception as e:
            duration = time.time() - start_time
            return BatchResult(
                request_id=request.request_id,
                prompt=request.prompt,
                model_name=request.model_name,
                success=False,
                error=str(e),
                duration=duration
            )
    
    async def _worker(self, worker_id: int):
        """å·¥ä½œçº¿ç¨‹ - æŒç»­ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡"""
        print(f"ğŸ‘· å·¥ä½œçº¿ç¨‹ {worker_id} å¯åŠ¨")
        
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                queued_task = await self.pending_queue.get()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯åœæ­¢ä¿¡å·
                if queued_task is None:
                    print(f"ğŸ‘· å·¥ä½œçº¿ç¨‹ {worker_id} æ”¶åˆ°åœæ­¢ä¿¡å·")
                    break
                
                task_id = queued_task.request.request_id
                print(f"ğŸ‘· å·¥ä½œçº¿ç¨‹ {worker_id} å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}")
                
                # å¤„ç†ä»»åŠ¡
                result = await self._process_single_task(queued_task)
                
                # å¤„ç†ç»“æœ
                if result.success:
                    self.completed_results.append(result)
                    self.stats['total_completed'] += 1
                    print(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ ({result.duration:.1f}s)")
                    
                    if result.file_paths:
                        print(f"   ğŸ“ ç”Ÿæˆæ–‡ä»¶: {len(result.file_paths)}ä¸ª")
                else:
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
                    queued_task.attempts += 1
                    if queued_task.attempts < self.max_retries and '429' in str(result.error).lower():
                        # é‡è¯•
                        print(f"âš ï¸  ä»»åŠ¡ {task_id} å¤±è´¥ï¼Œå‡†å¤‡é‡è¯• (ç¬¬{queued_task.attempts}æ¬¡)")
                        await asyncio.sleep(self.retry_delay * queued_task.attempts)  # æŒ‡æ•°é€€é¿
                        await self.pending_queue.put(queued_task)
                        self.stats['total_retried'] += 1
                    else:
                        # æœ€ç»ˆå¤±è´¥
                        self.failed_tasks.append(result)
                        self.stats['total_failed'] += 1
                        print(f"âŒ ä»»åŠ¡ {task_id} æœ€ç»ˆå¤±è´¥: {result.error}")
                
                # é€šçŸ¥è¿›åº¦å›è°ƒ
                if self.progress_callback:
                    total_processed = self.stats['total_completed'] + self.stats['total_failed']
                    self.progress_callback(total_processed, self.stats['total_submitted'], result)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.pending_queue.task_done()
                
            except asyncio.CancelledError:
                print(f"ğŸ‘· å·¥ä½œçº¿ç¨‹ {worker_id} è¢«å–æ¶ˆ")
                break
            except Exception as e:
                print(f"âŒ å·¥ä½œçº¿ç¨‹ {worker_id} å‘ç”Ÿé”™è¯¯: {e}")
                self.pending_queue.task_done()
    
    async def _monitor_progress(self):
        """ç›‘æ§å¤„ç†è¿›åº¦"""
        try:
            while True:
                await asyncio.sleep(10)  # æ¯10ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                
                total_processed = self.stats['total_completed'] + self.stats['total_failed']
                pending_count = self.pending_queue.qsize()
                
                # è·å–é€Ÿç‡é™åˆ¶ä½¿ç”¨æƒ…å†µ
                rate_usage = self.rate_limiter.get_status()
                
                print(f"ğŸ“Š è¿›åº¦æŠ¥å‘Š:")
                print(f"   å·²å®Œæˆ: {self.stats['total_completed']}")
                print(f"   å·²å¤±è´¥: {self.stats['total_failed']}")
                print(f"   é˜Ÿåˆ—ä¸­: {pending_count}")
                print(f"   é€Ÿç‡ä½¿ç”¨: {rate_usage['current_requests']}/{rate_usage['max_requests']} ({rate_usage['usage_percentage']:.1f}%)")
                
                # å¦‚æœå…¨éƒ¨å®Œæˆï¼Œé€€å‡ºç›‘æ§
                if total_processed >= self.stats['total_submitted'] and pending_count == 0:
                    break
                    
        except asyncio.CancelledError:
            pass
    
    async def process_intelligent_batch(self, requests: List[BatchRequest]) -> List[BatchResult]:
        """æ™ºèƒ½æ‰¹å¤„ç† - æ ¹æ®ä»»åŠ¡é‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥"""
        
        if not requests:
            return []
        
        print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½æ‰¹å¤„ç† - {len(requests)} ä¸ªä»»åŠ¡")
        
        # é‡ç½®ç»Ÿè®¡
        self.stats.update({
            'total_submitted': len(requests),
            'total_completed': 0,
            'total_failed': 0,
            'total_retried': 0,
            'start_time': time.time()
        })
        
        self.completed_results = []
        self.failed_tasks = []
        
        # åˆ†æå¤„ç†ç­–ç•¥
        strategy = await self.analyze_batch_strategy(requests)
        
        if strategy['recommended_approach'] == 'immediate_full_batch':
            # ç­–ç•¥1: ç«‹å³å…¨éƒ¨å¤„ç†
            results = await self._process_immediate_full_batch(requests)
            
        elif strategy['recommended_approach'] == 'single_window_batch':
            # ç­–ç•¥2: å•çª—å£æ‰¹å¤„ç†
            results = await self._process_single_window_batch(requests)
            
        else:
            # ç­–ç•¥3: åŠ¨æ€é˜Ÿåˆ—æ‰¹å¤„ç†
            results = await self._process_dynamic_queue_batch(requests)
        
        # è®°å½•ç»“æŸæ—¶é—´
        self.stats['end_time'] = time.time()
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()
        
        return results
    
    async def _process_immediate_full_batch(self, requests: List[BatchRequest]) -> List[BatchResult]:
        """ç­–ç•¥1: ç«‹å³å¤„ç†å…¨éƒ¨ä»»åŠ¡"""
        print(f"ğŸš€ æ‰§è¡Œç­–ç•¥1: ç«‹å³å…¨éƒ¨å¤„ç†")
        
        # é¢„ç•™é…é¢
        if not await self.rate_limiter.reserve_quota(len(requests)):
            print("âŒ é…é¢é¢„ç•™å¤±è´¥")
            return []
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        start_time = time.time()
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def process_with_semaphore(request):
            async with semaphore:
                return await self._process_task_simple(request)
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰ä»»åŠ¡
        tasks = [process_with_semaphore(request) for request in requests]
        results = []
        
        print(f"ğŸ”„ å¹¶å‘å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡...")
        
        for i, coro in enumerate(asyncio.as_completed(tasks)):
            result = await coro
            results.append(result)
            
            status = "âœ…" if result.success else "âŒ"
            print(f"[{i+1}/{len(tasks)}] {status} {result.model_name} - {result.duration:.1f}s")
        
        total_time = time.time() - start_time
        successful = [r for r in results if r.success]
        failed = [r for r in results if not r.success]
        
        # æ›´æ–°ç»Ÿè®¡æ•°æ®
        self.stats['total_completed'] = len(successful)
        self.stats['total_failed'] = len(failed)
        self.completed_results = successful
        self.failed_tasks = failed
        
        print(f"âœ… ç­–ç•¥1å®Œæˆ: {len(successful)}/{len(requests)} æˆåŠŸï¼Œè€—æ—¶ {total_time:.1f}ç§’")
        return results
    
    async def _process_single_window_batch(self, requests: List[BatchRequest]) -> List[BatchResult]:
        """ç­–ç•¥2: å•çª—å£æ‰¹å¤„ç†ï¼ˆéœ€è¦ç­‰å¾…é…é¢ï¼‰"""
        print(f"ğŸš€ æ‰§è¡Œç­–ç•¥2: å•çª—å£æ‰¹å¤„ç†")
        
        # ç­‰å¾…è¶³å¤Ÿé…é¢
        wait_time = await self.rate_limiter.wait_for_quota(len(requests))
        if wait_time > 0:
            print(f"â³ ç­‰å¾…é…é¢å¯ç”¨: {wait_time:.1f} ç§’")
            await asyncio.sleep(wait_time)
        
        # é¢„ç•™é…é¢
        if not await self.rate_limiter.reserve_quota(len(requests)):
            print("âŒ é…é¢é¢„ç•™å¤±è´¥")
            return []
        
        # æ‰§è¡Œå¤„ç†ï¼ˆä¸ç­–ç•¥1ç›¸åŒï¼‰
        results = await self._process_immediate_full_batch(requests)
        return results
    
    async def _process_dynamic_queue_batch(self, requests: List[BatchRequest]) -> List[BatchResult]:
        """ç­–ç•¥3: åŠ¨æ€é˜Ÿåˆ—æ‰¹å¤„ç† - é€‚ç”¨äºè¶…å¤§ä»»åŠ¡é‡"""
        print(f"ğŸš€ æ‰§è¡Œç­–ç•¥3: åŠ¨æ€é˜Ÿåˆ—æ‰¹å¤„ç†")
        
        # å°†æ‰€æœ‰ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
        for request in requests:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
            if not request.output_filepath:
                timestamp = int(time.time() * 1000)
                model_safe = request.model_name.replace('/', '_').replace('-', '_')
                request.output_filepath = f"output/intelligent_{model_safe}_{timestamp}_{request.request_id}.jpg"
            
            queued_task = QueuedTask(request=request, future=asyncio.Future())
            await self.pending_queue.put(queued_task)
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        workers = []
        for i in range(self.max_concurrent):
            worker = asyncio.create_task(self._worker(i))
            workers.append(worker)
        
        # ç›‘æ§è¿›åº¦
        monitor_task = asyncio.create_task(self._monitor_progress())
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await self.pending_queue.join()
        
        # åœæ­¢å·¥ä½œçº¿ç¨‹
        for _ in range(self.max_concurrent):
            await self.pending_queue.put(None)  # å‘é€åœæ­¢ä¿¡å·
        
        # ç­‰å¾…å·¥ä½œçº¿ç¨‹ç»“æŸ
        await asyncio.gather(*workers, return_exceptions=True)
        
        # åœæ­¢ç›‘æ§
        monitor_task.cancel()
        
        # åˆå¹¶ç»“æœ
        all_results = self.completed_results + self.failed_tasks
        
        return all_results
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        total_time = self.stats['end_time'] - self.stats['start_time']
        
        print("\n" + "="*80)
        print("ğŸ“Š æ™ºèƒ½æ‰¹å¤„ç†å®Œæˆç»Ÿè®¡")
        print("="*80)
        print(f"æ€»ä»»åŠ¡æ•°: {self.stats['total_submitted']}")
        print(f"âœ… æˆåŠŸ: {self.stats['total_completed']}")
        print(f"âŒ å¤±è´¥: {self.stats['total_failed']}")
        print(f"ğŸ”„ é‡è¯•: {self.stats['total_retried']}")
        print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {self.stats['total_completed']/total_time:.2f} ä»»åŠ¡/ç§’")
        
        if self.completed_results:
            avg_duration = sum(r.duration for r in self.completed_results) / len(self.completed_results)
            total_files = sum(len(r.file_paths) for r in self.completed_results if r.file_paths)
            print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {total_files}ä¸ª")
            print(f"â±ï¸  å¹³å‡å¤„ç†æ—¶é—´: {avg_duration:.2f}ç§’/ä»»åŠ¡")
        
        print("="*80)


# ä¾¿æ·æ¥å£å‡½æ•°
async def intelligent_batch_process(
    prompts: List[str],
    model_name: str,
    max_concurrent: int = 8,
    output_dir: str = "output/intelligent_batch",
    **common_kwargs
) -> List[str]:
    """
    æ™ºèƒ½æ‰¹å¤„ç†æ¥å£
    
    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        model_name: æ¨¡å‹åç§°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        output_dir: è¾“å‡ºç›®å½•ï¼ˆä»…åœ¨common_kwargsä¸­æ²¡æœ‰output_filepathæ—¶ä½¿ç”¨ï¼‰
        **common_kwargs: é€šç”¨å‚æ•°ï¼ŒåŒ…æ‹¬å¯é€‰çš„output_filepath
        
    Returns:
        æˆåŠŸç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Raises:
        ValueError: å½“æ¨¡å‹ä¸è¢«æ”¯æŒæ—¶
        
    ç¤ºä¾‹:
        # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
        files = await intelligent_batch_process(prompts, model_name)
        
        # é€šè¿‡kwargsæŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼ˆæ¨èæ–¹å¼ï¼‰
        files = await intelligent_batch_process(
            prompts=["sunset", "city"], 
            model_name="flux-dev",
            output_filepath=["output/scene_01_sunset.jpg", "output/scene_02_city.jpg"]
        )
    """
    import os
    from .config import REPLICATE_MODELS
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«æ”¯æŒ
    if model_name not in REPLICATE_MODELS:
        supported_models = list(REPLICATE_MODELS.keys())
        supported_models_str = '\n'.join([f'  - {model}' for model in supported_models])
        error_message = f"Model '{model_name}' is not supported.\n\nSupported models:\n{supported_models_str}\n\nPlease use one of the supported models listed above."
        print(f"âŒ {error_message}")
        raise ValueError(error_message)
    
    # åˆ›å»ºè¯·æ±‚
    requests = []
    timestamp = int(time.time())
    
    for i, prompt in enumerate(prompts):
        # å¤„ç†è¾“å‡ºæ–‡ä»¶è·¯å¾„
        kwargs_for_this_request = common_kwargs.copy()
        
        # ç¡®å®š output_filepath
        if 'output_filepath' in common_kwargs:
            output_filepath = common_kwargs['output_filepath']
            if isinstance(output_filepath, list):
                if len(output_filepath) != len(prompts):
                    raise ValueError(f"output_filepath åˆ—è¡¨é•¿åº¦({len(output_filepath)}) å¿…é¡»ä¸ prompts é•¿åº¦({len(prompts)}) ç›¸åŒ")
                filepath_for_this_request = output_filepath[i]
            else:
                # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œåˆ™æ‰€æœ‰è¯·æ±‚ä½¿ç”¨ç›¸åŒè·¯å¾„
                filepath_for_this_request = output_filepath
            # ä» kwargs ä¸­ç§»é™¤ output_filepath
            kwargs_for_this_request.pop('output_filepath', None)
        else:
            # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
            os.makedirs(output_dir, exist_ok=True)
            model_safe = model_name.replace('/', '_').replace('-', '_')
            filename = f"intelligent_{model_safe}_{timestamp}_{i:03d}.jpg"
            filepath_for_this_request = os.path.join(output_dir, filename)
        
        request = BatchRequest(
            prompt=prompt,
            model_name=model_name,
            output_filepath=filepath_for_this_request,
            kwargs=kwargs_for_this_request,
            request_id=f"intelligent_{timestamp}_{i:03d}"
        )
        requests.append(request)
    
    # æ™ºèƒ½å¤„ç†
    processor = IntelligentBatchProcessor(max_concurrent=max_concurrent)
    results = await processor.process_intelligent_batch(requests)
    
    # è¿”å›æˆåŠŸæ–‡ä»¶
    successful_files = []
    for result in results:
        if result.success and result.file_paths:
            successful_files.extend(result.file_paths)
    
    return successful_files


# æ¼”ç¤º
async def demo_intelligent_processing():
    """æ¼”ç¤ºæ™ºèƒ½å¤„ç†"""
    
    prompts = [
        f"Intelligent test image {i+1}: beautiful landscape scene"
        for i in range(15)  # æµ‹è¯•15ä¸ªä»»åŠ¡
    ]
    
    print("ğŸ§  æ™ºèƒ½æ‰¹å¤„ç†æ¼”ç¤º")
    files = await intelligent_batch_process(
        prompts=prompts,
        model_name="black-forest-labs/flux-dev",
        max_concurrent=6,
        output_dir="output/intelligent_demo",
        aspect_ratio="16:9"
    )
    
    print(f"âœ… å®Œæˆ! ç”Ÿæˆ {len(files)} ä¸ªæ–‡ä»¶")


if __name__ == "__main__":
    asyncio.run(demo_intelligent_processing())