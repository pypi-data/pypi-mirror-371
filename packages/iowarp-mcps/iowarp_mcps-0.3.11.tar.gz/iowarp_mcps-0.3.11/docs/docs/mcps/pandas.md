---
title: Pandas MCP
description: "Pandas MCP is a Model Context Protocol server that enables LLMs to perform advanced data analysis and manipulation using the powerful Pandas library, featuring comprehensive statistical analysis, data cleaning and transformation, time series operations, multi-format data I/O (CSV, Excel, JSON, Pa..."
---

import MCPDetail from '@site/src/components/MCPDetail';

<MCPDetail 
  name="Pandas"
  icon="🐼"
  category="Data Processing"
  description="Pandas MCP is a Model Context Protocol server that enables LLMs to perform advanced data analysis and manipulation using the powerful Pandas library, featuring comprehensive statistical analysis, data cleaning and transformation, time series operations, multi-format data I/O (CSV, Excel, JSON, Parquet, HDF5), and intelligent data quality assessment for seamless data science workflows."
  version="1.0.0"
  actions={["load_data", "save_data", "statistical_summary", "correlation_analysis", "hypothesis_testing", "handle_missing_data", "clean_data", "groupby_operations", "merge_datasets", "pivot_table", "time_series_operations", "validate_data", "filter_data", "optimize_memory", "profile_data"]}
  platforms={["claude", "cursor", "vscode"]}
  keywords={["pandas", "data-analysis", "statistical-analysis", "data-science", "data-manipulation", "time-series", "data-cleaning", "data-transformation", "mcp", "llm-integration"]}
  license="MIT"
  tools={[{"name": "load_data", "description": "Load and parse data from multiple file formats with advanced options for data ingestion. \n\nThis comprehensive tool supports CSV, Excel, JSON, Parquet, and HDF5 formats with intelligent \nparsing capabilities. It provides customizable encoding detection, selective column loading, \nand efficient data processing for optimal performance.\n\n**Smart Loading Strategy**:\n1. Automatically detects file format from extension\n2. Performs encoding detection for text files\n3. Provides memory-efficient loading with chunking support\n4. Validates data integrity during loading\n5. Generates comprehensive metadata and quality reports\n\n**Supported Formats**:\n- **CSV**: Comma-separated values with customizable delimiters\n- **Excel**: .xlsx and .xls files with multi-sheet support\n- **JSON**: Structured JSON data with nested object handling\n- **Parquet**: High-performance columnar format\n- **HDF5**: Hierarchical data format for large datasets\n\n**Performance Optimization**:\n- Selective column loading to reduce memory usage\n- Row limiting for large dataset sampling\n- Automatic data type inference and optimization\n- Memory usage reporting and recommendations\n\n**Prerequisites**: File must exist and be readable\n**Tools to use after this**: profile_data() for initial analysis, clean_data() for quality improvement\n\nUse this tool when:\n- Starting data analysis workflows (\"Load my dataset\")\n- Exploring new datasets for the first time\n- Converting between different data formats\n- Sampling large datasets for initial analysis\n- Validating data structure and quality", "function_name": "load_data_tool"}, {"name": "save_data", "description": "Save processed data to multiple file formats with optimization options for storage efficiency.\n\nThis tool provides comprehensive data export capabilities with format-specific optimizations,\ncompression options, and data integrity validation. It supports all major data formats\nwith intelligent format selection and performance tuning.\n\n**Export Strategy**:\n1. Automatically selects optimal format based on data characteristics\n2. Applies compression for space efficiency\n3. Validates data integrity before and after export\n4. Provides detailed export statistics and recommendations\n5. Supports incremental updates for large datasets\n\n**Format Optimization**:\n- **CSV**: Configurable separators and encoding options\n- **Excel**: Multi-sheet support with formatting preservation\n- **JSON**: Nested structure handling with compression\n- **Parquet**: Columnar compression for analytics workloads\n- **HDF5**: Hierarchical storage for complex data structures\n\n**Performance Features**:\n- Automatic compression selection based on data type\n- Memory-efficient writing for large datasets\n- Progress tracking for long-running operations\n- Storage space optimization recommendations\n\n**Prerequisites**: Data must be in valid format\n**Tools to use before this**: clean_data() for quality assurance, optimize_memory() for large datasets\n\nUse this tool when:\n- Exporting processed data for sharing or archival\n- Converting between different data formats\n- Creating compressed versions of large datasets\n- Saving intermediate results in analysis workflows\n- Preparing data for external systems or applications", "function_name": "save_data_tool"}, {"name": "statistical_summary", "description": "Generate comprehensive statistical summaries with descriptive statistics, distribution analysis, and data profiling.\n\nThis tool provides detailed insights into data characteristics including central tendencies,\nvariability, and distribution shapes. It goes beyond basic statistics to provide actionable\ninsights for data analysis and decision-making.\n\n**Analysis Strategy**:\n1. Computes comprehensive descriptive statistics\n2. Analyzes data distributions and normality\n3. Identifies outliers and anomalies\n4. Provides data quality assessments\n5. Generates actionable insights and recommendations\n\n**Statistical Measures**:\n- **Central Tendency**: Mean, median, mode with confidence intervals\n- **Variability**: Standard deviation, variance, range, IQR\n- **Distribution**: Skewness, kurtosis, normality tests\n- **Outlier Detection**: Z-score, IQR-based, and statistical methods\n- **Data Quality**: Missing values, duplicates, consistency checks\n\n**Advanced Features**:\n- Distribution fitting and goodness-of-fit tests\n- Correlation analysis between variables\n- Seasonal pattern detection for time series\n- Categorical variable analysis and frequency distributions\n- Statistical significance testing for group comparisons\n\n**Prerequisites**: Data must be loaded and accessible\n**Tools to use after this**: correlation_analysis() for relationships, hypothesis_testing() for inference\n\nUse this tool when:\n- Exploring new datasets for the first time\n- Understanding data characteristics and quality\n- Identifying patterns and anomalies in data\n- Preparing data for modeling or analysis\n- Generating reports for stakeholders", "function_name": "statistical_summary_tool"}, {"name": "correlation_analysis", "description": "Perform comprehensive correlation analysis with multiple correlation methods and significance testing.\n\nThis tool provides detailed insights into variable relationships, dependency patterns, and \nstatistical significance of correlations. It supports multiple correlation methods and \nprovides actionable insights for feature selection and data understanding.\n\n**Correlation Methods**:\n- **Pearson**: Linear relationships between continuous variables\n- **Spearman**: Monotonic relationships and ordinal data\n- **Kendall**: Rank-based correlation for non-parametric data\n\n**Analysis Features**:\n1. Computes correlation matrices with significance testing\n2. Identifies strong positive and negative correlations\n3. Provides p-values and confidence intervals\n4. Detects multicollinearity issues\n5. Generates correlation insights and recommendations\n\n**Advanced Capabilities**:\n- Partial correlation analysis controlling for confounding variables\n- Time-lagged correlation for temporal data\n- Categorical variable association measures\n- Correlation stability analysis across data subsets\n- Feature importance ranking based on correlations\n\n**Visualization Support**:\n- Correlation heatmap data preparation\n- Network graph data for correlation relationships\n- Scatter plot recommendations for strong correlations\n- Hierarchical clustering of correlated variables\n\n**Prerequisites**: Data must contain numerical variables\n**Tools to use before this**: statistical_summary() for data overview\n**Tools to use after this**: hypothesis_testing() for statistical inference\n\nUse this tool when:\n- Exploring relationships between variables\n- Feature selection for machine learning\n- Identifying redundant or highly correlated features\n- Understanding data structure and dependencies\n- Detecting multicollinearity in regression analysis", "function_name": "correlation_analysis_tool"}, {"name": "hypothesis_testing", "description": "Perform comprehensive statistical hypothesis testing with multiple test types and advanced analysis.\n\nThis tool supports a wide range of statistical tests including t-tests, chi-square tests, \nANOVA, and normality tests. It provides statistical inference with confidence intervals,\np-values, and effect size calculations for robust decision-making.\n\n**Supported Test Types**:\n- **t_test**: One-sample, two-sample, and paired t-tests\n- **chi_square**: Independence tests for categorical variables\n- **anova**: One-way and two-way analysis of variance\n- **normality**: Shapiro-Wilk, Kolmogorov-Smirnov tests\n- **mann_whitney**: Non-parametric alternative to t-test\n\n**Statistical Inference**:\n1. Computes test statistics and p-values\n2. Provides confidence intervals for parameters\n3. Calculates effect sizes (Cohen's d, eta-squared)\n4. Performs power analysis and sample size recommendations\n5. Generates statistical interpretation and conclusions\n\n**Advanced Features**:\n- Multiple comparison corrections (Bonferroni, FDR)\n- Assumption checking and validation\n- Bootstrap confidence intervals\n- Bayesian hypothesis testing alternatives\n- Practical significance assessment\n\n**Result Interpretation**:\n- Statistical significance vs practical significance\n- Effect size interpretation guidelines\n- Power analysis and sample size adequacy\n- Assumption violation warnings and alternatives\n- Actionable conclusions and recommendations\n\n**Prerequisites**: Data must be appropriate for the chosen test\n**Tools to use before this**: statistical_summary() for data exploration\n**Tools to use after this**: Additional analysis based on test results\n\nUse this tool when:\n- Testing specific hypotheses about your data\n- Comparing groups or treatments\n- Validating assumptions for modeling\n- Making statistical inferences and decisions\n- Preparing results for publication or reporting", "function_name": "hypothesis_testing_tool"}, {"name": "handle_missing_data", "description": "Comprehensive missing data handling with multiple strategies for detection, imputation, and removal.\n\nThis tool provides sophisticated approaches to data completeness including statistical \nimputation methods, missing data pattern analysis, and intelligent handling strategies\nbased on data characteristics and analysis requirements.\n\n**Missing Data Strategies**:\n- **detect**: Comprehensive missing data analysis and pattern identification\n- **impute**: Statistical imputation using various methods\n- **remove**: Intelligent removal of missing data with impact analysis\n- **analyze**: Deep analysis of missing data patterns and mechanisms\n\n**Imputation Methods**:\n- **mean/median/mode**: Central tendency imputation for numerical/categorical data\n- **forward_fill/backward_fill**: Temporal imputation for time series\n- **interpolate**: Mathematical interpolation for smooth data\n- **regression**: Predictive imputation using other variables\n- **knn**: K-nearest neighbors imputation\n\n**Pattern Analysis**:\n1. Identifies missing data patterns (MCAR, MAR, MNAR)\n2. Analyzes correlation between missingness and other variables\n3. Provides recommendations for optimal handling strategies\n4. Assesses impact of different imputation methods\n5. Validates imputation quality and bias assessment\n\n**Quality Assurance**:\n- Before/after comparison of data completeness\n- Imputation quality metrics and validation\n- Bias assessment and correction recommendations\n- Impact analysis on downstream analysis\n- Alternative strategy suggestions\n\n**Prerequisites**: Data must be loaded and accessible\n**Tools to use after this**: clean_data() for additional quality improvements, validate_data() for verification\n\nUse this tool when:\n- Dealing with incomplete datasets\n- Preparing data for analysis or modeling\n- Understanding missing data patterns and mechanisms\n- Choosing optimal imputation strategies\n- Validating data completeness requirements", "function_name": "handle_missing_data_tool"}, {"name": "clean_data", "description": "Comprehensive data cleaning with advanced outlier detection, duplicate removal, and intelligent type conversion.\n\nThis tool provides sophisticated data quality improvement with statistical validation\nand automated data standardization. It combines multiple cleaning techniques to ensure\ndata integrity and consistency for downstream analysis.\n\n**Cleaning Operations**:\n- **Duplicate Detection**: Intelligent duplicate identification and removal\n- **Outlier Detection**: Statistical outlier identification using IQR and Z-score methods\n- **Type Conversion**: Automatic data type optimization and correction\n- **Standardization**: Data format standardization and normalization\n- **Validation**: Data integrity checks and quality assurance\n\n**Outlier Detection Methods**:\n- **IQR Method**: Interquartile range-based outlier detection\n- **Z-Score**: Standard deviation-based outlier identification\n- **Isolation Forest**: Machine learning-based anomaly detection\n- **Local Outlier Factor**: Density-based outlier detection\n- **Statistical Tests**: Grubbs test and other statistical methods\n\n**Quality Improvements**:\n1. Removes exact and near-duplicate records\n2. Standardizes data formats and representations\n3. Corrects data type inconsistencies\n4. Validates data integrity and consistency\n5. Provides detailed cleaning reports and recommendations\n\n**Performance Optimization**:\n- Memory-efficient processing for large datasets\n- Parallel processing for computationally intensive operations\n- Progress tracking for long-running cleaning operations\n- Optimization recommendations for future data processing\n\n**Prerequisites**: Data must be loaded and accessible\n**Tools to use before this**: handle_missing_data() for completeness\n**Tools to use after this**: validate_data() for quality verification\n\nUse this tool when:\n- Preparing data for analysis or modeling\n- Improving data quality and consistency\n- Removing noise and anomalies from datasets\n- Standardizing data formats and types\n- Ensuring data integrity before processing", "function_name": "clean_data_tool"}, {"name": "groupby_operations", "description": "Perform sophisticated groupby operations with aggregations, transformations, and filtering.\n\nThis tool provides comprehensive data grouping capabilities with multiple aggregation\nfunctions and advanced analytical operations. It enables complex data summarization\nand analysis patterns commonly used in business intelligence and data analysis.\n\n**Grouping Strategy**:\n1. Groups data by specified columns with intelligent handling\n2. Applies multiple aggregation functions simultaneously\n3. Supports custom aggregation logic and calculations\n4. Provides group-level statistics and insights\n5. Enables hierarchical grouping and multi-level analysis\n\n**Aggregation Functions**:\n- **sum**: Total values within groups\n- **mean**: Average values with confidence intervals\n- **count**: Record counts and frequency analysis\n- **min/max**: Extreme values and range analysis\n- **std/var**: Variability measures within groups\n- **median**: Robust central tendency measures\n- **custom**: User-defined aggregation functions\n\n**Advanced Features**:\n- Multi-level grouping with hierarchical analysis\n- Conditional aggregation based on filters\n- Group-wise transformations and calculations\n- Statistical significance testing between groups\n- Performance optimization for large datasets\n\n**Filtering Integration**:\n- Pre-grouping filters for data subset analysis\n- Post-aggregation filters for result refinement\n- Dynamic filtering based on group characteristics\n- Conditional logic for complex business rules\n\n**Prerequisites**: Data must be loaded with grouping columns present\n**Tools to use after this**: statistical_summary() for group analysis, pivot_table() for cross-tabulation\n\nUse this tool when:\n- Summarizing data by categories or segments\n- Calculating group-wise statistics and metrics\n- Analyzing patterns across different data segments\n- Creating aggregated reports and dashboards\n- Performing business intelligence analysis", "function_name": "groupby_operations_tool"}, {"name": "merge_datasets", "description": "Merge and join datasets with sophisticated join operations and relationship analysis.\n\nThis tool supports all SQL-style joins (inner, outer, left, right) with comprehensive\ndata integration capabilities and merge conflict resolution. It provides intelligent\nhandling of data relationships and quality assessment of merged results.\n\n**Join Types**:\n- **inner**: Only matching records from both datasets\n- **outer**: All records from both datasets with null filling\n- **left**: All records from left dataset with matching from right\n- **right**: All records from right dataset with matching from left\n\n**Merge Strategy**:\n1. Analyzes data relationships and key distributions\n2. Validates merge keys and identifies potential issues\n3. Performs intelligent duplicate handling\n4. Provides merge statistics and quality assessment\n5. Offers optimization suggestions for large datasets\n\n**Quality Assurance**:\n- Pre-merge validation of key columns\n- Duplicate detection and handling strategies\n- Data type compatibility checking\n- Merge result validation and quality metrics\n- Performance optimization for large datasets\n\n**Relationship Analysis**:\n- One-to-one, one-to-many, many-to-many detection\n- Key distribution analysis and cardinality assessment\n- Merge effectiveness evaluation\n- Data overlap and coverage analysis\n- Referential integrity validation\n\n**Advanced Features**:\n- Fuzzy matching for approximate joins\n- Multi-column merge key support\n- Custom merge logic and transformations\n- Incremental merge support for large datasets\n- Conflict resolution strategies\n\n**Prerequisites**: Both datasets must be accessible and contain merge keys\n**Tools to use before this**: profile_data() for key analysis\n**Tools to use after this**: validate_data() for merge quality assessment\n\nUse this tool when:\n- Combining data from multiple sources\n- Enriching datasets with additional information\n- Creating comprehensive analytical datasets\n- Integrating related data tables\n- Performing data warehouse-style operations", "function_name": "merge_datasets_tool"}, {"name": "pivot_table", "description": "Create sophisticated pivot tables and cross-tabulations with advanced aggregation capabilities.\n\nThis tool provides comprehensive data summarization with multiple aggregation functions\nand hierarchical data organization. It enables complex data reshaping and analysis\npatterns commonly used in business reporting and data exploration.\n\n**Pivot Strategy**:\n1. Reshapes data from long to wide format\n2. Creates cross-tabulations with multiple dimensions\n3. Applies aggregation functions to summarize data\n4. Handles missing values and edge cases intelligently\n5. Provides hierarchical indexing for complex analysis\n\n**Aggregation Functions**:\n- **mean**: Average values with statistical significance\n- **sum**: Total values with subtotals and grand totals\n- **count**: Frequency analysis and contingency tables\n- **min/max**: Extreme value analysis\n- **std/var**: Variability measures across dimensions\n- **median**: Robust central tendency measures\n- **custom**: User-defined aggregation logic\n\n**Advanced Features**:\n- Multi-level row and column indexing\n- Percentage calculations and ratios\n- Marginal totals and subtotals\n- Missing value handling strategies\n- Performance optimization for large datasets\n\n**Cross-Tabulation Analysis**:\n- Contingency table creation and analysis\n- Chi-square tests for independence\n- Percentage breakdowns by row/column/total\n- Statistical significance testing\n- Association strength measures\n\n**Visualization Support**:\n- Data formatting for heatmaps and charts\n- Hierarchical data structure for tree maps\n- Time series pivot for trend analysis\n- Categorical analysis for bar charts\n\n**Prerequisites**: Data must contain categorical columns for pivoting\n**Tools to use before this**: groupby_operations() for preliminary analysis\n**Tools to use after this**: statistical_summary() for pivot result analysis\n\nUse this tool when:\n- Creating summary reports and dashboards\n- Analyzing data across multiple dimensions\n- Performing cross-tabulation analysis\n- Reshaping data for visualization\n- Creating business intelligence reports", "function_name": "pivot_table_tool"}, {"name": "time_series_operations", "description": "Perform comprehensive time series operations with advanced temporal analysis capabilities.\n\nThis tool supports resampling, rolling windows, lag features, trend analysis, and\nseasonality detection for temporal data insights. It provides sophisticated time\nseries analysis capabilities for forecasting and pattern recognition.\n\n**Time Series Operations**:\n- **resample**: Aggregate data at different time frequencies\n- **rolling_mean**: Moving averages with customizable windows\n- **lag**: Create lagged features for predictive modeling\n- **trend**: Trend analysis and decomposition\n- **seasonality**: Seasonal pattern detection and analysis\n\n**Temporal Analysis**:\n1. Automatic datetime parsing and validation\n2. Time series decomposition (trend, seasonal, residual)\n3. Stationarity testing and transformation\n4. Autocorrelation and partial autocorrelation analysis\n5. Seasonal pattern identification and quantification\n\n**Resampling Capabilities**:\n- **D**: Daily aggregation with business day handling\n- **W**: Weekly aggregation with customizable week start\n- **M**: Monthly aggregation with period-end alignment\n- **Q**: Quarterly analysis for business reporting\n- **Y**: Annual aggregation for long-term trends\n\n**Advanced Features**:\n- Missing timestamp handling and interpolation\n- Irregular time series processing\n- Multi-variate time series analysis\n- Change point detection\n- Anomaly detection in temporal data\n\n**Forecasting Support**:\n- Trend extrapolation and projection\n- Seasonal adjustment and normalization\n- Feature engineering for time series modeling\n- Cross-validation for temporal data\n- Performance metrics for forecasting accuracy\n\n**Prerequisites**: Data must contain datetime column\n**Tools to use before this**: load_data() with proper datetime parsing\n**Tools to use after this**: statistical_summary() for temporal pattern analysis\n\nUse this tool when:\n- Analyzing temporal patterns and trends\n- Preparing data for forecasting models\n- Detecting seasonal patterns and cycles\n- Creating time-based features for modeling\n- Performing time series decomposition and analysis", "function_name": "time_series_operations_tool"}, {"name": "validate_data", "description": "Comprehensive data validation with advanced constraint checking and quality assessment.\n\nThis tool performs range validation, consistency checks, business rule validation,\nand data integrity verification with detailed validation reports and error identification.\nIt ensures data quality and compliance with specified requirements.\n\n**Validation Types**:\n- **Range Validation**: Min/max value constraints for numerical data\n- **Type Validation**: Data type consistency and format checking\n- **Pattern Validation**: Regex pattern matching for structured data\n- **Uniqueness Validation**: Duplicate detection and uniqueness constraints\n- **Completeness Validation**: Missing value detection and null constraints\n- **Referential Integrity**: Foreign key and relationship validation\n\n**Business Rule Validation**:\n1. Custom validation rules and logic\n2. Cross-field validation and dependencies\n3. Conditional validation based on data context\n4. Industry-specific validation patterns\n5. Compliance checking for regulatory requirements\n\n**Quality Assessment**:\n- Data quality scoring and metrics\n- Validation violation severity assessment\n- Impact analysis of data quality issues\n- Recommendations for quality improvement\n- Trend analysis of data quality over time\n\n**Validation Rules Structure**:\n```\n{\n    \"column_name\": {\n        \"min\": minimum_value,\n        \"max\": maximum_value,\n        \"type\": expected_data_type,\n        \"regex\": pattern_string,\n        \"not_null\": boolean,\n        \"unique\": boolean,\n        \"in_list\": [allowed_values]\n    }\n}\n```\n\n**Error Reporting**:\n- Detailed violation reports with row-level errors\n- Statistical summary of validation results\n- Severity classification and prioritization\n- Actionable recommendations for error resolution\n- Export capabilities for validation reports\n\n**Prerequisites**: Data must be loaded and validation rules defined\n**Tools to use before this**: clean_data() for basic quality improvement\n**Tools to use after this**: Additional cleaning based on validation results\n\nUse this tool when:\n- Ensuring data quality and integrity\n- Validating data against business rules\n- Preparing data for critical applications\n- Monitoring data quality over time\n- Compliance checking and auditing", "function_name": "validate_data_tool"}, {"name": "filter_data", "description": "Advanced data filtering with sophisticated boolean indexing and conditional expressions.\n\nThis tool supports complex multi-condition filtering, logical operations, range-based\nfiltering, and pattern matching with flexible query syntax for precise data selection.\nIt provides powerful data subsetting capabilities for analysis and reporting.\n\n**Filtering Capabilities**:\n- **Comparison Operators**: eq, ne, gt, lt, ge, le for numerical comparisons\n- **Membership Operators**: in, not_in for categorical filtering\n- **Pattern Matching**: contains, regex for text-based filtering\n- **Logical Operators**: AND, OR, NOT for complex conditions\n- **Range Filtering**: Between, outside range for numerical data\n- **Null Filtering**: is_null, not_null for missing value handling\n\n**Advanced Features**:\n1. Multi-condition filtering with logical operators\n2. Dynamic filtering based on statistical thresholds\n3. Percentile-based filtering for outlier removal\n4. Time-based filtering for temporal data\n5. Categorical filtering with fuzzy matching\n\n**Filter Condition Structure**:\n```\n{\n    \"column_name\": {\n        \"operator\": \"value\"\n    }\n}\n# or simple format:\n{\n    \"column_name\": \"value\"  # defaults to equality\n}\n```\n\n**Performance Optimization**:\n- Efficient indexing for large datasets\n- Query optimization and execution planning\n- Memory-efficient filtering for large files\n- Parallel processing for complex filters\n- Progress tracking for long-running operations\n\n**Quality Assurance**:\n- Filter validation and syntax checking\n- Result set statistics and summaries\n- Data quality assessment of filtered results\n- Performance metrics and optimization suggestions\n- Export capabilities for filtered datasets\n\n**Prerequisites**: Data must be loaded and accessible\n**Tools to use before this**: profile_data() for filter planning\n**Tools to use after this**: statistical_summary() for filtered data analysis\n\nUse this tool when:\n- Selecting specific data subsets for analysis\n- Removing outliers and anomalies\n- Creating focused datasets for reporting\n- Implementing business logic and rules\n- Preparing data for specific analytical tasks", "function_name": "filter_data_tool"}, {"name": "optimize_memory", "description": "Advanced memory optimization for large datasets with intelligent type conversion and chunking strategies.\n\nThis tool provides automatic dtype optimization, memory usage analysis, sparse data\nhandling, and efficient memory allocation for optimal performance with large datasets.\nIt enables processing of datasets that exceed available memory.\n\n**Optimization Strategies**:\n- **Dtype Optimization**: Automatic conversion to memory-efficient data types\n- **Sparse Data Handling**: Efficient storage for datasets with many zeros/nulls\n- **Chunking**: Process large datasets in manageable chunks\n- **Memory Mapping**: Use memory-mapped files for very large datasets\n- **Compression**: Apply compression for storage and memory efficiency\n\n**Memory Analysis**:\n1. Detailed memory usage profiling by column and data type\n2. Identification of memory optimization opportunities\n3. Impact assessment of different optimization strategies\n4. Performance benchmarking before and after optimization\n5. Recommendations for optimal memory configuration\n\n**Chunking Strategies**:\n- **Fixed Size**: Process data in fixed-size chunks\n- **Adaptive**: Dynamic chunk sizing based on memory availability\n- **Column-wise**: Process columns independently for wide datasets\n- **Time-based**: Chunk temporal data by time periods\n- **Stratified**: Maintain data distribution across chunks\n\n**Performance Features**:\n- Parallel processing for chunk operations\n- Progress tracking for long-running optimizations\n- Memory usage monitoring and alerts\n- Automatic garbage collection and cleanup\n- Performance metrics and benchmarking\n\n**Optimization Results**:\n- Memory usage reduction statistics\n- Processing speed improvements\n- Storage space savings\n- Optimal configuration recommendations\n- Performance comparison metrics\n\n**Prerequisites**: Data must be accessible and memory usage must be a concern\n**Tools to use before this**: profile_data() for memory analysis\n**Tools to use after this**: Continue with optimized data processing\n\nUse this tool when:\n- Working with large datasets that exceed memory\n- Optimizing data processing performance\n- Reducing memory footprint for applications\n- Preparing data for memory-constrained environments\n- Improving overall system efficiency", "function_name": "optimize_memory_tool"}, {"name": "profile_data", "description": "Comprehensive data profiling with detailed statistical analysis and quality assessment.\n\nThis tool provides dataset overview including shape, data types, missing values,\nvalue distributions, statistical summaries, and data quality metrics for thorough\ndata exploration and understanding.\n\n**Profiling Components**:\n- **Dataset Overview**: Shape, size, memory usage, and basic statistics\n- **Column Analysis**: Data types, unique values, missing values, and distributions\n- **Data Quality**: Completeness, consistency, validity, and accuracy metrics\n- **Statistical Summary**: Descriptive statistics and distribution analysis\n- **Correlation Analysis**: Variable relationships and dependencies (optional)\n- **Pattern Detection**: Common patterns and anomalies in the data\n\n**Quality Assessment**:\n1. Data completeness analysis and missing value patterns\n2. Data consistency checks and validation\n3. Outlier detection and anomaly identification\n4. Duplicate analysis and record uniqueness\n5. Data freshness and currency assessment\n\n**Distribution Analysis**:\n- Frequency distributions for categorical variables\n- Histogram analysis for numerical variables\n- Percentile analysis and quartile distributions\n- Skewness and kurtosis measurements\n- Normality testing and distribution fitting\n\n**Advanced Features**:\n- Correlation matrix computation and analysis\n- Principal component analysis for dimensionality insight\n- Clustering analysis for pattern identification\n- Time series profiling for temporal data\n- Text analysis for string columns\n\n**Sampling Strategy**:\n- Intelligent sampling for large datasets\n- Stratified sampling to maintain data distribution\n- Random sampling with statistical validity\n- Time-based sampling for temporal data\n- Quality-preserving sampling techniques\n\n**Prerequisites**: Data must be loaded and accessible\n**Tools to use after this**: Based on profiling results, use appropriate cleaning or analysis tools\n\nUse this tool when:\n- Exploring new datasets for the first time\n- Understanding data characteristics and quality\n- Planning data analysis and modeling strategies\n- Documenting data for stakeholders\n- Identifying data quality issues and opportunities", "function_name": "profile_data_tool"}]}
>

### 1. Data Loading and Profiling
```
I have a large CSV file with sales data that I need to load and get a comprehensive profile including data types, missing values, and basic statistics.
```

**Tools called:**
- `load_data` - Load CSV file with intelligent format detection
- `profile_data` - Get comprehensive data profile and quality metrics
- `statistical_summary` - Generate descriptive statistics and distributions

### 2. Data Cleaning and Quality Assessment
```
My dataset has missing values and outliers that need to be handled. I also want to remove duplicates and validate the data quality.
```

**Tools called:**
- `handle_missing_data` - Impute missing values with appropriate strategies
- `clean_data` - Remove outliers, duplicates, and optimize data types
- `validate_data` - Apply business rules and data quality checks

### 3. Statistical Analysis and Correlation
```
Analyze the relationships between different variables in my dataset and perform hypothesis testing to validate my assumptions.
```

**Tools called:**
- `correlation_analysis` - Calculate correlation matrices with different methods
- `hypothesis_testing` - Perform t-tests, ANOVA, and normality tests
- `statistical_summary` - Generate comprehensive statistical insights

### 4. Data Transformation and Aggregation
```
I need to group my sales data by region and product category, then create pivot tables for cross-analysis and merge with customer data.
```

**Tools called:**
- `groupby_operations` - Group data and perform multiple aggregations
- `pivot_table` - Create pivot tables with multi-level indexing
- `merge_datasets` - Join datasets using different merge strategies

### 5. Time Series Analysis and Filtering
```
Analyze my time series data by resampling to different frequencies, calculating rolling averages, and filtering specific date ranges.
```

**Tools called:**
- `time_series_operations` - Resample, rolling windows, and lag features
- `filter_data` - Apply complex time-based filtering conditions
- `statistical_summary` - Analyze time series patterns and trends

### 6. Data Export and Memory Optimization
```
Optimize memory usage of my large dataset and export the cleaned data to multiple formats for different teams.
```

**Tools called:**
- `optimize_memory` - Reduce memory usage with dtype optimization
- `save_data` - Export to CSV, Excel, Parquet, and JSON formats
- `profile_data` - Verify optimization results and final data quality

</MCPDetail>

