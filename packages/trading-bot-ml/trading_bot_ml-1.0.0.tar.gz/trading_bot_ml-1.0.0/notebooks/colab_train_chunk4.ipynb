{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7165a27",
   "metadata": {},
   "source": [
    "# üìã Create Training Manifest\n",
    "\n",
    "Create a comprehensive manifest of the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_manifest(artifacts, timestamp, symbol, cfg, repo_path):\n",
    "    \"\"\"Create comprehensive training manifest\"\"\"\n",
    "    \n",
    "    # Create runs directory\n",
    "    runs_path = Path(repo_path) / 'runs' / f'colab-{timestamp}'\n",
    "    runs_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Collect model summaries\n",
    "    model_summaries = []\n",
    "    for artifact in artifacts:\n",
    "        # Load metadata\n",
    "        meta_file = Path(artifact['path']) / 'meta.json'\n",
    "        if meta_file.exists():\n",
    "            with open(meta_file, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "            model_summaries.append({\n",
    "                'model_id': artifact['model_id'],\n",
    "                'type': meta.get('model_type', 'unknown'),\n",
    "                'task': meta.get('task_type', 'unknown'),\n",
    "                'performance': meta.get('performance', {}),\n",
    "                'size_mb': artifact['size_mb']\n",
    "            })\n",
    "    \n",
    "    # Create manifest\n",
    "    manifest = {\n",
    "        'run_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'environment': 'google_colab',\n",
    "            'symbol': symbol,\n",
    "            'config': cfg,\n",
    "            'commit_hash': get_git_commit_hash(repo_path),\n",
    "            'dataset_hash': calculate_dataset_hash(X, y_binary)\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'n_samples': len(X),\n",
    "            'n_features': len(X.columns),\n",
    "            'feature_names': list(X.columns),\n",
    "            'class_distribution': y_binary.value_counts().to_dict(),\n",
    "            'time_range': {\n",
    "                'start': str(timestamps.iloc[0]),\n",
    "                'end': str(timestamps.iloc[-1])\n",
    "            }\n",
    "        },\n",
    "        'models': model_summaries,\n",
    "        'artifacts': {\n",
    "            'repository_path': MODEL_SAVE_REPO_PATH,\n",
    "            'drive_path': MODEL_SAVE_DRIVE_PATH if DRIVE_MOUNTED else None,\n",
    "            'total_models': len(artifacts),\n",
    "            'total_size_mb': sum(a['size_mb'] for a in artifacts)\n",
    "        },\n",
    "        'system_info': {\n",
    "            'python_version': sys.version.split()[0],\n",
    "            'packages': {\n",
    "                'lightgbm': getattr(lgb, '__version__', 'unknown'),\n",
    "                'xgboost': getattr(xgb, '__version__', 'unknown') if XGB_AVAILABLE else 'not_available',\n",
    "                'sklearn': '1.0+',  # Approximate\n",
    "                'pandas': pd.__version__,\n",
    "                'numpy': np.__version__\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_file = runs_path / 'manifest.json'\n",
    "    with open(manifest_file, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Training manifest saved to: {manifest_file}\")\n",
    "    return manifest, str(manifest_file)\n",
    "\n",
    "# Create training manifest\n",
    "manifest, manifest_path = create_training_manifest(\n",
    "    repo_artifacts, training_timestamp, SYMBOL, CFG, REPO_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e7a43",
   "metadata": {},
   "source": [
    "# üìã Display Results Summary\n",
    "\n",
    "Show a comprehensive summary of the training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88459465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_training_summary(manifest, artifacts):\n",
    "    \"\"\"Display comprehensive training summary\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ TRAINING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run Info\n",
    "    print(f\"\\nüìÖ Training Run: {manifest['run_info']['timestamp']}\")\n",
    "    print(f\"üí± Symbol: {manifest['run_info']['symbol']}\")\n",
    "    print(f\"üß™ Mode: {'Fast Test' if manifest['run_info']['config']['fast_test'] else 'Full Training'}\")\n",
    "    print(f\"üìä Dataset: {manifest['dataset_info']['n_samples']} samples, {manifest['dataset_info']['n_features']} features\")\n",
    "    \n",
    "    # Model Performance\n",
    "    print(f\"\\nüèÜ Model Performance:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for model in manifest['models']:\n",
    "        model_name = model['model_id'].replace('_', ' ').title()\n",
    "        perf = model.get('performance', {})\n",
    "        \n",
    "        if model['task'] == 'classification':\n",
    "            auc = perf.get('auc', 0)\n",
    "            print(f\"{model_name:<20} | AUC: {auc:.4f} | Size: {model['size_mb']:.1f} MB\")\n",
    "        else:\n",
    "            rmse = perf.get('rmse', 0)\n",
    "            r2 = perf.get('r2', 0)\n",
    "            print(f\"{model_name:<20} | RMSE: {rmse:.6f} | R¬≤: {r2:.4f} | Size: {model['size_mb']:.1f} MB\")\n",
    "    \n",
    "    # Artifacts\n",
    "    print(f\"\\nüì¶ Artifacts Summary:\")\n",
    "    print(f\"  Models Trained: {len(artifacts)}\")\n",
    "    print(f\"  Total Size: {sum(a['size_mb'] for a in artifacts):.1f} MB\")\n",
    "    print(f\"  Repository Path: {MODEL_SAVE_REPO_PATH}\")\n",
    "    if DRIVE_MOUNTED:\n",
    "        print(f\"  Google Drive Path: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "    \n",
    "    # File Listing\n",
    "    print(f\"\\nüìÇ Saved Artifacts:\")\n",
    "    for artifact in artifacts:\n",
    "        print(f\"  üìÅ {artifact['model_id']}/\")\n",
    "        for file in artifact['files']:\n",
    "            print(f\"     üìÑ {file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Display summary\n",
    "display_training_summary(manifest, repo_artifacts)\n",
    "\n",
    "# Display sample metadata\n",
    "if repo_artifacts:\n",
    "    sample_artifact = repo_artifacts[0]\n",
    "    sample_meta_path = Path(sample_artifact['path']) / 'meta.json'\n",
    "    \n",
    "    if sample_meta_path.exists():\n",
    "        print(f\"\\nüìÑ Sample Metadata ({sample_artifact['model_id']}):\")\n",
    "        print(\"-\" * 30)\n",
    "        with open(sample_meta_path, 'r') as f:\n",
    "            sample_meta = json.load(f)\n",
    "        \n",
    "        # Display key metadata fields\n",
    "        key_fields = ['model_name', 'task_type', 'timestamp', 'performance', 'n_features', 'n_samples']\n",
    "        for field in key_fields:\n",
    "            if field in sample_meta:\n",
    "                value = sample_meta[field]\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"  {field}: {json.dumps(value, indent=4)}\")\n",
    "                else:\n",
    "                    print(f\"  {field}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4cb29",
   "metadata": {},
   "source": [
    "# ‚úÖ Validate Model Artifacts\n",
    "\n",
    "Test that saved models can be loaded and used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_artifact(artifact_path, model_id, X_sample):\n",
    "    \"\"\"Validate that a model artifact can be loaded and used\"\"\"\n",
    "    \n",
    "    try:\n",
    "        model_path = Path(artifact_path)\n",
    "        \n",
    "        # Check files exist\n",
    "        model_file = model_path / 'model.pkl'\n",
    "        scaler_file = model_path / 'scaler.pkl'\n",
    "        meta_file = model_path / 'meta.json'\n",
    "        \n",
    "        if not all(f.exists() for f in [model_file, scaler_file, meta_file]):\n",
    "            return False, \"Missing required files\"\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(meta_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Load model and scaler\n",
    "        model = joblib.load(model_file)\n",
    "        scaler = joblib.load(scaler_file)\n",
    "        \n",
    "        # Test inference\n",
    "        X_scaled = scaler.transform(X_sample)\n",
    "        \n",
    "        if 'xgb' in metadata.get('model_type', '').lower():\n",
    "            # XGBoost model\n",
    "            dtest = xgb.DMatrix(X_scaled)\n",
    "            prediction = model.predict(dtest)\n",
    "        else:\n",
    "            # LightGBM or sklearn model\n",
    "            prediction = model.predict(X_scaled)\n",
    "        \n",
    "        # Validate prediction format\n",
    "        if metadata.get('task_type') == 'classification':\n",
    "            # Should be probabilities [0,1]\n",
    "            if prediction.min() < 0 or prediction.max() > 1:\n",
    "                # Apply sigmoid if needed\n",
    "                prediction = 1 / (1 + np.exp(-prediction))\n",
    "            result_desc = f\"P(positive) = {prediction[0]:.4f}\"\n",
    "        else:\n",
    "            # Regression - should be reasonable returns\n",
    "            result_desc = f\"Predicted return = {prediction[0]:.6f}\"\n",
    "        \n",
    "        return True, result_desc\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Error: {e}\"\n",
    "\n",
    "print(\"üîç Validating model artifacts...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test with first row of test data\n",
    "X_sample = binary_splits['X_test'].iloc[:1]\n",
    "\n",
    "validation_results = []\n",
    "for artifact in repo_artifacts:\n",
    "    model_id = artifact['model_id']\n",
    "    print(f\"\\nüîÑ Testing {model_id}...\")\n",
    "    \n",
    "    success, message = validate_model_artifact(\n",
    "        artifact['path'], model_id, X_sample\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"‚úÖ {model_id}: {message}\")\n",
    "        validation_results.append({'model_id': model_id, 'status': 'success', 'message': message})\n",
    "    else:\n",
    "        print(f\"‚ùå {model_id}: {message}\")\n",
    "        validation_results.append({'model_id': model_id, 'status': 'failed', 'message': message})\n",
    "\n",
    "# Summary\n",
    "successful_validations = sum(1 for r in validation_results if r['status'] == 'success')\n",
    "total_validations = len(validation_results)\n",
    "\n",
    "print(f\"\\nüìä Validation Summary: {successful_validations}/{total_validations} models passed\")\n",
    "\n",
    "if successful_validations == total_validations:\n",
    "    print(\"‚úÖ All model artifacts validated successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some artifacts failed validation - check the errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60f52e",
   "metadata": {},
   "source": [
    "# üì• Download Artifacts\n",
    "\n",
    "Create a ZIP file of all artifacts and download to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artifacts_zip(artifacts, manifest_path, symbol, timestamp):\n",
    "    \"\"\"Create ZIP file of all artifacts for download\"\"\"\n",
    "    \n",
    "    import zipfile\n",
    "    from pathlib import Path\n",
    "    \n",
    "    zip_filename = f\"trading_bot_models_{symbol}_{timestamp}.zip\"\n",
    "    zip_path = Path(\"/content\") / zip_filename\n",
    "    \n",
    "    print(f\"üì¶ Creating ZIP archive: {zip_filename}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add manifest\n",
    "        zipf.write(manifest_path, f\"runs/colab-{timestamp}/manifest.json\")\n",
    "        print(f\"  ‚úÖ Added manifest\")\n",
    "        \n",
    "        # Add all model artifacts\n",
    "        for artifact in artifacts:\n",
    "            model_path = Path(artifact['path'])\n",
    "            model_id = artifact['model_id']\n",
    "            \n",
    "            for file in model_path.glob('*'):\n",
    "                if file.is_file():\n",
    "                    # Create archive path maintaining structure\n",
    "                    archive_path = f\"models/{symbol}/{timestamp}/{model_id}/{file.name}\"\n",
    "                    zipf.write(file, archive_path)\n",
    "            \n",
    "            print(f\"  ‚úÖ Added {model_id} artifacts\")\n",
    "        \n",
    "        # Add summary file\n",
    "        summary_content = f\"\"\"# Trading Bot ML Training Results\n",
    "\n",
    "**Timestamp:** {timestamp}\n",
    "**Symbol:** {symbol}\n",
    "**Environment:** Google Colab\n",
    "\n",
    "## Models Trained:\n",
    "{chr(10).join([f\"- {a['model_id']} ({a['size_mb']:.1f} MB)\" for a in artifacts])}\n",
    "\n",
    "## Usage Instructions:\n",
    "\n",
    "1. Extract this ZIP file to your local repository\n",
    "2. Models are organized as: models/{symbol}/{timestamp}/{model_id}/\n",
    "3. Each model directory contains:\n",
    "   - model.pkl: The trained model\n",
    "   - scaler.pkl: Feature scaler\n",
    "   - meta.json: Model metadata and performance\n",
    "\n",
    "## Loading Models:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('models/{symbol}/{timestamp}/{{model_id}}/model.pkl')\n",
    "scaler = joblib.load('models/{symbol}/{timestamp}/{{model_id}}/scaler.pkl')\n",
    "\n",
    "# Load metadata\n",
    "with open('models/{symbol}/{timestamp}/{{model_id}}/meta.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Make predictions\n",
    "X_scaled = scaler.transform(your_features)\n",
    "predictions = model.predict(X_scaled)\n",
    "```\n",
    "\n",
    "Generated by Trading Bot ML Training Pipeline\n",
    "\"\"\"\n",
    "        \n",
    "        summary_path = \"/tmp/README.md\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(summary_content)\n",
    "        zipf.write(summary_path, \"README.md\")\n",
    "    \n",
    "    zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"üì¶ ZIP created: {zip_filename} ({zip_size_mb:.1f} MB)\")\n",
    "    \n",
    "    return str(zip_path)\n",
    "\n",
    "def trigger_download(file_path):\n",
    "    \"\"\"Trigger file download in Colab\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(f\"üì• Starting download: {Path(file_path).name}\")\n",
    "        files.download(file_path)\n",
    "        print(\"‚úÖ Download initiated - check your browser downloads\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Colab - download skipped\")\n",
    "        print(f\"File available at: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Download failed: {e}\")\n",
    "        print(f\"File available at: {file_path}\")\n",
    "\n",
    "# Create and download artifacts ZIP\n",
    "if repo_artifacts:\n",
    "    zip_path = create_artifacts_zip(repo_artifacts, manifest_path, SYMBOL, training_timestamp)\n",
    "    \n",
    "    print(\"\\nüéØ Download ready!\")\n",
    "    print(\"Click the download link above or run the next cell to trigger download.\")\n",
    "    \n",
    "    # Store zip path for download cell\n",
    "    ARTIFACTS_ZIP_PATH = zip_path\n",
    "else:\n",
    "    print(\"‚ùå No artifacts to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b75d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger download (run this cell to download the ZIP file)\n",
    "if 'ARTIFACTS_ZIP_PATH' in globals():\n",
    "    trigger_download(ARTIFACTS_ZIP_PATH)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No ZIP file available for download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90464ab3",
   "metadata": {},
   "source": [
    "# üéâ Training Complete!\n",
    "\n",
    "## ‚úÖ What Was Accomplished:\n",
    "\n",
    "1. **Repository Setup**: Cloned your trading bot repository and set up the environment\n",
    "2. **Dependencies**: Installed all required ML packages (LightGBM, XGBoost, etc.)\n",
    "3. **Data Preparation**: Created training dataset with engineered features\n",
    "4. **Model Training**: Trained baseline LightGBM and XGBoost models\n",
    "5. **Model Evaluation**: Tested models on hold-out test set\n",
    "6. **Artifact Storage**: Saved models with metadata to repository and Google Drive\n",
    "7. **Validation**: Verified all artifacts can be loaded and used for inference\n",
    "8. **Download**: Created downloadable ZIP with all artifacts\n",
    "\n",
    "## üìÅ Artifact Structure:\n",
    "\n",
    "```\n",
    "models/\n",
    "‚îú‚îÄ‚îÄ {symbol}/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ {timestamp}/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ lightgbm_binary/\n",
    "‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ model.pkl\n",
    "‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl\n",
    "‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ meta.json\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ lightgbm_regression/\n",
    "‚îÇ           ‚îú‚îÄ‚îÄ model.pkl\n",
    "‚îÇ           ‚îú‚îÄ‚îÄ scaler.pkl\n",
    "‚îÇ           ‚îî‚îÄ‚îÄ meta.json\n",
    "runs/\n",
    "‚îî‚îÄ‚îÄ colab-{timestamp}/\n",
    "    ‚îî‚îÄ‚îÄ manifest.json\n",
    "```\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "\n",
    "1. **Download** the ZIP file using the cell above\n",
    "2. **Extract** to your local repository\n",
    "3. **Integrate** with your backtesting system\n",
    "4. **Deploy** using your existing inference pipeline\n",
    "5. **Monitor** model performance in production\n",
    "\n",
    "## üîß Advanced Development:\n",
    "\n",
    "- Add **CatBoost** and **Random Forest** to the ensemble\n",
    "- Implement **stacked ensembles** for better performance\n",
    "- Use **Optuna** for hyperparameter optimization\n",
    "- Add **time series cross-validation** with purging\n",
    "- Implement **SHAP analysis** for model interpretability\n",
    "\n",
    "**Happy Trading! üìà**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
