{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ead84a7",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Training Notebook for Trading Bot ML Models\n",
    "\n",
    "This notebook provides a complete ML training pipeline that can be run in Google Colab. It will:\n",
    "\n",
    "1. **Clone your repository** or upload as ZIP\n",
    "2. **Install dependencies** with fallback handling\n",
    "3. **Mount Google Drive** for artifact storage\n",
    "4. **Train models** using your existing modules\n",
    "5. **Save artifacts** to both repo and Drive\n",
    "6. **Validate models** and generate manifest\n",
    "7. **Download results** as ZIP file\n",
    "\n",
    "## üìã Instructions:\n",
    "\n",
    "1. **Replace `GITHUB_REPO_URL`** with your actual repository URL (or upload repo as ZIP)\n",
    "2. **Set `fast_test = True`** for quick testing, `False` for full training\n",
    "3. **Run all cells** in order\n",
    "4. **Check outputs** and download your trained models\n",
    "\n",
    "‚ö†Ô∏è **Private repos**: Use personal access token in URL format: `https://TOKEN@github.com/user/repo.git`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Configuration Section - MODIFY THESE VALUES\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== USER CONFIGURATION =====\n",
    "GITHUB_REPO_URL = \"https://github.com/krish567366/bot-model.git\"  # Replace with your GitHub repo URL\n",
    "REPO_NAME = \"bot-model\"  # Your repository name\n",
    "\n",
    "# Training configuration\n",
    "SYMBOL = \"BTC-USD\"\n",
    "INTERVAL = \"1m\"\n",
    "CFG = {\n",
    "    \"fast_test\": False,     # Set to False for full training\n",
    "    \"horizon\": 5,           # Prediction horizon\n",
    "    \"pos_thresh\": 0.002,    # Positive threshold (0.2%)\n",
    "    \"n_splits\": 2,          # Cross-validation splits\n",
    "    \"seed\": 42,             # Random seed\n",
    "    \"n_estimators\": 100,    # Boosting rounds (fast_test)\n",
    "    \"n_estimators_full\": 1000  # Boosting rounds (full training)\n",
    "}\n",
    "\n",
    "# Paths (automatically configured)\n",
    "REPO_PATH = f\"/content/{REPO_NAME}\"\n",
    "MODEL_SAVE_REPO_PATH = f\"{REPO_PATH}/models/\"\n",
    "MODEL_SAVE_DRIVE_PATH = \"/content/drive/MyDrive/trading_bot_models/\"\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Global state\n",
    "DRIVE_MOUNTED = False\n",
    "MODULES_IMPORTED = False\n",
    "\n",
    "print(\"üîß Configuration loaded:\")\n",
    "print(f\"  Symbol: {SYMBOL}\")\n",
    "print(f\"  Fast test mode: {CFG['fast_test']}\")\n",
    "print(f\"  Repository: {GITHUB_REPO_URL}\")\n",
    "print(f\"  Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec93699",
   "metadata": {},
   "source": [
    "# üéØ Data Strategy: Real Market Data Integration\n",
    "\n",
    "This notebook now integrates with your **existing data pipeline** for production-quality training:\n",
    "\n",
    "## üìä **Data Source Priority:**\n",
    "1. **üåü Real Market Data** (Yahoo Finance via your pipeline)\n",
    "2. **üíæ Cached Data** (From previous runs)  \n",
    "3. **üîß Synthetic Data** (Fallback only)\n",
    "\n",
    "## üöÄ **Your Data Pipeline Features:**\n",
    "- ‚úÖ **Multi-source ingestion** (yfinance, Alpha Vantage, CCXT)\n",
    "- ‚úÖ **Data validation & cleaning**\n",
    "- ‚úÖ **Technical indicator computation** \n",
    "- ‚úÖ **Flexible storage** (SQLite/PostgreSQL)\n",
    "- ‚úÖ **Real-time & historical processing**\n",
    "- ‚úÖ **ML-ready feature engineering**\n",
    "\n",
    "## ‚ö° **What This Means:**\n",
    "- **Training on REAL market data** instead of synthetic\n",
    "- **Production-grade features** from your existing pipeline\n",
    "- **Consistent data between training and inference**\n",
    "- **Automatic fallback** if real data unavailable\n",
    "\n",
    "**Set `CFG['fast_test'] = False` for full 2-year training dataset!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e6f4a",
   "metadata": {},
   "source": [
    "# üì• Step 1: Clone Repository\n",
    "\n",
    "Clone your trading bot repository to access the training modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ffe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repository():\n",
    "    \"\"\"Clone the GitHub repository\"\"\"\n",
    "    \n",
    "    if GITHUB_REPO_URL == \"<YOUR_REPO_URL>\":\n",
    "        print(\"‚ùå Please set GITHUB_REPO_URL in the configuration section above!\")\n",
    "        print(\"   Example: GITHUB_REPO_URL = 'https://github.com/username/trading-bot.git'\")\n",
    "        print(\"   For private repos: GITHUB_REPO_URL = 'https://TOKEN@github.com/username/trading-bot.git'\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Cloning repository from {GITHUB_REPO_URL}...\")\n",
    "        \n",
    "        # Remove existing directory if present\n",
    "        if os.path.exists(REPO_PATH):\n",
    "            print(\"üìÅ Removing existing repository...\")\n",
    "            import shutil\n",
    "            shutil.rmtree(REPO_PATH)\n",
    "        \n",
    "        # Clone repository\n",
    "        clone_cmd = f\"git clone {GITHUB_REPO_URL} {REPO_PATH}\"\n",
    "        result = os.system(clone_cmd)\n",
    "        \n",
    "        if result == 0 and os.path.exists(REPO_PATH):\n",
    "            print(\"‚úÖ Repository cloned successfully\")\n",
    "            \n",
    "            # Add to Python path\n",
    "            if REPO_PATH not in sys.path:\n",
    "                sys.path.insert(0, REPO_PATH)\n",
    "                print(f\"‚úÖ Added {REPO_PATH} to Python path\")\n",
    "            \n",
    "            # Check for key files\n",
    "            key_files = [\"requirements.txt\", \"arbi/ai/\", \"arbi/core/\"]\n",
    "            for file in key_files:\n",
    "                if os.path.exists(os.path.join(REPO_PATH, file)):\n",
    "                    print(f\"  ‚úì Found {file}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Missing {file}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Repository cloning failed\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cloning repository: {e}\")\n",
    "        return False\n",
    "\n",
    "# Alternative: Upload ZIP file\n",
    "def upload_repo_zip():\n",
    "    \"\"\"Upload repository as ZIP file (alternative to git clone)\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"üìÅ Upload your repository as a ZIP file:\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        if len(uploaded) == 1:\n",
    "            zip_name = list(uploaded.keys())[0]\n",
    "            print(f\"üì¶ Extracting {zip_name}...\")\n",
    "            \n",
    "            import zipfile\n",
    "            with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(\"/content/\")\n",
    "            \n",
    "            # Find extracted directory\n",
    "            extracted_dirs = [d for d in os.listdir(\"/content/\") if os.path.isdir(f\"/content/{d}\") and d != \"sample_data\"]\n",
    "            \n",
    "            if extracted_dirs:\n",
    "                global REPO_PATH\n",
    "                REPO_PATH = f\"/content/{extracted_dirs[0]}\"\n",
    "                \n",
    "                if REPO_PATH not in sys.path:\n",
    "                    sys.path.insert(0, REPO_PATH)\n",
    "                \n",
    "                print(f\"‚úÖ Repository extracted to {REPO_PATH}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"‚ùå Could not find extracted directory\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"‚ùå Please upload exactly one ZIP file\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Colab - ZIP upload not available\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading ZIP: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clone repository (or use ZIP upload as fallback)\n",
    "clone_success = clone_repository()\n",
    "\n",
    "if not clone_success:\n",
    "    print(\"\\nüí° Alternative: Upload repository as ZIP file\")\n",
    "    print(\"   Uncomment the next line to use ZIP upload instead:\")\n",
    "    print(\"   # clone_success = upload_repo_zip()\")\n",
    "    \n",
    "    # Uncomment this line if you want to use ZIP upload:\n",
    "    # clone_success = upload_repo_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc29ff5",
   "metadata": {},
   "source": [
    "# üì¶ Step 2: Install Dependencies\n",
    "\n",
    "Install required Python packages with robust fallback handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_dependencies():\n",
    "    \"\"\"Install required dependencies with fallbacks\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Installing dependencies...\")\n",
    "    \n",
    "    # Try requirements.txt first\n",
    "    requirements_path = os.path.join(REPO_PATH, \"requirements.txt\")\n",
    "    \n",
    "    if os.path.exists(requirements_path):\n",
    "        print(\"üìÑ Found requirements.txt, installing...\")\n",
    "        result = os.system(f\"pip install -q -r {requirements_path}\")\n",
    "        \n",
    "        if result == 0:\n",
    "            print(\"‚úÖ Requirements installed from requirements.txt\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Some packages from requirements.txt failed, continuing with manual installs...\")\n",
    "    else:\n",
    "        print(\"üìÑ No requirements.txt found, installing core packages...\")\n",
    "    \n",
    "    # Core ML packages\n",
    "    core_packages = [\n",
    "        \"pandas\", \"numpy\", \"scikit-learn\", \"joblib\",\n",
    "        \"lightgbm\", \"xgboost\", \"matplotlib\", \"seaborn\",\n",
    "        \"nest_asyncio\"  # For async support in Colab\n",
    "    ]\n",
    "    \n",
    "    # Optional packages (won't fail if not installed)\n",
    "    optional_packages = [\n",
    "        \"catboost\", \"optuna\", \"shap\", \"yfinance\", \"ccxt\", \n",
    "        \"ta\", \"alpha_vantage\", \"sqlalchemy\"  # For data pipeline\n",
    "    ]\n",
    "    \n",
    "    print(\"üîÑ Installing core ML packages...\")\n",
    "    for package in core_packages:\n",
    "        try:\n",
    "            result = os.system(f\"pip install -q {package}\")\n",
    "            if result == 0:\n",
    "                print(f\"  ‚úì {package}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  {package} - failed\")\n",
    "        except:\n",
    "            print(f\"  ‚ùå {package} - error\")\n",
    "    \n",
    "    print(\"üîÑ Installing data pipeline packages...\")\n",
    "    for package in optional_packages:\n",
    "        try:\n",
    "            result = os.system(f\"pip install -q {package}\")\n",
    "            if result == 0:\n",
    "                print(f\"  ‚úì {package}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  {package} - skipped\")\n",
    "        except:\n",
    "            print(f\"  ‚ö†Ô∏è  {package} - skipped\")\n",
    "    \n",
    "    # Verify key packages\n",
    "    print(\"\\nüîç Verifying package installations...\")\n",
    "    key_imports = {\n",
    "        'pandas': 'pd',\n",
    "        'numpy': 'np',\n",
    "        'sklearn': 'sklearn',\n",
    "        'lightgbm': 'lgb',\n",
    "        'joblib': 'joblib',\n",
    "        'nest_asyncio': 'nest_asyncio',\n",
    "        'yfinance': 'yf'\n",
    "    }\n",
    "    \n",
    "    successful_imports = []\n",
    "    failed_imports = []\n",
    "    \n",
    "    for package, alias in key_imports.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "            successful_imports.append(package)\n",
    "            print(f\"  ‚úì {package}\")\n",
    "        except ImportError:\n",
    "            failed_imports.append(package)\n",
    "            print(f\"  ‚ùå {package}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully imported: {len(successful_imports)}/{len(key_imports)} key packages\")\n",
    "    \n",
    "    if failed_imports:\n",
    "        print(f\"‚ö†Ô∏è  Failed imports: {failed_imports}\")\n",
    "        print(\"   Training will continue but some features may be unavailable\")\n",
    "        \n",
    "        # Special handling for yfinance failure\n",
    "        if 'yfinance' in failed_imports:\n",
    "            print(\"   üìâ yfinance unavailable - will use synthetic data only\")\n",
    "    \n",
    "    return len(failed_imports) == 0\n",
    "\n",
    "# Install dependencies\n",
    "install_success = install_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e05be",
   "metadata": {},
   "source": [
    "# üíæ Step 3: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to save trained models for long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive safely\"\"\"\n",
    "    global DRIVE_MOUNTED\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üîÑ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Verify mount\n",
    "        if os.path.exists('/content/drive/MyDrive'):\n",
    "            print(\"‚úÖ Google Drive mounted successfully\")\n",
    "            print(f\"üìÅ Drive path: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "            \n",
    "            # Create models directory in Drive if it doesn't exist\n",
    "            os.makedirs(MODEL_SAVE_DRIVE_PATH, exist_ok=True)\n",
    "            DRIVE_MOUNTED = True\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Drive mount verification failed\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - Drive mount skipped\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Drive mount failed: {e}\")\n",
    "        print(\"Continuing without Drive backup...\")\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive\n",
    "mount_success = mount_google_drive()\n",
    "\n",
    "if mount_success:\n",
    "    print(\"üí° Models will be saved to both repo and Google Drive\")\n",
    "else:\n",
    "    print(\"üí° Models will be saved to repo only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16e643",
   "metadata": {},
   "source": [
    "# üì• Step 4: Import Modules\n",
    "\n",
    "Import the trading bot modules and verify everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trading_modules():\n",
    "    \"\"\"Import trading bot modules with fallbacks\"\"\"\n",
    "    global MODULES_IMPORTED\n",
    "    \n",
    "    print(\"üîÑ Importing trading bot modules...\")\n",
    "    \n",
    "    # Core imports\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    from datetime import datetime, timedelta\n",
    "    import json\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(CFG['seed'])\n",
    "    \n",
    "    # Try to import trading bot modules\n",
    "    modules = {}\n",
    "    \n",
    "    try:\n",
    "        # Real data integration (NEW)\n",
    "        try:\n",
    "            from arbi.ai.real_data_integration import MLDataIntegrator, get_real_training_data\n",
    "            modules['real_data'] = MLDataIntegrator({\n",
    "                'data_sources': ['yfinance'],\n",
    "                'storage_path': f'{REPO_PATH}/data/',\n",
    "                'cache_enabled': True,\n",
    "                'real_time_enabled': False\n",
    "            })\n",
    "            print(\"  ‚úì ai.real_data_integration - REAL DATA ENABLED!\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Real data integration not found - creating fallback...\")\n",
    "            # Create synthetic fallback\n",
    "            class SyntheticDataIntegrator:\n",
    "                def __init__(self, config):\n",
    "                    self.config = config\n",
    "                \n",
    "                def get_training_data(self, symbol, days=30):\n",
    "                    \"\"\"Generate synthetic training data\"\"\"\n",
    "                    dates = pd.date_range(end=datetime.now(), periods=days*1440, freq='1min')\n",
    "                    \n",
    "                    # Generate realistic OHLCV data\n",
    "                    np.random.seed(42)\n",
    "                    base_price = 50000 if 'BTC' in symbol else 3000\n",
    "                    returns = np.random.normal(0, 0.002, len(dates))\n",
    "                    prices = base_price * (1 + returns).cumprod()\n",
    "                    \n",
    "                    df = pd.DataFrame({\n",
    "                        'timestamp': dates,\n",
    "                        'open': prices * (1 + np.random.normal(0, 0.001, len(dates))),\n",
    "                        'high': prices * (1 + np.abs(np.random.normal(0, 0.002, len(dates)))),\n",
    "                        'low': prices * (1 - np.abs(np.random.normal(0, 0.002, len(dates)))),\n",
    "                        'close': prices,\n",
    "                        'volume': np.random.uniform(100, 1000, len(dates))\n",
    "                    })\n",
    "                    \n",
    "                    return df\n",
    "            \n",
    "            modules['real_data'] = SyntheticDataIntegrator({})\n",
    "            print(\"  ‚úì Synthetic data integrator created\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        try:\n",
    "            from arbi.ai.feature_engineering_v2 import compute_features_deterministic\n",
    "            modules['feature_engineering'] = compute_features_deterministic\n",
    "            print(\"  ‚úì ai.feature_engineering_v2\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from arbi.ai.feature_engineering import compute_features_deterministic\n",
    "                modules['feature_engineering'] = compute_features_deterministic\n",
    "                print(\"  ‚úì ai.feature_engineering\")\n",
    "            except ImportError:\n",
    "                print(\"  ‚ö†Ô∏è  Feature engineering module not found - creating fallback...\")\n",
    "                \n",
    "                def synthetic_feature_engineering(df):\n",
    "                    \"\"\"Generate synthetic features\"\"\"\n",
    "                    # Simple technical indicators\n",
    "                    df['sma_5'] = df['close'].rolling(5).mean()\n",
    "                    df['sma_20'] = df['close'].rolling(20).mean()\n",
    "                    df['rsi'] = 50 + np.random.normal(0, 15, len(df))  # Random RSI around 50\n",
    "                    df['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "                    \n",
    "                    # Price ratios\n",
    "                    df['price_change'] = df['close'].pct_change()\n",
    "                    df['volatility'] = df['price_change'].rolling(20).std()\n",
    "                    \n",
    "                    # Generate target\n",
    "                    df['target'] = (df['close'].shift(-CFG['horizon']) > df['close'] * (1 + CFG['pos_thresh'])).astype(int)\n",
    "                    \n",
    "                    return df.dropna()\n",
    "                \n",
    "                modules['feature_engineering'] = synthetic_feature_engineering\n",
    "                print(\"  ‚úì Synthetic feature engineering created\")\n",
    "        \n",
    "        # Training modules\n",
    "        try:\n",
    "            from arbi.ai.training_v2 import train_lightgbm_model\n",
    "            modules['training'] = train_lightgbm_model\n",
    "            print(\"  ‚úì ai.training_v2\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from arbi.ai.train_lgbm import train_and_validate_lgbm\n",
    "                modules['training'] = train_and_validate_lgbm\n",
    "                print(\"  ‚úì ai.train_lgbm\")\n",
    "            except ImportError:\n",
    "                print(\"  ‚ö†Ô∏è  Training module not found - creating fallback...\")\n",
    "                \n",
    "                def synthetic_training(df, config):\n",
    "                    \"\"\"Simple LightGBM training\"\"\"\n",
    "                    from sklearn.model_selection import train_test_split\n",
    "                    import lightgbm as lgb\n",
    "                    \n",
    "                    # Features and target\n",
    "                    feature_cols = ['sma_5', 'sma_20', 'rsi', 'volume_ratio', 'price_change', 'volatility']\n",
    "                    X = df[feature_cols]\n",
    "                    y = df['target']\n",
    "                    \n",
    "                    # Train-test split\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                    \n",
    "                    # Train model\n",
    "                    model = lgb.LGBMClassifier(\n",
    "                        n_estimators=config.get('n_estimators', 100),\n",
    "                        random_state=42,\n",
    "                        verbosity=-1\n",
    "                    )\n",
    "                    model.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Simple validation\n",
    "                    train_score = model.score(X_train, y_train)\n",
    "                    test_score = model.score(X_test, y_test)\n",
    "                    \n",
    "                    return {\n",
    "                        'model': model,\n",
    "                        'train_score': train_score,\n",
    "                        'test_score': test_score,\n",
    "                        'feature_importance': dict(zip(feature_cols, model.feature_importances_))\n",
    "                    }\n",
    "                \n",
    "                modules['training'] = synthetic_training\n",
    "                print(\"  ‚úì Synthetic training function created\")\n",
    "        \n",
    "        # Model registry\n",
    "        try:\n",
    "            from arbi.ai.registry import ModelRegistry\n",
    "            modules['registry'] = ModelRegistry()\n",
    "            print(\"  ‚úì ai.registry\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Model registry not found - will save manually\")\n",
    "        \n",
    "        # Data pipeline components\n",
    "        try:\n",
    "            from arbi.core.pipeline import YFinanceSource, DataPipeline\n",
    "            modules['data_pipeline'] = DataPipeline()\n",
    "            modules['yfinance'] = YFinanceSource()\n",
    "            print(\"  ‚úì core.pipeline - Data sources available\")\n",
    "        except ImportError as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Data pipeline import error: {e}\")\n",
    "            print(\"  ‚ö†Ô∏è  Will use direct yfinance calls\")\n",
    "            \n",
    "            # Simple yfinance wrapper\n",
    "            class SimpleYFinance:\n",
    "                def fetch_data(self, symbol, period=\"30d\", interval=\"1m\"):\n",
    "                    try:\n",
    "                        import yfinance as yf\n",
    "                        ticker = yf.Ticker(symbol)\n",
    "                        df = ticker.history(period=period, interval=interval)\n",
    "                        df.reset_index(inplace=True)\n",
    "                        df.columns = df.columns.str.lower()\n",
    "                        if 'datetime' in df.columns:\n",
    "                            df.rename(columns={'datetime': 'timestamp'}, inplace=True)\n",
    "                        return df\n",
    "                    except Exception as e:\n",
    "                        print(f\"YFinance error: {e}\")\n",
    "                        return pd.DataFrame()\n",
    "            \n",
    "            modules['yfinance'] = SimpleYFinance()\n",
    "            print(\"  ‚úì Simple yfinance wrapper created\")\n",
    "        \n",
    "        # Data generation (for testing)\n",
    "        try:\n",
    "            from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "            modules['data_generator'] = generate_synthetic_ohlcv_data\n",
    "            print(\"  ‚úì Synthetic data generator\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö†Ô∏è  Will create basic synthetic data\")\n",
    "        \n",
    "        MODULES_IMPORTED = True\n",
    "        print(\"‚úÖ Module import completed\")\n",
    "        \n",
    "        # Show data source priority\n",
    "        if 'real_data' in modules:\n",
    "            print(\"\\nüéØ DATA SOURCE PRIORITY:\")\n",
    "            print(\"  1. Real market data (Yahoo Finance)\")\n",
    "            print(\"  2. Cached historical data\") \n",
    "            print(\"  3. Synthetic data (fallback)\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Using synthetic data only\")\n",
    "            \n",
    "        return modules\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error importing modules: {e}\")\n",
    "        print(\"   Will proceed with basic fallback implementations\")\n",
    "        return {}\n",
    "\n",
    "# Import modules\n",
    "trading_modules = import_trading_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e40bf",
   "metadata": {},
   "source": [
    "# üîÑ Step 5: Generate Training Data\n",
    "\n",
    "Create or load training data for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_training_data():\n",
    "    \"\"\"Generate training data - prioritizing real market data over synthetic\"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Try to use real data first\n",
    "    if 'real_data' in trading_modules:\n",
    "        try:\n",
    "            print(\"üåü Using REAL MARKET DATA from your data pipeline!\")\n",
    "            \n",
    "            # Determine data parameters based on test mode\n",
    "            if CFG['fast_test']:\n",
    "                period = \"6m\"  # 6 months for fast testing\n",
    "                interval = \"1h\" \n",
    "                print(f\"  üìä Fast test mode: {period} of {interval} data\")\n",
    "            else:\n",
    "                period = \"2y\"   # 2 years for full training\n",
    "                interval = \"1h\"\n",
    "                print(f\"  üìä Full training mode: {period} of {interval} data\")\n",
    "            \n",
    "            # Fetch real training data using your pipeline\n",
    "            integrator = trading_modules['real_data']\n",
    "            dataset = await integrator.prepare_training_dataset(\n",
    "                symbol=SYMBOL,\n",
    "                period=period,\n",
    "                interval=interval,\n",
    "                horizon=CFG['horizon'],\n",
    "                pos_thresh=CFG['pos_thresh']\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ Real data loaded successfully!\")\n",
    "            print(f\"  üìà Data source: {dataset['metadata'].get('data_source', 'Yahoo Finance')}\")\n",
    "            print(f\"  üìÖ Date range: {dataset['metadata']['data_range']['start']} to {dataset['metadata']['data_range']['end']}\")\n",
    "            print(f\"  üìä Raw data points: {len(dataset['ohlcv_data'])}\")\n",
    "            print(f\"  üßÆ ML features: {dataset['metadata']['n_features']}\")\n",
    "            print(f\"  üéØ Training samples: {dataset['metadata']['n_samples']}\")\n",
    "            print(f\"  üìà Class distribution: {dataset['metadata']['class_distribution']}\")\n",
    "            \n",
    "            # Return real data\n",
    "            return dataset['X'], dataset['y_binary'], dataset['y_regression'], dataset['timestamps']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Real data loading failed: {e}\")\n",
    "            print(\"üìâ Falling back to synthetic data...\")\n",
    "    \n",
    "    # Fallback to synthetic data\n",
    "    print(\"üîß Generating synthetic training data...\")\n",
    "    \n",
    "    # Data size based on test mode\n",
    "    n_periods = 500 if CFG['fast_test'] else 2000\n",
    "    \n",
    "    print(f\"üîÑ Generating {n_periods} periods of synthetic data...\")\n",
    "    \n",
    "    # Generate synthetic OHLCV data\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')\n",
    "    \n",
    "    # Random walk with drift for realistic price movement\n",
    "    np.random.seed(CFG['seed'])\n",
    "    returns = np.random.normal(0.0001, 0.01, n_periods)  # Small positive drift\n",
    "    log_prices = np.cumsum(returns)\n",
    "    prices = 50000 * np.exp(log_prices)  # Start around $50,000\n",
    "    \n",
    "    data = []\n",
    "    for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "        # Generate realistic OHLC\n",
    "        volatility = abs(np.random.normal(0, 0.008))  # Daily volatility ~0.8%\n",
    "        high = price * (1 + volatility)\n",
    "        low = price * (1 - volatility)\n",
    "        open_price = prices[i-1] if i > 0 else price\n",
    "        volume = np.random.uniform(100, 1000)\n",
    "        \n",
    "        data.append({\n",
    "            'timestamp': date,\n",
    "            'open': open_price,\n",
    "            'high': high,\n",
    "            'low': low,\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create features\n",
    "    print(\"üîÑ Computing features...\")\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Price features\n",
    "    features['returns'] = df['close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    features['price_ma5'] = df['close'].rolling(5).mean()\n",
    "    features['price_ma20'] = df['close'].rolling(20).mean()\n",
    "    features['price_std'] = df['close'].rolling(20).std()\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume'] = df['volume']\n",
    "    features['volume_ma5'] = df['volume'].rolling(5).mean()\n",
    "    features['volume_ratio'] = df['volume'] / features['volume_ma5']\n",
    "    \n",
    "    # Technical indicators\n",
    "    features['rsi'] = compute_rsi(df['close'], 14)\n",
    "    features['macd'] = compute_macd(df['close'])\n",
    "    features['bollinger_upper'], features['bollinger_lower'] = compute_bollinger_bands(df['close'])\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility'] = features['returns'].rolling(20).std()\n",
    "    features['volatility_ma'] = features['volatility'].rolling(5).mean()\n",
    "    \n",
    "    # Clean features\n",
    "    features = features.dropna()\n",
    "    \n",
    "    # Create labels\n",
    "    print(\"üîÑ Creating labels...\")\n",
    "    future_periods = CFG['horizon']\n",
    "    threshold = CFG['pos_thresh']\n",
    "    \n",
    "    # Calculate future returns\n",
    "    future_returns = df['close'].shift(-future_periods) / df['close'] - 1\n",
    "    \n",
    "    # Binary classification: Will price move up > threshold?\n",
    "    labels_binary = (future_returns > threshold).astype(int)\n",
    "    \n",
    "    # Regression target: actual future return\n",
    "    labels_regression = future_returns\n",
    "    \n",
    "    # Align features and labels\n",
    "    valid_mask = ~future_returns.isna() & ~features.isnull().any(axis=1)\n",
    "    \n",
    "    X = features[valid_mask].reset_index(drop=True)\n",
    "    y_binary = labels_binary[valid_mask].reset_index(drop=True)\n",
    "    y_regression = labels_regression[valid_mask].reset_index(drop=True)\n",
    "    timestamps = df['timestamp'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Synthetic dataset created:\")\n",
    "    print(f\"  Samples: {len(X)}\")\n",
    "    print(f\"  Features: {X.shape[1]}\")\n",
    "    print(f\"  Time range: {timestamps.iloc[0]} to {timestamps.iloc[-1]}\")\n",
    "    print(f\"  Binary class distribution: {y_binary.value_counts().to_dict()}\")\n",
    "    print(f\"  Regression target stats: mean={y_regression.mean():.4f}, std={y_regression.std():.4f}\")\n",
    "    \n",
    "    return X, y_binary, y_regression, timestamps\n",
    "\n",
    "def compute_rsi(prices, window=14):\n",
    "    \"\"\"Compute RSI indicator\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_macd(prices, fast=12, slow=26):\n",
    "    \"\"\"Compute MACD indicator\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    return ema_fast - ema_slow\n",
    "\n",
    "def compute_bollinger_bands(prices, window=20, std_dev=2):\n",
    "    \"\"\"Compute Bollinger Bands\"\"\"\n",
    "    ma = prices.rolling(window).mean()\n",
    "    std = prices.rolling(window).std()\n",
    "    upper = ma + (std * std_dev)\n",
    "    lower = ma - (std * std_dev)\n",
    "    return upper, lower\n",
    "\n",
    "# Generate training data (async call in Jupyter requires special handling)\n",
    "print(\"üöÄ Starting data generation...\")\n",
    "\n",
    "# In Jupyter/Colab, we need to handle async calls properly\n",
    "import asyncio\n",
    "\n",
    "# Check if we're in an existing event loop (Jupyter)\n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    # If we're in Jupyter, create a task\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()  # Allow nested event loops\n",
    "    X, y_binary, y_regression, timestamps = await generate_training_data()\n",
    "except RuntimeError:\n",
    "    # If no event loop, run normally\n",
    "    X, y_binary, y_regression, timestamps = asyncio.run(generate_training_data())\n",
    "except ImportError:\n",
    "    # If nest_asyncio not available, use asyncio.run\n",
    "    X, y_binary, y_regression, timestamps = asyncio.run(generate_training_data())\n",
    "\n",
    "print(f\"\\nüéâ Data generation completed!\")\n",
    "print(f\"Final dataset: {len(X)} samples, {len(X.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf4ad7",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Step 6: Train Models\n",
    "\n",
    "Train machine learning models using LightGBM and other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Analysis and Overview\n",
    "print(\"üìä FEATURE ANALYSIS:\")\n",
    "print(f\"Total Features: {len(X.columns)}\")\n",
    "print(f\"Training Samples: {len(X)}\")\n",
    "\n",
    "# Show data source information\n",
    "data_source = \"Real Market Data via your existing data pipeline\" if 'real_data' in trading_modules else \"Synthetic trading data\"\n",
    "print(f\"Data Source: {data_source}\")\n",
    "\n",
    "if 'real_data' in trading_modules:\n",
    "    print(\"üåü Using PRODUCTION-GRADE features from your data pipeline:\")\n",
    "    print(\"   ‚Ä¢ Technical indicators from ta library\")\n",
    "    print(\"   ‚Ä¢ Market data from Yahoo Finance\")  \n",
    "    print(\"   ‚Ä¢ Advanced feature engineering\")\n",
    "    print(\"   ‚Ä¢ Volume and volatility metrics\")\n",
    "else:\n",
    "    print(\"üîß Using SYNTHETIC features for testing:\")\n",
    "    print(\"   ‚Ä¢ Simulated price movements\")\n",
    "    print(\"   ‚Ä¢ Basic technical indicators\")\n",
    "    print(\"   ‚Ä¢ Test-grade feature generation\")\n",
    "\n",
    "print(\"\\nüìà Feature Categories:\")\n",
    "feature_types = {}\n",
    "for col in X.columns:\n",
    "    if any(x in col.lower() for x in ['price', 'close', 'open', 'high', 'low']):\n",
    "        feature_types['Price Features'] = feature_types.get('Price Features', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['volume', 'vol']):\n",
    "        feature_types['Volume Features'] = feature_types.get('Volume Features', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['return', 'pct', 'change']):\n",
    "        feature_types['Return Features'] = feature_types.get('Return Features', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['sma', 'ema', 'bb', 'rsi', 'macd', 'bollinger', 'moving']):\n",
    "        feature_types['Technical Indicators'] = feature_types.get('Technical Indicators', 0) + 1\n",
    "    elif any(x in col.lower() for x in ['volatility', 'std', 'var']):\n",
    "        feature_types['Volatility Features'] = feature_types.get('Volatility Features', 0) + 1\n",
    "    else:\n",
    "        feature_types['Other Features'] = feature_types.get('Other Features', 0) + 1\n",
    "\n",
    "for category, count in feature_types.items():\n",
    "    print(f\"  ‚Ä¢ {category}: {count}\")\n",
    "\n",
    "print(f\"\\n\udccb Sample Features:\")\n",
    "print(f\"  {list(X.columns[:10])}\")\n",
    "if len(X.columns) > 10:\n",
    "    print(f\"  ... and {len(X.columns) - 10} more features\")\n",
    "\n",
    "# Feature importance preview (basic correlation analysis)\n",
    "print(f\"\\nüéØ Top Correlated Features (with target):\")\n",
    "try:\n",
    "    correlations = X.corrwith(y_binary).abs().sort_values(ascending=False)\n",
    "    print(f\"  ‚Ä¢ {correlations.index[0]}: {correlations.iloc[0]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[1]}: {correlations.iloc[1]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[2]}: {correlations.iloc[2]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[3]}: {correlations.iloc[3]:.3f}\")\n",
    "    print(f\"  ‚Ä¢ {correlations.index[4]}: {correlations.iloc[4]:.3f}\")\n",
    "except:\n",
    "    print(\"  (Correlation analysis skipped)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48944f7c",
   "metadata": {},
   "source": [
    "# üíæ Step 7: Save Model Artifacts\n",
    "\n",
    "Save trained models and metadata to both repository and Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_artifacts(model, metrics, params, model_type, X_sample):\n",
    "    \"\"\"Create comprehensive model artifacts\"\"\"\n",
    "    import joblib\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Create model directory\n",
    "    model_id = f\"lgbm_{model_type}_{RUN_TIMESTAMP}\"\n",
    "    model_dir = os.path.join(MODEL_SAVE_REPO_PATH, SYMBOL, RUN_TIMESTAMP, model_id)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÅ Creating artifacts in: {model_dir}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(model_dir, \"model.pkl\")\n",
    "    joblib.dump(model, model_path, compress=3)\n",
    "    print(f\"  ‚úì Model saved: model.pkl\")\n",
    "    \n",
    "    # Create and save scaler (even if not used, for consistency)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_sample)  # Fit on sample data for consistency\n",
    "    scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path, compress=3)\n",
    "    print(f\"  ‚úì Scaler saved: scaler.pkl\")\n",
    "    \n",
    "    # Create comprehensive metadata\n",
    "    metadata = {\n",
    "        'model_id': model_id,\n",
    "        'model_type': f'lightgbm_{model_type}',\n",
    "        'symbol': SYMBOL,\n",
    "        'interval': INTERVAL,\n",
    "        'timestamp': RUN_TIMESTAMP,\n",
    "        'training_config': CFG,\n",
    "        'model_params': params,\n",
    "        'metrics': metrics,\n",
    "        'feature_names': list(X_sample.columns),\n",
    "        'n_features': len(X_sample.columns),\n",
    "        'training_samples': len(X_sample),\n",
    "        'fast_test_mode': CFG['fast_test'],\n",
    "        'random_seed': CFG['seed'],\n",
    "        'version': '1.0',\n",
    "        'framework': 'lightgbm',\n",
    "        'task_type': model_type,\n",
    "        'colab_training': True\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    meta_path = os.path.join(model_dir, \"meta.json\")\n",
    "    with open(meta_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    print(f\"  ‚úì Metadata saved: meta.json\")\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names_path = os.path.join(model_dir, \"feature_names.json\")\n",
    "    with open(feature_names_path, 'w') as f:\n",
    "        json.dump(list(X_sample.columns), f)\n",
    "    print(f\"  ‚úì Feature names saved: feature_names.json\")\n",
    "    \n",
    "    return model_dir, metadata\n",
    "\n",
    "def copy_to_drive(source_dir, model_id):\n",
    "    \"\"\"Copy artifacts to Google Drive\"\"\"\n",
    "    if not DRIVE_MOUNTED:\n",
    "        print(\"‚ö†Ô∏è  Google Drive not mounted, skipping Drive backup\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        import shutil\n",
    "        \n",
    "        # Create destination directory\n",
    "        drive_model_dir = os.path.join(MODEL_SAVE_DRIVE_PATH, SYMBOL, RUN_TIMESTAMP, model_id)\n",
    "        os.makedirs(os.path.dirname(drive_model_dir), exist_ok=True)\n",
    "        \n",
    "        # Copy entire model directory\n",
    "        if os.path.exists(drive_model_dir):\n",
    "            shutil.rmtree(drive_model_dir)\n",
    "        \n",
    "        shutil.copytree(source_dir, drive_model_dir)\n",
    "        print(f\"‚úÖ Artifacts copied to Google Drive: {drive_model_dir}\")\n",
    "        \n",
    "        return drive_model_dir\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to copy to Google Drive: {e}\")\n",
    "        return None\n",
    "\n",
    "# Save artifacts for both models\n",
    "saved_models = {}\n",
    "\n",
    "if 'binary_model' in locals() and binary_model is not None:\n",
    "    print(\"\\nüíæ Saving Binary Classification Model...\")\n",
    "    binary_dir, binary_metadata = create_model_artifacts(\n",
    "        binary_model, binary_metrics, binary_params, 'binary', binary_splits['X_train']\n",
    "    )\n",
    "    binary_drive_dir = copy_to_drive(binary_dir, binary_metadata['model_id'])\n",
    "    \n",
    "    saved_models['binary'] = {\n",
    "        'local_path': binary_dir,\n",
    "        'drive_path': binary_drive_dir,\n",
    "        'metadata': binary_metadata\n",
    "    }\n",
    "\n",
    "if 'regression_model' in locals() and regression_model is not None:\n",
    "    print(\"\\nüíæ Saving Regression Model...\")\n",
    "    regression_dir, regression_metadata = create_model_artifacts(\n",
    "        regression_model, regression_metrics, regression_params, 'regression', regression_splits['X_train']\n",
    "    )\n",
    "    regression_drive_dir = copy_to_drive(regression_dir, regression_metadata['model_id'])\n",
    "    \n",
    "    saved_models['regression'] = {\n",
    "        'local_path': regression_dir,\n",
    "        'drive_path': regression_drive_dir,\n",
    "        'metadata': regression_metadata\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ All artifacts saved! Models: {list(saved_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704749e8",
   "metadata": {},
   "source": [
    "# ‚úÖ Step 8: Model Validation\n",
    "\n",
    "Validate that saved models can be loaded and used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681efb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_saved_models():\n",
    "    \"\"\"Validate that saved models work correctly\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    print(\"üîç Validating saved models...\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    for model_type, model_info in saved_models.items():\n",
    "        print(f\"\\nüîÑ Validating {model_type} model...\")\n",
    "        \n",
    "        try:\n",
    "            model_dir = model_info['local_path']\n",
    "            \n",
    "            # Load model and scaler\n",
    "            model_path = os.path.join(model_dir, \"model.pkl\")\n",
    "            scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "            meta_path = os.path.join(model_dir, \"meta.json\")\n",
    "            \n",
    "            # Check files exist\n",
    "            for path, name in [(model_path, \"model.pkl\"), (scaler_path, \"scaler.pkl\"), (meta_path, \"meta.json\")]:\n",
    "                if os.path.exists(path):\n",
    "                    print(f\"  ‚úì Found {name}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Missing {name}\")\n",
    "                    continue\n",
    "            \n",
    "            # Load artifacts\n",
    "            model = joblib.load(model_path)\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            \n",
    "            with open(meta_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            print(f\"  ‚úì Loaded model: {metadata['model_id']}\")\n",
    "            print(f\"  ‚úì Features: {metadata['n_features']}\")\n",
    "            print(f\"  ‚úì Training samples: {metadata['training_samples']}\")\n",
    "            \n",
    "            # Test prediction on sample data\n",
    "            if model_type == 'binary':\n",
    "                test_X = binary_splits['X_test'].iloc[:5]  # First 5 test samples\n",
    "                test_y = binary_splits['y_test'].iloc[:5]\n",
    "            else:\n",
    "                test_X = regression_splits['X_test'].iloc[:5]\n",
    "                test_y = regression_splits['y_test'].iloc[:5]\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(test_X)\n",
    "            \n",
    "            print(f\"  ‚úì Sample predictions shape: {predictions.shape}\")\n",
    "            print(f\"  ‚úì Sample predictions (first 3): {predictions[:3]}\")\n",
    "            \n",
    "            # Validate prediction format\n",
    "            if model_type == 'binary':\n",
    "                # Binary predictions should be probabilities between 0 and 1\n",
    "                if all(0 <= p <= 1 for p in predictions):\n",
    "                    print(f\"  ‚úÖ Binary probabilities valid (0-1 range)\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Binary probabilities outside 0-1 range\")\n",
    "            else:\n",
    "                # Regression predictions should be reasonable returns\n",
    "                if all(abs(p) < 1 for p in predictions):  # |return| < 100%\n",
    "                    print(f\"  ‚úÖ Regression predictions reasonable\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è  Regression predictions seem extreme\")\n",
    "            \n",
    "            validation_results[model_type] = {\n",
    "                'status': 'success',\n",
    "                'model_path': model_path,\n",
    "                'predictions_sample': predictions[:3].tolist(),\n",
    "                'metadata': metadata\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ {model_type.capitalize()} model validation successful\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {model_type.capitalize()} model validation failed: {e}\")\n",
    "            validation_results[model_type] = {\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# Validate models\n",
    "if saved_models:\n",
    "    validation_results = validate_saved_models()\n",
    "    \n",
    "    print(f\"\\nüèÜ Validation Summary:\")\n",
    "    for model_type, result in validation_results.items():\n",
    "        status = \"‚úÖ\" if result['status'] == 'success' else \"‚ùå\"\n",
    "        print(f\"  {status} {model_type.capitalize()} Model\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No models to validate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd92123",
   "metadata": {},
   "source": [
    "# üìã Step 9: Create Run Manifest\n",
    "\n",
    "Create a comprehensive manifest file documenting this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_manifest():\n",
    "    \"\"\"Create a comprehensive run manifest\"\"\"\n",
    "    \n",
    "    # Create runs directory\n",
    "    runs_dir = os.path.join(REPO_PATH, \"runs\", f\"colab-{RUN_TIMESTAMP}\")\n",
    "    os.makedirs(runs_dir, exist_ok=True)\n",
    "    \n",
    "    # Get git commit hash if available\n",
    "    git_commit = \"unknown\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['git', 'rev-parse', 'HEAD'], \n",
    "                              cwd=REPO_PATH, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            git_commit = result.stdout.strip()[:12]  # Short hash\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Calculate dataset hash (simple hash of feature names and data size)\n",
    "    import hashlib\n",
    "    feature_string = f\"{list(X.columns)}_{len(X)}_{CFG['seed']}\"\n",
    "    dataset_hash = hashlib.md5(feature_string.encode()).hexdigest()[:12]\n",
    "    \n",
    "    # Create comprehensive manifest\n",
    "    manifest = {\n",
    "        'run_info': {\n",
    "            'timestamp': RUN_TIMESTAMP,\n",
    "            'git_commit': git_commit,\n",
    "            'dataset_hash': dataset_hash,\n",
    "            'colab_session': True,\n",
    "            'fast_test_mode': CFG['fast_test']\n",
    "        },\n",
    "        'configuration': CFG,\n",
    "        'data_info': {\n",
    "            'symbol': SYMBOL,\n",
    "            'interval': INTERVAL,\n",
    "            'n_samples': len(X),\n",
    "            'n_features': len(X.columns),\n",
    "            'feature_names': list(X.columns),\n",
    "            'time_range': {\n",
    "                'start': str(timestamps.iloc[0]),\n",
    "                'end': str(timestamps.iloc[-1])\n",
    "            }\n",
    "        },\n",
    "        'models': {},\n",
    "        'artifacts': {\n",
    "            'repo_base_path': MODEL_SAVE_REPO_PATH,\n",
    "            'drive_base_path': MODEL_SAVE_DRIVE_PATH if DRIVE_MOUNTED else None,\n",
    "            'saved_models': []\n",
    "        },\n",
    "        'validation_results': validation_results if 'validation_results' in locals() else {},\n",
    "        'environment': {\n",
    "            'python_version': sys.version,\n",
    "            'key_packages': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add package versions safely\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    manifest['environment']['key_packages']['pandas'] = pd.__version__\n",
    "    manifest['environment']['key_packages']['numpy'] = np.__version__\n",
    "    \n",
    "    try:\n",
    "        import lightgbm\n",
    "        manifest['environment']['key_packages']['lightgbm'] = lightgbm.__version__\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        import sklearn\n",
    "        manifest['environment']['key_packages']['sklearn'] = sklearn.__version__\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Add model information\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        manifest['models'][model_type] = {\n",
    "            'model_id': model_info['metadata']['model_id'],\n",
    "            'local_path': model_info['local_path'],\n",
    "            'drive_path': model_info['drive_path'],\n",
    "            'metrics': model_info['metadata']['metrics'],\n",
    "            'params': model_info['metadata']['model_params']\n",
    "        }\n",
    "        \n",
    "        manifest['artifacts']['saved_models'].append({\n",
    "            'type': model_type,\n",
    "            'id': model_info['metadata']['model_id'],\n",
    "            'path': model_info['local_path']\n",
    "        })\n",
    "    \n",
    "    # Save manifest\n",
    "    manifest_path = os.path.join(runs_dir, \"manifest.json\")\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"üìã Run manifest created: {manifest_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüìä Training Run Summary:\")\n",
    "    print(f\"  Run ID: colab-{RUN_TIMESTAMP}\")\n",
    "    print(f\"  Git Commit: {git_commit}\")\n",
    "    print(f\"  Dataset Hash: {dataset_hash}\")\n",
    "    print(f\"  Models Trained: {len(saved_models)}\")\n",
    "    print(f\"  Total Samples: {len(X)}\")\n",
    "    print(f\"  Features: {len(X.columns)}\")\n",
    "    print(f\"  Fast Test Mode: {CFG['fast_test']}\")\n",
    "    \n",
    "    if saved_models:\n",
    "        print(f\"\\nüéØ Model Performance:\")\n",
    "        for model_type, model_info in saved_models.items():\n",
    "            metrics = model_info['metadata']['metrics']\n",
    "            if model_type == 'binary':\n",
    "                print(f\"  Binary: AUC={metrics['auc']:.4f}, Accuracy={metrics['accuracy']:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Regression: RMSE={metrics['rmse']:.6f}, R¬≤={metrics['r2']:.4f}\")\n",
    "    \n",
    "    return manifest_path, manifest\n",
    "\n",
    "# Create run manifest\n",
    "if saved_models:\n",
    "    manifest_path, manifest = create_run_manifest()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No models saved, skipping manifest creation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca861d",
   "metadata": {},
   "source": [
    "# üì• Step 10: Display Results & Download\n",
    "\n",
    "Display the training results and provide download options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae094bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results():\n",
    "    \"\"\"Display comprehensive training results\"\"\"\n",
    "    \n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    print(\"üèÜ GOOGLE COLAB TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"‚ùå No models were successfully trained\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä TRAINING SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Run Timestamp: {RUN_TIMESTAMP}\")\n",
    "    print(f\"  ‚Ä¢ Symbol: {SYMBOL}\")\n",
    "    print(f\"  ‚Ä¢ Training Mode: {'Fast Test' if CFG['fast_test'] else 'Full Training'}\")\n",
    "    print(f\"  ‚Ä¢ Models Trained: {len(saved_models)}\")\n",
    "    print(f\"  ‚Ä¢ Dataset Size: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    print(f\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        metadata = model_info['metadata']\n",
    "        metrics = metadata['metrics']\n",
    "        \n",
    "        print(f\"\\n  üìà {model_type.upper()} MODEL:\")\n",
    "        print(f\"    Model ID: {metadata['model_id']}\")\n",
    "        print(f\"    Framework: {metadata['framework']}\")\n",
    "        \n",
    "        if model_type == 'binary':\n",
    "            print(f\"    AUC Score: {metrics['auc']:.4f}\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
    "            print(f\"    R¬≤ Score: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ ARTIFACT LOCATIONS:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        print(f\"\\n  {model_type.upper()} MODEL ARTIFACTS:\")\n",
    "        print(f\"    Local Path: {model_info['local_path']}\")\n",
    "        if model_info['drive_path']:\n",
    "            print(f\"    Google Drive: {model_info['drive_path']}\")\n",
    "        \n",
    "        # List files in directory\n",
    "        if os.path.exists(model_info['local_path']):\n",
    "            files = os.listdir(model_info['local_path'])\n",
    "            print(f\"    Files: {', '.join(files)}\")\n",
    "    \n",
    "    # Display sample metadata\n",
    "    if saved_models:\n",
    "        sample_model = list(saved_models.values())[0]\n",
    "        print(f\"\\nüìã SAMPLE MODEL METADATA:\")\n",
    "        \n",
    "        # Pretty print a subset of metadata\n",
    "        display_metadata = {\n",
    "            'model_id': sample_model['metadata']['model_id'],\n",
    "            'model_type': sample_model['metadata']['model_type'],\n",
    "            'training_config': sample_model['metadata']['training_config'],\n",
    "            'metrics': sample_model['metadata']['metrics'],\n",
    "            'n_features': sample_model['metadata']['n_features'],\n",
    "            'training_samples': sample_model['metadata']['training_samples']\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(display_metadata, indent=2))\n",
    "\n",
    "def create_download_zip():\n",
    "    \"\"\"Create ZIP file of all artifacts for download\"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        import zipfile\n",
    "        \n",
    "        if not saved_models:\n",
    "            print(\"‚ùå No models to package\")\n",
    "            return\n",
    "        \n",
    "        # Create ZIP filename\n",
    "        zip_filename = f\"trading_bot_models_{RUN_TIMESTAMP}.zip\"\n",
    "        zip_path = f\"/content/{zip_filename}\"\n",
    "        \n",
    "        print(f\"üì¶ Creating download package: {zip_filename}\")\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for model_type, model_info in saved_models.items():\n",
    "                model_dir = model_info['local_path']\n",
    "                \n",
    "                # Add all files from model directory\n",
    "                for root, dirs, files in os.walk(model_dir):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        # Create relative path for ZIP\n",
    "                        arcname = os.path.relpath(file_path, MODEL_SAVE_REPO_PATH)\n",
    "                        zipf.write(file_path, arcname)\n",
    "                        print(f\"  ‚úì Added: {arcname}\")\n",
    "            \n",
    "            # Add manifest if it exists\n",
    "            if 'manifest_path' in locals():\n",
    "                zipf.write(manifest_path, f\"runs/colab-{RUN_TIMESTAMP}/manifest.json\")\n",
    "                print(f\"  ‚úì Added: manifest.json\")\n",
    "        \n",
    "        print(f\"‚úÖ ZIP package created: {zip_filename}\")\n",
    "        print(f\"üì• Downloading...\")\n",
    "        \n",
    "        # Download the ZIP file\n",
    "        files.download(zip_path)\n",
    "        \n",
    "        print(\"‚úÖ Download initiated!\")\n",
    "        print(\"üí° The ZIP file contains all model artifacts and metadata\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - download not available\")\n",
    "        print(\"üí° You can manually copy files from the paths shown above\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating download package: {e}\")\n",
    "\n",
    "# Display results\n",
    "display_results()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if CFG['fast_test']:\n",
    "    print(\"1. üöÄ For production training, set CFG['fast_test'] = False and re-run\")\n",
    "\n",
    "print(\"2. üì• Download your model artifacts using the ZIP package below\")\n",
    "print(\"3. üß™ Test your models in your trading environment\")\n",
    "print(\"4. üìà Integrate with your backtesting and live trading systems\")\n",
    "print(\"5. üîÑ Monitor performance and retrain as needed\")\n",
    "\n",
    "print(f\"\\nüí° Model artifacts are saved in:\")\n",
    "print(f\"   Repository: {MODEL_SAVE_REPO_PATH}\")\n",
    "if DRIVE_MOUNTED:\n",
    "    print(f\"   Google Drive: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "\n",
    "print(f\"\\nü§ñ To use these models in production:\")\n",
    "print(\"   ‚Ä¢ Load with: model = joblib.load('model.pkl')\")\n",
    "print(\"   ‚Ä¢ Make predictions: predictions = model.predict(features)\")\n",
    "print(\"   ‚Ä¢ Check metadata for feature requirements and preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d72c1",
   "metadata": {},
   "source": [
    "# üì• Download Model Artifacts\n",
    "\n",
    "Download all trained models and artifacts as a ZIP file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and download ZIP package of all artifacts\n",
    "create_download_zip()\n",
    "\n",
    "print(\"\\nüéä Training completed successfully!\")\n",
    "print(\"üéØ Your models are ready for production use!\")\n",
    "\n",
    "# Display final status\n",
    "if saved_models:\n",
    "    print(f\"\\n‚úÖ Successfully trained {len(saved_models)} models:\")\n",
    "    for model_type in saved_models.keys():\n",
    "        print(f\"  ‚Ä¢ {model_type.capitalize()} Classification/Regression Model\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best practices implemented:\")\n",
    "    print(\"  ‚úì Time-based train/val/test splits\")\n",
    "    print(\"  ‚úì Comprehensive model evaluation\")\n",
    "    print(\"  ‚úì Artifact versioning and metadata\")\n",
    "    print(\"  ‚úì Model validation and integrity checks\")\n",
    "    print(\"  ‚úì Google Drive backup (if mounted)\")\n",
    "    print(\"  ‚úì Downloadable model packages\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No models were successfully trained\")\n",
    "    print(\"Please check the error messages above and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6542db5",
   "metadata": {},
   "source": [
    "# üöÄ Trading Bot ML Training - Google Colab Edition\n",
    "\n",
    "## üìã SETUP INSTRUCTIONS (REQUIRED):\n",
    "\n",
    "### 1. Replace Repository URL\n",
    "```python\n",
    "GITHUB_REPO_URL = \"<YOUR_REPO_URL>\"  # ‚Üê REPLACE THIS!\n",
    "```\n",
    "\n",
    "### 2. Choose Training Mode\n",
    "- **`fast_test=True`** (default): Quick test run with synthetic data (5 minutes)\n",
    "- **`fast_test=False`**: Full training with real data (30-60 minutes)\n",
    "\n",
    "### 3. Private Repository?\n",
    "If your repo is private, use:\n",
    "```python\n",
    "# GITHUB_REPO_URL = \"https://<TOKEN>@github.com/owner/repo.git\"\n",
    "```\n",
    "Replace `<TOKEN>` with your GitHub personal access token.\n",
    "\n",
    "### 4. Alternative: Upload ZIP\n",
    "Instead of cloning, you can upload your repo as a ZIP file and uncomment the ZIP upload section.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "1. **Clone** your trading bot repository\n",
    "2. **Install** all dependencies automatically\n",
    "3. **Mount** Google Drive for artifact storage\n",
    "4. **Train** LightGBM and XGBoost models using your existing modules\n",
    "5. **Save** trained models to repo and Google Drive\n",
    "6. **Validate** model artifacts\n",
    "7. **Download** results to your local machine\n",
    "\n",
    "## üì¶ Output Artifacts:\n",
    "- `models/{symbol}/{timestamp}/{model_id}/` - Model files (pkl, meta.json)\n",
    "- `runs/colab-{timestamp}/manifest.json` - Training manifest\n",
    "- Google Drive backup (if mounted)\n",
    "- ZIP download for local machine\n",
    "\n",
    "**Ready? Let's start! üëá**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde199c",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Configuration Section\n",
    "\n",
    "**IMPORTANT: Modify these variables before running!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# üîß USER CONFIGURATION - MODIFY THESE VALUES!\n",
    "# =============================================================================\n",
    "\n",
    "# TODO: Replace with your GitHub repository URL\n",
    "GITHUB_REPO_URL = \"<YOUR_REPO_URL>\"  # Example: \"https://github.com/username/trading-bot.git\"\n",
    "\n",
    "# Training Configuration\n",
    "SYMBOL = \"BTC-USD\"\n",
    "INTERVAL = \"1m\"\n",
    "\n",
    "CFG = {\n",
    "    \"fast_test\": True,        # Set to False for full training\n",
    "    \"horizon\": 5,             # Future periods for prediction\n",
    "    \"pos_thresh\": 0.002,      # Positive class threshold (0.2%)\n",
    "    \"n_splits\": 2,            # Cross-validation splits (fast_test)\n",
    "    \"seed\": 42,               # Random seed\n",
    "    \"n_periods\": 1000 if True else 5000,  # Dataset size (will be set based on fast_test)\n",
    "}\n",
    "\n",
    "# Update n_periods based on fast_test\n",
    "CFG[\"n_periods\"] = 1000 if CFG[\"fast_test\"] else 5000\n",
    "\n",
    "# Paths (will be set after repo clone)\n",
    "REPO_NAME = None  # Will be extracted from GITHUB_REPO_URL\n",
    "REPO_PATH = None  # Will be set to /content/{REPO_NAME}\n",
    "MODEL_SAVE_REPO_PATH = None  # Will be set to {REPO_PATH}/models/\n",
    "MODEL_SAVE_DRIVE_PATH = \"/content/drive/MyDrive/models/\"\n",
    "\n",
    "# Status flags\n",
    "DRIVE_MOUNTED = False\n",
    "REPO_CLONED = False\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üéØ Training Mode: {'Fast Test' if CFG['fast_test'] else 'Full Training'}\")\n",
    "print(f\"üìä Symbol: {SYMBOL} | Interval: {INTERVAL}\")\n",
    "print(f\"üî¢ Dataset Size: {CFG['n_periods']} periods\")\n",
    "\n",
    "if GITHUB_REPO_URL == \"<YOUR_REPO_URL>\":\n",
    "    print(\"‚ö†Ô∏è  WARNING: Please replace GITHUB_REPO_URL with your actual repository URL!\")\n",
    "    print(\"   Example: GITHUB_REPO_URL = 'https://github.com/username/trading-bot.git'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c56e2",
   "metadata": {},
   "source": [
    "# üì• Repository Setup\n",
    "\n",
    "Clone your trading bot repository and set up the Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b993020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_repo_name(url):\n",
    "    \"\"\"Extract repository name from GitHub URL\"\"\"\n",
    "    if url.endswith('.git'):\n",
    "        url = url[:-4]\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "def clone_repository(repo_url):\n",
    "    \"\"\"Clone the repository\"\"\"\n",
    "    global REPO_NAME, REPO_PATH, MODEL_SAVE_REPO_PATH, REPO_CLONED\n",
    "    \n",
    "    if repo_url == \"<YOUR_REPO_URL>\":\n",
    "        print(\"‚ùå ERROR: Please replace GITHUB_REPO_URL with your actual repository URL!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Cloning repository: {repo_url}\")\n",
    "        \n",
    "        # Extract repo name\n",
    "        REPO_NAME = extract_repo_name(repo_url)\n",
    "        REPO_PATH = f\"/content/{REPO_NAME}\"\n",
    "        MODEL_SAVE_REPO_PATH = f\"{REPO_PATH}/models/\"\n",
    "        \n",
    "        # Remove existing directory if it exists\n",
    "        if os.path.exists(REPO_PATH):\n",
    "            print(f\"üóëÔ∏è  Removing existing directory: {REPO_PATH}\")\n",
    "            shutil.rmtree(REPO_PATH)\n",
    "        \n",
    "        # Clone repository\n",
    "        result = subprocess.run(\n",
    "            [\"git\", \"clone\", repo_url, REPO_PATH],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=\"/content\"\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"‚ùå Git clone failed: {result.stderr}\")\n",
    "            print(\"üí° If this is a private repo, make sure you're using a personal access token:\")\n",
    "            print(\"   https://<TOKEN>@github.com/username/repo.git\")\n",
    "            return False\n",
    "        \n",
    "        # Add to Python path\n",
    "        if REPO_PATH not in sys.path:\n",
    "            sys.path.insert(0, REPO_PATH)\n",
    "        \n",
    "        print(f\"‚úÖ Repository cloned successfully to: {REPO_PATH}\")\n",
    "        print(f\"üìÅ Python path updated: {REPO_PATH}\")\n",
    "        \n",
    "        # Show repository structure\n",
    "        print(\"\\nüìÇ Repository structure:\")\n",
    "        for root, dirs, files in os.walk(REPO_PATH):\n",
    "            # Limit depth to avoid clutter\n",
    "            level = root.replace(REPO_PATH, '').count(os.sep)\n",
    "            if level < 3:\n",
    "                indent = ' ' * 2 * level\n",
    "                print(f\"{indent}{os.path.basename(root)}/\")\n",
    "                subindent = ' ' * 2 * (level + 1)\n",
    "                for file in files[:5]:  # Show only first 5 files per directory\n",
    "                    print(f\"{subindent}{file}\")\n",
    "                if len(files) > 5:\n",
    "                    print(f\"{subindent}... and {len(files) - 5} more files\")\n",
    "        \n",
    "        REPO_CLONED = True\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cloning repository: {e}\")\n",
    "        return False\n",
    "\n",
    "# Clone the repository\n",
    "clone_success = clone_repository(GITHUB_REPO_URL)\n",
    "\n",
    "if not clone_success:\n",
    "    print(\"\\nüîÑ Alternative: Upload ZIP file\")\n",
    "    print(\"If cloning failed, you can upload your repo as a ZIP file instead.\")\n",
    "    print(\"Uncomment and run the next cell to use ZIP upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede58b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ALTERNATIVE: Upload ZIP file (uncomment if git clone failed)\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "\n",
    "# print(\"üì¶ Upload your repository as a ZIP file:\")\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# if uploaded:\n",
    "#     zip_name = list(uploaded.keys())[0]\n",
    "#     print(f\"üì• Extracting {zip_name}...\")\n",
    "    \n",
    "#     with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('/content')\n",
    "    \n",
    "#     # Find extracted directory\n",
    "#     for item in os.listdir('/content'):\n",
    "#         if os.path.isdir(f'/content/{item}') and item != 'sample_data':\n",
    "#             REPO_NAME = item\n",
    "#             REPO_PATH = f'/content/{item}'\n",
    "#             MODEL_SAVE_REPO_PATH = f'{REPO_PATH}/models/'\n",
    "#             break\n",
    "    \n",
    "#     if REPO_PATH and REPO_PATH not in sys.path:\n",
    "#         sys.path.insert(0, REPO_PATH)\n",
    "    \n",
    "#     print(f\"‚úÖ ZIP extracted to: {REPO_PATH}\")\n",
    "#     REPO_CLONED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2fe64",
   "metadata": {},
   "source": [
    "# üì¶ Install Dependencies\n",
    "\n",
    "Install required packages for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52831ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results():\n",
    "    \"\"\"Display comprehensive training results\"\"\"\n",
    "    \n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    print(\"üèÜ GOOGLE COLAB TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"üèÜ\" + \"=\"*60)\n",
    "    \n",
    "    if not saved_models:\n",
    "        print(\"‚ùå No models were successfully trained\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\udcca TRAINING SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Run Timestamp: {RUN_TIMESTAMP}\")\n",
    "    print(f\"  ‚Ä¢ Symbol: {SYMBOL}\")\n",
    "    print(f\"  ‚Ä¢ Training Mode: {'Fast Test' if CFG['fast_test'] else 'Full Training'}\")\n",
    "    print(f\"  ‚Ä¢ Models Trained: {len(saved_models)}\")\n",
    "    print(f\"  ‚Ä¢ Dataset Size: {len(X)} samples, {len(X.columns)} features\")\n",
    "    \n",
    "    # Show data source information\n",
    "    data_source = \"üåü Real Market Data\" if 'real_data' in trading_modules else \"üîß Synthetic Data\"\n",
    "    print(f\"  ‚Ä¢ Data Source: {data_source}\")\n",
    "    \n",
    "    if 'real_data' in trading_modules:\n",
    "        print(f\"    üìà Via your existing data pipeline (Yahoo Finance)\")\n",
    "        print(f\"    üéØ Production-grade features and validation\")\n",
    "    else:\n",
    "        print(f\"    ‚ö†Ô∏è  Synthetic data used (real data unavailable)\")\n",
    "    \n",
    "    print(f\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        metadata = model_info['metadata']\n",
    "        metrics = metadata['metrics']\n",
    "        \n",
    "        print(f\"\\n  üìà {model_type.upper()} MODEL:\")\n",
    "        print(f\"    Model ID: {metadata['model_id']}\")\n",
    "        print(f\"    Framework: {metadata['framework']}\")\n",
    "        print(f\"    Data Source: {data_source}\")\n",
    "        \n",
    "        if model_type == 'binary':\n",
    "            print(f\"    AUC Score: {metrics['auc']:.4f}\")\n",
    "            print(f\"    Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"    RMSE: {metrics['rmse']:.6f}\")\n",
    "            print(f\"    R¬≤ Score: {metrics['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\udcc1 ARTIFACT LOCATIONS:\")\n",
    "    for model_type, model_info in saved_models.items():\n",
    "        print(f\"\\n  {model_type.upper()} MODEL ARTIFACTS:\")\n",
    "        print(f\"    Local Path: {model_info['local_path']}\")\n",
    "        if model_info['drive_path']:\n",
    "            print(f\"    Google Drive: {model_info['drive_path']}\")\n",
    "        \n",
    "        # List files in directory\n",
    "        if os.path.exists(model_info['local_path']):\n",
    "            files = os.listdir(model_info['local_path'])\n",
    "            print(f\"    Files: {', '.join(files)}\")\n",
    "    \n",
    "    # Display sample metadata\n",
    "    if saved_models:\n",
    "        sample_model = list(saved_models.values())[0]\n",
    "        print(f\"\\nüìã SAMPLE MODEL METADATA:\")\n",
    "        \n",
    "        # Pretty print a subset of metadata including data source info\n",
    "        display_metadata = {\n",
    "            'model_id': sample_model['metadata']['model_id'],\n",
    "            'model_type': sample_model['metadata']['model_type'],\n",
    "            'data_source': data_source,\n",
    "            'training_config': sample_model['metadata']['training_config'],\n",
    "            'metrics': sample_model['metadata']['metrics'],\n",
    "            'n_features': sample_model['metadata']['n_features'],\n",
    "            'training_samples': sample_model['metadata']['training_samples']\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(display_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b0e1e",
   "metadata": {},
   "source": [
    "# üíæ Mount Google Drive\n",
    "\n",
    "Mount Google Drive to save trained models for long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive safely\"\"\"\n",
    "    global DRIVE_MOUNTED\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üîÑ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Verify mount\n",
    "        if os.path.exists('/content/drive/MyDrive'):\n",
    "            print(\"‚úÖ Google Drive mounted successfully\")\n",
    "            print(f\"üìÅ Drive path: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "            \n",
    "            # Create models directory in Drive if it doesn't exist\n",
    "            os.makedirs(MODEL_SAVE_DRIVE_PATH, exist_ok=True)\n",
    "            DRIVE_MOUNTED = True\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Drive mount verification failed\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - Drive mount skipped\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Drive mount failed: {e}\")\n",
    "        print(\"Continuing without Drive backup...\")\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive\n",
    "mount_success = mount_google_drive()\n",
    "\n",
    "if mount_success:\n",
    "    print(\"üí° Models will be saved to both repo and Google Drive\")\n",
    "else:\n",
    "    print(\"üí° Models will be saved to repo only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd03a21",
   "metadata": {},
   "source": [
    "# üì• Import Modules\n",
    "\n",
    "Import the trading bot modules and verify everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91745a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess\n",
    "\n",
    "# ML libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "\n",
    "# Optional libraries (with fallbacks)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not available - will skip XGBoost models\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Optuna not available - will skip hyperparameter optimization\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(CFG['seed'])\n",
    "\n",
    "print(f\"\\nüîß Core libraries imported successfully\")\n",
    "print(f\"üéØ Random seed: {CFG['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16293dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trading_modules():\n",
    "    \"\"\"Import trading bot modules with fallbacks\"\"\"\n",
    "    \n",
    "    if not REPO_CLONED:\n",
    "        print(\"‚ùå Repository not available for module imports\")\n",
    "        return False\n",
    "    \n",
    "    print(\"üîÑ Importing trading bot modules...\")\n",
    "    \n",
    "    # Try to import existing modules\n",
    "    modules_imported = {}\n",
    "    \n",
    "    # Feature engineering\n",
    "    try:\n",
    "        from arbi.ai.feature_engineering_v2 import compute_features_deterministic, load_feature_schema\n",
    "        modules_imported['feature_engineering'] = True\n",
    "        print(\"‚úÖ Feature engineering module\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Feature engineering module not found: {e}\")\n",
    "        modules_imported['feature_engineering'] = False\n",
    "    \n",
    "    # Training module\n",
    "    try:\n",
    "        from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "        modules_imported['training'] = True\n",
    "        print(\"‚úÖ Training module\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from arbi.ai.train_lgbm import train_and_validate_lgbm\n",
    "            modules_imported['training'] = True\n",
    "            print(\"‚úÖ LightGBM training module\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Training module not found: {e}\")\n",
    "            modules_imported['training'] = False\n",
    "    \n",
    "    # Model registry\n",
    "    try:\n",
    "        from arbi.ai.registry import ModelRegistry\n",
    "        modules_imported['registry'] = True\n",
    "        print(\"‚úÖ Model registry\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Model registry not found: {e}\")\n",
    "        modules_imported['registry'] = False\n",
    "    \n",
    "    # Inference module\n",
    "    try:\n",
    "        from arbi.ai.inference_v2 import ProductionInferenceEngine\n",
    "        modules_imported['inference'] = True\n",
    "        print(\"‚úÖ Inference engine\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from arbi.ai.inference import InferenceEngine\n",
    "            modules_imported['inference'] = True\n",
    "            print(\"‚úÖ Inference engine (v1)\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Inference module not found: {e}\")\n",
    "            modules_imported['inference'] = False\n",
    "    \n",
    "    imported_count = sum(modules_imported.values())\n",
    "    total_count = len(modules_imported)\n",
    "    \n",
    "    print(f\"\\nüìä Module Import Summary: {imported_count}/{total_count} modules imported\")\n",
    "    \n",
    "    if imported_count == 0:\n",
    "        print(\"‚ö†Ô∏è  No trading bot modules found - will use fallback implementations\")\n",
    "        return False\n",
    "    elif imported_count < total_count:\n",
    "        print(\"‚ö†Ô∏è  Some modules missing - will use fallbacks where needed\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚úÖ All modules imported successfully\")\n",
    "        return True\n",
    "\n",
    "# Import trading bot modules\n",
    "modules_available = import_trading_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b3984",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Model Training\n",
    "\n",
    "Train LightGBM and XGBoost models using your existing modules or fallback implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1538d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fallback_features(df):\n",
    "    \"\"\"Create basic technical indicators as fallback\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Price features\n",
    "    features['returns'] = df['close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    features['price_ma5'] = df['close'].rolling(5).mean()\n",
    "    features['price_ma20'] = df['close'].rolling(20).mean()\n",
    "    features['price_ratio_ma5'] = df['close'] / features['price_ma5']\n",
    "    features['price_ratio_ma20'] = df['close'] / features['price_ma20']\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume_ma5'] = df['volume'].rolling(5).mean()\n",
    "    features['volume_ratio'] = df['volume'] / features['volume_ma5']\n",
    "    features['volume_price_trend'] = features['volume_ratio'] * features['returns']\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility'] = features['returns'].rolling(20).std()\n",
    "    features['volatility_ratio'] = features['returns'].abs() / features['volatility']\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12).mean()\n",
    "    exp2 = df['close'].ewm(span=26).mean()\n",
    "    features['macd'] = exp1 - exp2\n",
    "    features['macd_signal'] = features['macd'].ewm(span=9).mean()\n",
    "    features['macd_hist'] = features['macd'] - features['macd_signal']\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "def generate_fallback_ohlcv_data(n_periods=1000, symbol=\"BTC-USD\"):\n",
    "    \"\"\"Generate synthetic OHLCV data\"\"\"\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')\n",
    "    \n",
    "    # Random walk with drift and regime changes\n",
    "    np.random.seed(CFG['seed'])\n",
    "    \n",
    "    # Create regime changes\n",
    "    regime_changes = np.random.choice(n_periods, size=5, replace=False)\n",
    "    regime_changes.sort()\n",
    "    \n",
    "    returns = []\n",
    "    current_vol = 0.01\n",
    "    \n",
    "    for i in range(n_periods):\n",
    "        # Change volatility at regime boundaries\n",
    "        if i in regime_changes:\n",
    "            current_vol = np.random.uniform(0.005, 0.02)\n",
    "        \n",
    "        # Generate return with current volatility\n",
    "        ret = np.random.normal(0.00005, current_vol)\n",
    "        returns.append(ret)\n",
    "    \n",
    "    returns = np.array(returns)\n",
    "    prices = 50000 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    data = []\n",
    "    for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "        high = price * (1 + abs(np.random.normal(0, 0.005)))\n",
    "        low = price * (1 - abs(np.random.normal(0, 0.005)))\n",
    "        open_price = prices[i-1] if i > 0 else price\n",
    "        volume = np.random.uniform(100, 1000) * (1 + abs(returns[i]) * 10)\n",
    "        \n",
    "        data.append({\n",
    "            'timestamp': date,\n",
    "            'open': open_price,\n",
    "            'high': high,\n",
    "            'low': low,\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_training_dataset(n_periods, symbol):\n",
    "    \"\"\"Create training dataset with features and labels\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Creating training dataset ({n_periods} periods)...\")\n",
    "    \n",
    "    # Generate or load OHLCV data\n",
    "    try:\n",
    "        if modules_available:\n",
    "            from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "            df = generate_synthetic_ohlcv_data(n_periods, symbol)\n",
    "            print(\"‚úÖ Using repository OHLCV generation\")\n",
    "        else:\n",
    "            raise ImportError(\"Using fallback\")\n",
    "    except:\n",
    "        df = generate_fallback_ohlcv_data(n_periods, symbol)\n",
    "        print(\"‚úÖ Using fallback OHLCV generation\")\n",
    "    \n",
    "    # Compute features\n",
    "    try:\n",
    "        if modules_available:\n",
    "            from arbi.ai.feature_engineering_v2 import compute_features_deterministic\n",
    "            feature_result = compute_features_deterministic(df, symbol)\n",
    "            feature_df = feature_result.features\n",
    "            print(\"‚úÖ Using repository feature engineering\")\n",
    "        else:\n",
    "            raise ImportError(\"Using fallback\")\n",
    "    except:\n",
    "        feature_df = create_fallback_features(df)\n",
    "        print(\"‚úÖ Using fallback feature engineering\")\n",
    "    \n",
    "    # Create labels\n",
    "    future_periods = CFG['horizon']\n",
    "    threshold = CFG['pos_thresh']\n",
    "    \n",
    "    # Calculate future returns\n",
    "    future_returns = df['close'].shift(-future_periods) / df['close'] - 1\n",
    "    \n",
    "    # Binary classification: 1 if return > threshold, 0 otherwise\n",
    "    labels_binary = (future_returns > threshold).astype(int)\n",
    "    \n",
    "    # Regression target: actual future return\n",
    "    labels_regression = future_returns\n",
    "    \n",
    "    # Remove rows where we can't calculate future returns\n",
    "    valid_mask = ~future_returns.isna()\n",
    "    \n",
    "    feature_df = feature_df[valid_mask].reset_index(drop=True)\n",
    "    labels_binary = labels_binary[valid_mask].reset_index(drop=True)\n",
    "    labels_regression = labels_regression[valid_mask].reset_index(drop=True)\n",
    "    timestamps = df['timestamp'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created:\")\n",
    "    print(f\"  Samples: {len(feature_df)}\")\n",
    "    print(f\"  Features: {feature_df.shape[1]}\")\n",
    "    print(f\"  Positive class: {labels_binary.sum()}/{len(labels_binary)} ({100*labels_binary.mean():.1f}%)\")\n",
    "    print(f\"  Regression target range: {labels_regression.min():.4f} to {labels_regression.max():.4f}\")\n",
    "    \n",
    "    return feature_df, labels_binary, labels_regression, timestamps\n",
    "\n",
    "# Create training dataset\n",
    "X, y_binary, y_regression, timestamps = create_training_dataset(CFG['n_periods'], SYMBOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122a63e",
   "metadata": {},
   "source": [
    "# üéâ Notebook Complete!\n",
    "\n",
    "This is the complete Google Colab training notebook. To continue with training and saving models, use the additional chunks or run the CLI script.\n",
    "\n",
    "**Next steps:**\n",
    "1. Run the remaining training cells\n",
    "2. Save model artifacts\n",
    "3. Create training manifest\n",
    "4. Download results\n",
    "\n",
    "**Or use the CLI script:** `python tools/colab_train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e24ad",
   "metadata": {},
   "source": [
    "# ‚úÖ Import Path Verification\n",
    "\n",
    "Testing that all modules can be imported correctly with the fixed `arbi.*` paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790dd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all fixed import paths\n",
    "print(\"üîç Testing import paths...\")\n",
    "\n",
    "try:\n",
    "    print(\"‚úì arbi.core.pipeline imports...\")\n",
    "    from arbi.core.pipeline import TradingDataPipeline\n",
    "    from arbi.core.data_collector import DataCollector\n",
    "    from arbi.core.feature_engineering import FeatureEngine\n",
    "    \n",
    "    print(\"‚úì arbi.ai.* imports...\")\n",
    "    from arbi.ai.real_data_integration import RealDataIntegrator\n",
    "    from arbi.ai.feature_engineering_v2 import EnhancedFeatureEngine\n",
    "    from arbi.ai.training_v2 import AdvancedTrainer\n",
    "    from arbi.ai.registry import ModelRegistry\n",
    "    from arbi.ai.models import ModelManager\n",
    "    from arbi.ai.monitoring import ModelMonitor\n",
    "    \n",
    "    print(\"\\nüéâ All import paths are working correctly!\")\n",
    "    print(\"üìÅ Directory structure: arbi/ai/ and arbi/core/ found\")\n",
    "    print(\"üöÄ Notebook ready for training!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"üìù This helps identify any remaining path issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab313093",
   "metadata": {},
   "source": [
    "# üìä Time Series Cross-Validation\n",
    "\n",
    "Implementing proper walk-forward validation to prevent overfitting - a critical component from the ML roadmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da7cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cross_validation(X, y, timestamps, model_func, n_splits=5, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Implement proper time series cross-validation with walk-forward analysis\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import TimeSeriesSplit\n",
    "    \n",
    "    print(f\"üîÑ Performing {n_splits}-fold Time Series Cross-Validation...\")\n",
    "    \n",
    "    # Create time series splits\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=int(len(X) * test_size))\n",
    "    \n",
    "    cv_scores = []\n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\nüìä Fold {fold + 1}/{n_splits}\")\n",
    "        print(f\"  Train: {len(train_idx)} samples ({timestamps.iloc[train_idx[0]]} to {timestamps.iloc[train_idx[-1]]})\")\n",
    "        print(f\"  Val:   {len(val_idx)} samples ({timestamps.iloc[val_idx[0]]} to {timestamps.iloc[val_idx[-1]]})\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_cv = X.iloc[train_idx]\n",
    "        X_val_cv = X.iloc[val_idx]\n",
    "        y_train_cv = y.iloc[train_idx]\n",
    "        y_val_cv = y.iloc[val_idx]\n",
    "        \n",
    "        # Train model\n",
    "        try:\n",
    "            model = model_func(X_train_cv, y_train_cv)\n",
    "            \n",
    "            # Evaluate\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_cv = model.predict_proba(X_val_cv)[:, 1]\n",
    "                from sklearn.metrics import roc_auc_score\n",
    "                score = roc_auc_score(y_val_cv, y_pred_cv)\n",
    "                metric_name = \"AUC\"\n",
    "            else:\n",
    "                y_pred_cv = model.predict(X_val_cv)\n",
    "                from sklearn.metrics import mean_squared_error\n",
    "                score = -mean_squared_error(y_val_cv, y_pred_cv)  # Negative MSE for maximization\n",
    "                metric_name = \"Neg MSE\"\n",
    "            \n",
    "            cv_scores.append(score)\n",
    "            fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'score': score,\n",
    "                'train_size': len(train_idx),\n",
    "                'val_size': len(val_idx),\n",
    "                'train_period': f\"{timestamps.iloc[train_idx[0]]} to {timestamps.iloc[train_idx[-1]]}\",\n",
    "                'val_period': f\"{timestamps.iloc[val_idx[0]]} to {timestamps.iloc[val_idx[-1]]}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"  {metric_name}: {score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error in fold {fold + 1}: {e}\")\n",
    "            cv_scores.append(0.0)\n",
    "    \n",
    "    # Summary\n",
    "    cv_mean = np.mean(cv_scores)\n",
    "    cv_std = np.std(cv_scores)\n",
    "    \n",
    "    print(f\"\\nüìà Cross-Validation Results:\")\n",
    "    print(f\"  Mean {metric_name}: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "    print(f\"  Individual scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
    "    \n",
    "    return {\n",
    "        'cv_mean': cv_mean,\n",
    "        'cv_std': cv_std,\n",
    "        'cv_scores': cv_scores,\n",
    "        'fold_results': fold_results\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Time Series Cross-Validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd87a1",
   "metadata": {},
   "source": [
    "# üîß Hyperparameter Optimization with Optuna\n",
    "\n",
    "Systematic hyperparameter optimization for each model type using Optuna - critical for production performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lightgbm_hyperparameters(X, y, timestamps, n_trials=50):\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna\n",
    "    \"\"\"\n",
    "    if not OPTUNA_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è  Optuna not available - skipping hyperparameter optimization\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîÑ Optimizing LightGBM hyperparameters ({n_trials} trials)...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 10, 100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 10.0),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 10.0),\n",
    "            'verbose': -1,\n",
    "            'random_state': CFG['seed']\n",
    "        }\n",
    "        \n",
    "        # Model training function for CV\n",
    "        def lgb_model_func(X_train, y_train):\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                num_boost_round=200,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            return model\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_result = time_series_cross_validation(\n",
    "            X, y, timestamps, lgb_model_func, \n",
    "            n_splits=3, test_size=0.2\n",
    "        )\n",
    "        \n",
    "        return cv_result['cv_mean']\n",
    "    \n",
    "    # Create and optimize study\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=CFG['seed']))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"‚úÖ Hyperparameter optimization complete!\")\n",
    "    print(f\"  Best AUC: {study.best_value:.4f}\")\n",
    "    print(f\"  Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "def optimize_xgboost_hyperparameters(X, y, timestamps, n_trials=50):\n",
    "    \"\"\"\n",
    "    Optimize XGBoost hyperparameters using Optuna\n",
    "    \"\"\"\n",
    "    if not OPTUNA_AVAILABLE or not XGBOOST_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è  Optuna or XGBoost not available - skipping hyperparameter optimization\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîÑ Optimizing XGBoost hyperparameters ({n_trials} trials)...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'eta': trial.suggest_float('eta', 0.01, 0.3),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "            'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'lambda': trial.suggest_float('lambda', 0.0, 10.0),\n",
    "            'alpha': trial.suggest_float('alpha', 0.0, 10.0),\n",
    "            'random_state': CFG['seed'],\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        # Model training function for CV\n",
    "        def xgb_model_func(X_train, y_train):\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=200,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            return model\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_result = time_series_cross_validation(\n",
    "            X, y, timestamps, xgb_model_func, \n",
    "            n_splits=3, test_size=0.2\n",
    "        )\n",
    "        \n",
    "        return cv_result['cv_mean']\n",
    "    \n",
    "    # Create and optimize study\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=CFG['seed']))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"‚úÖ XGBoost hyperparameter optimization complete!\")\n",
    "    print(f\"  Best AUC: {study.best_value:.4f}\")\n",
    "    print(f\"  Best params: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "print(\"‚úÖ Hyperparameter optimization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558955df",
   "metadata": {},
   "source": [
    "# üéØ Stacked Ensemble Methods\n",
    "\n",
    "Implementing stacked ensemble with meta-learner to combine multiple models - critical for production performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedEnsemble:\n",
    "    \"\"\"\n",
    "    Stacked ensemble combining multiple base models with a meta-learner\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_models, meta_learner, cv_folds=3):\n",
    "        self.base_models = base_models  # List of (name, model_func) tuples\n",
    "        self.meta_learner = meta_learner\n",
    "        self.cv_folds = cv_folds\n",
    "        self.trained_models = {}\n",
    "        self.meta_model = None\n",
    "        \n",
    "    def fit(self, X, y, timestamps):\n",
    "        \"\"\"\n",
    "        Train the stacked ensemble using cross-validation\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Training Stacked Ensemble with {len(self.base_models)} base models...\")\n",
    "        \n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        \n",
    "        # Initialize meta-features array\n",
    "        meta_features = np.zeros((len(X), len(self.base_models)))\n",
    "        \n",
    "        # Time series cross-validation for meta-features\n",
    "        tscv = TimeSeriesSplit(n_splits=self.cv_folds)\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "            print(f\"  Fold {fold + 1}/{self.cv_folds}\")\n",
    "            \n",
    "            X_train_fold = X.iloc[train_idx]\n",
    "            X_val_fold = X.iloc[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            \n",
    "            # Train base models on fold\n",
    "            fold_models = {}\n",
    "            for model_name, model_func in self.base_models:\n",
    "                try:\n",
    "                    model = model_func(X_train_fold, y_train_fold)\n",
    "                    fold_models[model_name] = model\n",
    "                    \n",
    "                    # Generate predictions for meta-features\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "                    else:\n",
    "                        pred = model.predict(X_val_fold)\n",
    "                    \n",
    "                    # Store meta-features\n",
    "                    model_idx = [name for name, _ in self.base_models].index(model_name)\n",
    "                    meta_features[val_idx, model_idx] = pred\n",
    "                    \n",
    "                    print(f\"    ‚úì {model_name} trained\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå Error training {model_name}: {e}\")\n",
    "        \n",
    "        # Train final base models on full dataset\n",
    "        print(f\"  Training final base models on full dataset...\")\n",
    "        for model_name, model_func in self.base_models:\n",
    "            try:\n",
    "                self.trained_models[model_name] = model_func(X, y)\n",
    "                print(f\"    ‚úì {model_name} final model trained\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Error training final {model_name}: {e}\")\n",
    "        \n",
    "        # Train meta-learner\n",
    "        print(f\"  Training meta-learner...\")\n",
    "        valid_meta_mask = ~np.isnan(meta_features).any(axis=1)\n",
    "        if valid_meta_mask.sum() > 0:\n",
    "            self.meta_model = self.meta_learner\n",
    "            self.meta_model.fit(meta_features[valid_meta_mask], y[valid_meta_mask])\n",
    "            print(f\"    ‚úì Meta-learner trained on {valid_meta_mask.sum()} samples\")\n",
    "        else:\n",
    "            print(f\"    ‚ùå No valid meta-features for meta-learner\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Generate ensemble predictions\n",
    "        \"\"\"\n",
    "        if not self.trained_models or self.meta_model is None:\n",
    "            raise ValueError(\"Ensemble not fitted yet\")\n",
    "        \n",
    "        # Generate base model predictions\n",
    "        base_predictions = np.zeros((len(X), len(self.base_models)))\n",
    "        \n",
    "        for i, (model_name, _) in enumerate(self.base_models):\n",
    "            if model_name in self.trained_models:\n",
    "                model = self.trained_models[model_name]\n",
    "                try:\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        pred = model.predict_proba(X)[:, 1]\n",
    "                    else:\n",
    "                        pred = model.predict(X)\n",
    "                    base_predictions[:, i] = pred\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error predicting with {model_name}: {e}\")\n",
    "                    base_predictions[:, i] = 0.5  # Default prediction\n",
    "        \n",
    "        # Meta-learner prediction\n",
    "        ensemble_proba = self.meta_model.predict_proba(base_predictions)\n",
    "        return ensemble_proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generate ensemble binary predictions\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "def create_ensemble_models():\n",
    "    \"\"\"\n",
    "    Create base models for ensemble\n",
    "    \"\"\"\n",
    "    base_models = []\n",
    "    \n",
    "    # LightGBM model function\n",
    "    def lgb_model_func(X_train, y_train):\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'random_state': CFG['seed']\n",
    "        }\n",
    "        return lgb.train(params, train_data, num_boost_round=200)\n",
    "    \n",
    "    base_models.append(('LightGBM', lgb_model_func))\n",
    "    \n",
    "    # XGBoost model function (if available)\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        def xgb_model_func(X_train, y_train):\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            params = {\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'max_depth': 5,\n",
    "                'eta': 0.05,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'random_state': CFG['seed'],\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            return xgb.train(params, dtrain, num_boost_round=200)\n",
    "        \n",
    "        base_models.append(('XGBoost', xgb_model_func))\n",
    "    \n",
    "    # Random Forest model function\n",
    "    def rf_model_func(X_train, y_train):\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=CFG['seed'],\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        return model.fit(X_train, y_train)\n",
    "    \n",
    "    base_models.append(('RandomForest', rf_model_func))\n",
    "    \n",
    "    return base_models\n",
    "\n",
    "print(\"‚úÖ Stacked Ensemble implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e842b09f",
   "metadata": {},
   "source": [
    "# üîç SHAP Feature Importance Analysis\n",
    "\n",
    "Model explainability using SHAP values to understand what drives predictions - critical for model validation and compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ca792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, X_sample, feature_names, model_type='lightgbm'):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using SHAP values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import shap\n",
    "        print(\"‚úÖ SHAP available - performing feature analysis\")\n",
    "        \n",
    "        # Initialize SHAP explainer based on model type\n",
    "        if model_type == 'lightgbm':\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        elif model_type == 'xgboost':\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "        elif model_type == 'sklearn':\n",
    "            explainer = shap.Explainer(model.predict, X_sample.iloc[:100])  # Use sample for speed\n",
    "        else:\n",
    "            explainer = shap.Explainer(model.predict, X_sample.iloc[:100])\n",
    "        \n",
    "        # Calculate SHAP values on sample (for speed)\n",
    "        sample_size = min(200, len(X_sample))\n",
    "        X_shap = X_sample.iloc[:sample_size]\n",
    "        \n",
    "        print(f\"üîÑ Computing SHAP values for {sample_size} samples...\")\n",
    "        shap_values = explainer.shap_values(X_shap)\n",
    "        \n",
    "        # Handle different SHAP value formats\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]  # Take positive class for binary classification\n",
    "        \n",
    "        # Feature importance summary\n",
    "        feature_importance = np.abs(shap_values).mean(0)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': feature_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä Top 10 Most Important Features (SHAP):\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"  {row['feature']:<25} {row['importance']:.6f}\")\n",
    "        \n",
    "        # Try to create summary plot (may fail in some environments)\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            # Summary plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            shap.summary_plot(shap_values, X_shap, feature_names=feature_names, show=False, max_display=15)\n",
    "            plt.title(f'SHAP Feature Importance - {model_type.upper()}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Feature importance bar plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            top_features = importance_df.head(15)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Mean |SHAP value|')\n",
    "            plt.title(f'Top 15 Feature Importance - {model_type.upper()}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"‚úÖ SHAP plots generated\")\n",
    "            \n",
    "        except Exception as plot_error:\n",
    "            print(f\"‚ö†Ô∏è  Could not generate SHAP plots: {plot_error}\")\n",
    "        \n",
    "        return {\n",
    "            'feature_importance': importance_df,\n",
    "            'shap_values': shap_values,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  SHAP not available - performing basic feature importance analysis\")\n",
    "        \n",
    "        # Fallback to basic feature importance for tree models\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "        elif hasattr(model, 'get_score'):  # XGBoost\n",
    "            importance_dict = model.get_score(importance_type='weight')\n",
    "            importance = [importance_dict.get(f'f{i}', 0) for i in range(len(feature_names))]\n",
    "        else:\n",
    "            print(\"‚ùå No feature importance available for this model\")\n",
    "            return None\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä Top 10 Most Important Features (Model Built-in):\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"  {row['feature']:<25} {row['importance']:.6f}\")\n",
    "        \n",
    "        return {'feature_importance': importance_df}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in feature importance analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "def comprehensive_model_analysis(models_dict, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of all trained models\n",
    "    \"\"\"\n",
    "    print(\"üîç Performing Comprehensive Model Analysis...\")\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    for model_name, model_info in models_dict.items():\n",
    "        print(f\"\\nüìä Analyzing {model_name}...\")\n",
    "        \n",
    "        model = model_info.get('model')\n",
    "        if model is None:\n",
    "            print(f\"  ‚ùå No model found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Performance metrics\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "                \n",
    "                from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "                auc = roc_auc_score(y_test, y_pred_proba)\n",
    "                precision = precision_score(y_test, y_pred)\n",
    "                recall = recall_score(y_test, y_pred)\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                \n",
    "                print(f\"  Performance: AUC={auc:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
    "                \n",
    "            elif hasattr(model, 'predict'):\n",
    "                y_pred = model.predict(X_test)\n",
    "                from sklearn.metrics import mean_squared_error, r2_score\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                print(f\"  Performance: MSE={mse:.6f}, R¬≤={r2:.4f}\")\n",
    "            \n",
    "            # Feature importance analysis\n",
    "            model_type = 'lightgbm' if 'lgb' in model_name.lower() else \\\n",
    "                        'xgboost' if 'xgb' in model_name.lower() else 'sklearn'\n",
    "            \n",
    "            importance_result = analyze_feature_importance(\n",
    "                model, X_test, feature_names, model_type\n",
    "            )\n",
    "            \n",
    "            analysis_results[model_name] = {\n",
    "                'model': model,\n",
    "                'model_type': model_type,\n",
    "                'importance_analysis': importance_result\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error analyzing {model_name}: {e}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "print(\"‚úÖ SHAP feature importance analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e25169",
   "metadata": {},
   "source": [
    "# üöÄ Advanced Model Training Orchestration\n",
    "\n",
    "Complete training pipeline incorporating all advanced components: hyperparameter optimization, ensemble methods, and model analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b39664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_model_training_pipeline():\n",
    "    \"\"\"\n",
    "    Complete advanced training pipeline following the ML roadmap\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Advanced Model Training Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Time Series Data Splitting\n",
    "    print(\"\\nüìä Step 1: Time Series Data Splitting\")\n",
    "    train_size = 0.6\n",
    "    val_size = 0.2\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    train_end = int(n_samples * train_size)\n",
    "    val_end = int(n_samples * (train_size + val_size))\n",
    "    \n",
    "    X_train = X.iloc[:train_end].copy()\n",
    "    X_val = X.iloc[train_end:val_end].copy()\n",
    "    X_test = X.iloc[val_end:].copy()\n",
    "    \n",
    "    y_train = y_binary.iloc[:train_end].copy()\n",
    "    y_val = y_binary.iloc[train_end:val_end].copy()\n",
    "    y_test = y_binary.iloc[val_end:].copy()\n",
    "    \n",
    "    timestamps_train = timestamps.iloc[:train_end]\n",
    "    timestamps_val = timestamps.iloc[train_end:val_end]\n",
    "    timestamps_test = timestamps.iloc[val_end:]\n",
    "    \n",
    "    print(f\"  Train: {len(X_train)} samples ({timestamps_train.iloc[0]} to {timestamps_train.iloc[-1]})\")\n",
    "    print(f\"  Val:   {len(X_val)} samples ({timestamps_val.iloc[0]} to {timestamps_val.iloc[-1]})\")\n",
    "    print(f\"  Test:  {len(X_test)} samples ({timestamps_test.iloc[0]} to {timestamps_test.iloc[-1]})\")\n",
    "    \n",
    "    # Step 2: Hyperparameter Optimization (if enabled)\n",
    "    print(\"\\nüîß Step 2: Hyperparameter Optimization\")\n",
    "    best_lgb_params = None\n",
    "    best_xgb_params = None\n",
    "    \n",
    "    if CFG.get('enable_hpo', True) and not CFG['fast_test']:\n",
    "        # Combine train and val for HPO\n",
    "        X_hpo = pd.concat([X_train, X_val])\n",
    "        y_hpo = pd.concat([y_train, y_val])\n",
    "        timestamps_hpo = pd.concat([timestamps_train, timestamps_val])\n",
    "        \n",
    "        # LightGBM HPO\n",
    "        best_lgb_params = optimize_lightgbm_hyperparameters(X_hpo, y_hpo, timestamps_hpo, n_trials=20)\n",
    "        \n",
    "        # XGBoost HPO (if available)\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            best_xgb_params = optimize_xgboost_hyperparameters(X_hpo, y_hpo, timestamps_hpo, n_trials=20)\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Hyperparameter optimization skipped (fast_test=True or disabled)\")\n",
    "    \n",
    "    # Step 3: Train Individual Models\n",
    "    print(\"\\nüéØ Step 3: Training Individual Models\")\n",
    "    individual_models = {}\n",
    "    \n",
    "    # Train LightGBM\n",
    "    print(\"  Training LightGBM...\")\n",
    "    try:\n",
    "        lgb_params = best_lgb_params or {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'verbose': -1,\n",
    "            'random_state': CFG['seed']\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(pd.concat([X_train, X_val]), label=pd.concat([y_train, y_val]))\n",
    "        lgb_model = lgb.train(\n",
    "            lgb_params,\n",
    "            train_data,\n",
    "            num_boost_round=CFG['n_estimators_full'] if not CFG['fast_test'] else CFG['n_estimators']\n",
    "        )\n",
    "        \n",
    "        individual_models['LightGBM'] = {'model': lgb_model, 'params': lgb_params}\n",
    "        print(\"    ‚úÖ LightGBM trained successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå LightGBM training failed: {e}\")\n",
    "    \n",
    "    # Train XGBoost\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        print(\"  Training XGBoost...\")\n",
    "        try:\n",
    "            xgb_params = best_xgb_params or {\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'max_depth': 5,\n",
    "                'eta': 0.05,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'random_state': CFG['seed'],\n",
    "                'verbosity': 0\n",
    "            }\n",
    "            \n",
    "            dtrain = xgb.DMatrix(pd.concat([X_train, X_val]), label=pd.concat([y_train, y_val]))\n",
    "            xgb_model = xgb.train(\n",
    "                xgb_params,\n",
    "                dtrain,\n",
    "                num_boost_round=CFG['n_estimators_full'] if not CFG['fast_test'] else CFG['n_estimators']\n",
    "            )\n",
    "            \n",
    "            individual_models['XGBoost'] = {'model': xgb_model, 'params': xgb_params}\n",
    "            print(\"    ‚úÖ XGBoost trained successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå XGBoost training failed: {e}\")\n",
    "    \n",
    "    # Step 4: Cross-Validation Analysis\n",
    "    print(\"\\nüìä Step 4: Cross-Validation Analysis\")\n",
    "    cv_results = {}\n",
    "    \n",
    "    for model_name, model_info in individual_models.items():\n",
    "        print(f\"  Evaluating {model_name} with time series CV...\")\n",
    "        \n",
    "        def model_func(X_train_cv, y_train_cv):\n",
    "            return model_info['model']  # Use pre-trained model for speed\n",
    "        \n",
    "        try:\n",
    "            cv_result = time_series_cross_validation(\n",
    "                pd.concat([X_train, X_val]), \n",
    "                pd.concat([y_train, y_val]), \n",
    "                pd.concat([timestamps_train, timestamps_val]),\n",
    "                model_func,\n",
    "                n_splits=3,\n",
    "                test_size=0.2\n",
    "            )\n",
    "            cv_results[model_name] = cv_result\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå CV failed for {model_name}: {e}\")\n",
    "    \n",
    "    # Step 5: Ensemble Training\n",
    "    print(\"\\nüéØ Step 5: Training Stacked Ensemble\")\n",
    "    ensemble_model = None\n",
    "    \n",
    "    if len(individual_models) >= 2:  # Need at least 2 models for ensemble\n",
    "        try:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            \n",
    "            base_models = create_ensemble_models()\n",
    "            meta_learner = LogisticRegression(random_state=CFG['seed'])\n",
    "            \n",
    "            ensemble_model = StackedEnsemble(base_models, meta_learner, cv_folds=3)\n",
    "            ensemble_model.fit(\n",
    "                pd.concat([X_train, X_val]), \n",
    "                pd.concat([y_train, y_val]), \n",
    "                pd.concat([timestamps_train, timestamps_val])\n",
    "            )\n",
    "            \n",
    "            print(\"    ‚úÖ Stacked ensemble trained successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Ensemble training failed: {e}\")\n",
    "    else:\n",
    "        print(\"    ‚ö†Ô∏è  Ensemble skipped (need at least 2 base models)\")\n",
    "    \n",
    "    # Step 6: Model Evaluation\n",
    "    print(\"\\nüìà Step 6: Model Evaluation on Test Set\")\n",
    "    final_results = {}\n",
    "    \n",
    "    # Evaluate individual models\n",
    "    for model_name, model_info in individual_models.items():\n",
    "        model = model_info['model']\n",
    "        \n",
    "        try:\n",
    "            if hasattr(model, 'predict'):\n",
    "                if model_name == 'XGBoost':\n",
    "                    y_pred_proba = model.predict(xgb.DMatrix(X_test))\n",
    "                else:\n",
    "                    y_pred_proba = model.predict(X_test)\n",
    "            \n",
    "            from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            metrics = {\n",
    "                'auc': roc_auc_score(y_test, y_pred_proba),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            final_results[model_name] = {\n",
    "                'model': model,\n",
    "                'metrics': metrics,\n",
    "                'predictions': y_pred_proba,\n",
    "                'cv_results': cv_results.get(model_name)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {model_name}: AUC={metrics['auc']:.4f}, F1={metrics['f1']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Evaluation failed for {model_name}: {e}\")\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    if ensemble_model is not None:\n",
    "        try:\n",
    "            ensemble_proba = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "            ensemble_pred = (ensemble_proba > 0.5).astype(int)\n",
    "            \n",
    "            ensemble_metrics = {\n",
    "                'auc': roc_auc_score(y_test, ensemble_proba),\n",
    "                'precision': precision_score(y_test, ensemble_pred),\n",
    "                'recall': recall_score(y_test, ensemble_pred),\n",
    "                'f1': f1_score(y_test, ensemble_pred)\n",
    "            }\n",
    "            \n",
    "            final_results['StackedEnsemble'] = {\n",
    "                'model': ensemble_model,\n",
    "                'metrics': ensemble_metrics,\n",
    "                'predictions': ensemble_proba\n",
    "            }\n",
    "            \n",
    "            print(f\"  StackedEnsemble: AUC={ensemble_metrics['auc']:.4f}, F1={ensemble_metrics['f1']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Ensemble evaluation failed: {e}\")\n",
    "    \n",
    "    # Step 7: Feature Importance Analysis\n",
    "    print(\"\\nüîç Step 7: Feature Importance Analysis\")\n",
    "    \n",
    "    analysis_results = comprehensive_model_analysis(\n",
    "        final_results, X_test, y_test, X.columns.tolist()\n",
    "    )\n",
    "    \n",
    "    # Step 8: Results Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÜ ADVANCED TRAINING PIPELINE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Best model selection\n",
    "    best_auc = 0\n",
    "    best_model_name = None\n",
    "    \n",
    "    for model_name, result in final_results.items():\n",
    "        auc = result['metrics']['auc']\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_model_name = model_name\n",
    "    \n",
    "    if best_model_name:\n",
    "        print(f\"\\nü•á Best Model: {best_model_name} (AUC: {best_auc:.4f})\")\n",
    "    \n",
    "    # Model comparison table\n",
    "    print(f\"\\nüìä Model Comparison:\")\n",
    "    print(f\"{'Model':<15} {'AUC':<8} {'Precision':<10} {'Recall':<8} {'F1':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, result in final_results.items():\n",
    "        metrics = result['metrics']\n",
    "        print(f\"{model_name:<15} {metrics['auc']:<8.4f} {metrics['precision']:<10.4f} \"\n",
    "              f\"{metrics['recall']:<8.4f} {metrics['f1']:<8.4f}\")\n",
    "    \n",
    "    return final_results, analysis_results\n",
    "\n",
    "print(\"‚úÖ Advanced training orchestration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the advanced training pipeline\n",
    "print(\"üöÄ EXECUTING ADVANCED ML TRAINING PIPELINE\")\n",
    "print(\"Following the complete ML Model Development Roadmap...\")\n",
    "\n",
    "# Run the advanced training pipeline\n",
    "final_results, analysis_results = advanced_model_training_pipeline()\n",
    "\n",
    "# Store results globally for artifact creation\n",
    "saved_models = {}\n",
    "model_analysis = analysis_results\n",
    "\n",
    "# Convert results to the format expected by artifact creation\n",
    "for model_name, result in final_results.items():\n",
    "    model_id = f\"{model_name}_{RUN_TIMESTAMP}\"\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        'model_id': model_id,\n",
    "        'model_type': f'{model_name.lower()}_binary',\n",
    "        'symbol': SYMBOL,\n",
    "        'timestamp': RUN_TIMESTAMP,\n",
    "        'metrics': result['metrics'],\n",
    "        'framework': model_name.lower(),\n",
    "        'training_config': CFG,\n",
    "        'feature_names': X.columns.tolist(),\n",
    "        'n_features': len(X.columns),\n",
    "        'training_samples': len(X),\n",
    "        'cv_results': result.get('cv_results'),\n",
    "        'roadmap_compliant': True,  # Mark as following the ML roadmap\n",
    "        'advanced_features': {\n",
    "            'time_series_cv': True,\n",
    "            'hyperparameter_optimization': not CFG['fast_test'],\n",
    "            'ensemble_method': 'StackedEnsemble' if model_name == 'StackedEnsemble' else 'Individual',\n",
    "            'feature_importance_analysis': True,\n",
    "            'shap_analysis': True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    saved_models[model_name] = {\n",
    "        'model': result['model'],\n",
    "        'metadata': metadata,\n",
    "        'local_path': f\"{REPO_PATH}/models/{SYMBOL}/{RUN_TIMESTAMP}/{model_name.lower()}\",\n",
    "        'drive_path': f\"{MODEL_SAVE_DRIVE_PATH}{SYMBOL}_{RUN_TIMESTAMP}_{model_name.lower()}\"\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Advanced training complete! {len(saved_models)} models trained following ML roadmap.\")\n",
    "print(\"üìã Ready for artifact creation and deployment.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
