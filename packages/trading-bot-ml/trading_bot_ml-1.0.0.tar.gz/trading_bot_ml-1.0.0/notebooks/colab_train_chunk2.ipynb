{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d883a9",
   "metadata": {},
   "source": [
    "# üíæ Mount Google Drive\n",
    "\n",
    "Mount Google Drive to save trained models for long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5971270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_google_drive():\n",
    "    \"\"\"Mount Google Drive safely\"\"\"\n",
    "    global DRIVE_MOUNTED\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"üîÑ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Verify mount\n",
    "        if os.path.exists('/content/drive/MyDrive'):\n",
    "            print(\"‚úÖ Google Drive mounted successfully\")\n",
    "            print(f\"üìÅ Drive path: {MODEL_SAVE_DRIVE_PATH}\")\n",
    "            \n",
    "            # Create models directory in Drive if it doesn't exist\n",
    "            os.makedirs(MODEL_SAVE_DRIVE_PATH, exist_ok=True)\n",
    "            DRIVE_MOUNTED = True\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Drive mount verification failed\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  Not running in Google Colab - Drive mount skipped\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Drive mount failed: {e}\")\n",
    "        print(\"Continuing without Drive backup...\")\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive\n",
    "mount_success = mount_google_drive()\n",
    "\n",
    "if mount_success:\n",
    "    print(\"üí° Models will be saved to both repo and Google Drive\")\n",
    "else:\n",
    "    print(\"üí° Models will be saved to repo only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb24bc",
   "metadata": {},
   "source": [
    "# üì• Import Modules\n",
    "\n",
    "Import the trading bot modules and verify everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e39e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess\n",
    "\n",
    "# ML libraries\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "\n",
    "# Optional libraries (with fallbacks)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "    print(\"‚úÖ XGBoost available\")\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not available - will skip XGBoost models\")\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"‚úÖ Optuna available\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Optuna not available - will skip hyperparameter optimization\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(CFG['seed'])\n",
    "\n",
    "print(f\"\\nüîß Core libraries imported successfully\")\n",
    "print(f\"üéØ Random seed: {CFG['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175da536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trading_modules():\n",
    "    \"\"\"Import trading bot modules with fallbacks\"\"\"\n",
    "    \n",
    "    if not REPO_CLONED:\n",
    "        print(\"‚ùå Repository not available for module imports\")\n",
    "        return False\n",
    "    \n",
    "    print(\"üîÑ Importing trading bot modules...\")\n",
    "    \n",
    "    # Try to import existing modules\n",
    "    modules_imported = {}\n",
    "    \n",
    "    # Feature engineering\n",
    "    try:\n",
    "        from arbi.ai.feature_engineering_v2 import compute_features_deterministic, load_feature_schema\n",
    "        modules_imported['feature_engineering'] = True\n",
    "        print(\"‚úÖ Feature engineering module\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Feature engineering module not found: {e}\")\n",
    "        modules_imported['feature_engineering'] = False\n",
    "    \n",
    "    # Training module\n",
    "    try:\n",
    "        from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "        modules_imported['training'] = True\n",
    "        print(\"‚úÖ Training module\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from arbi.ai.train_lgbm import train_and_validate_lgbm\n",
    "            modules_imported['training'] = True\n",
    "            print(\"‚úÖ LightGBM training module\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Training module not found: {e}\")\n",
    "            modules_imported['training'] = False\n",
    "    \n",
    "    # Model registry\n",
    "    try:\n",
    "        from arbi.ai.registry import ModelRegistry\n",
    "        modules_imported['registry'] = True\n",
    "        print(\"‚úÖ Model registry\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Model registry not found: {e}\")\n",
    "        modules_imported['registry'] = False\n",
    "    \n",
    "    # Inference module\n",
    "    try:\n",
    "        from arbi.ai.inference_v2 import ProductionInferenceEngine\n",
    "        modules_imported['inference'] = True\n",
    "        print(\"‚úÖ Inference engine\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from arbi.ai.inference import InferenceEngine\n",
    "            modules_imported['inference'] = True\n",
    "            print(\"‚úÖ Inference engine (v1)\")\n",
    "        except ImportError as e:\n",
    "            print(f\"‚ö†Ô∏è  Inference module not found: {e}\")\n",
    "            modules_imported['inference'] = False\n",
    "    \n",
    "    imported_count = sum(modules_imported.values())\n",
    "    total_count = len(modules_imported)\n",
    "    \n",
    "    print(f\"\\nüìä Module Import Summary: {imported_count}/{total_count} modules imported\")\n",
    "    \n",
    "    if imported_count == 0:\n",
    "        print(\"‚ö†Ô∏è  No trading bot modules found - will use fallback implementations\")\n",
    "        return False\n",
    "    elif imported_count < total_count:\n",
    "        print(\"‚ö†Ô∏è  Some modules missing - will use fallbacks where needed\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚úÖ All modules imported successfully\")\n",
    "        return True\n",
    "\n",
    "# Import trading bot modules\n",
    "modules_available = import_trading_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a3942",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Model Training\n",
    "\n",
    "Train LightGBM and XGBoost models using your existing modules or fallback implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f54bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fallback_features(df):\n",
    "    \"\"\"Create basic technical indicators as fallback\"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Price features\n",
    "    features['returns'] = df['close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    features['price_ma5'] = df['close'].rolling(5).mean()\n",
    "    features['price_ma20'] = df['close'].rolling(20).mean()\n",
    "    features['price_ratio_ma5'] = df['close'] / features['price_ma5']\n",
    "    features['price_ratio_ma20'] = df['close'] / features['price_ma20']\n",
    "    \n",
    "    # Volume features\n",
    "    features['volume_ma5'] = df['volume'].rolling(5).mean()\n",
    "    features['volume_ratio'] = df['volume'] / features['volume_ma5']\n",
    "    features['volume_price_trend'] = features['volume_ratio'] * features['returns']\n",
    "    \n",
    "    # Volatility\n",
    "    features['volatility'] = features['returns'].rolling(20).std()\n",
    "    features['volatility_ratio'] = features['returns'].abs() / features['volatility']\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12).mean()\n",
    "    exp2 = df['close'].ewm(span=26).mean()\n",
    "    features['macd'] = exp1 - exp2\n",
    "    features['macd_signal'] = features['macd'].ewm(span=9).mean()\n",
    "    features['macd_hist'] = features['macd'] - features['macd_signal']\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "def generate_fallback_ohlcv_data(n_periods=1000, symbol=\"BTC-USD\"):\n",
    "    \"\"\"Generate synthetic OHLCV data\"\"\"\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_periods, freq='1H')\n",
    "    \n",
    "    # Random walk with drift and regime changes\n",
    "    np.random.seed(CFG['seed'])\n",
    "    \n",
    "    # Create regime changes\n",
    "    regime_changes = np.random.choice(n_periods, size=5, replace=False)\n",
    "    regime_changes.sort()\n",
    "    \n",
    "    returns = []\n",
    "    current_vol = 0.01\n",
    "    \n",
    "    for i in range(n_periods):\n",
    "        # Change volatility at regime boundaries\n",
    "        if i in regime_changes:\n",
    "            current_vol = np.random.uniform(0.005, 0.02)\n",
    "        \n",
    "        # Generate return with current volatility\n",
    "        ret = np.random.normal(0.00005, current_vol)\n",
    "        returns.append(ret)\n",
    "    \n",
    "    returns = np.array(returns)\n",
    "    prices = 50000 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    data = []\n",
    "    for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "        high = price * (1 + abs(np.random.normal(0, 0.005)))\n",
    "        low = price * (1 - abs(np.random.normal(0, 0.005)))\n",
    "        open_price = prices[i-1] if i > 0 else price\n",
    "        volume = np.random.uniform(100, 1000) * (1 + abs(returns[i]) * 10)\n",
    "        \n",
    "        data.append({\n",
    "            'timestamp': date,\n",
    "            'open': open_price,\n",
    "            'high': high,\n",
    "            'low': low,\n",
    "            'close': price,\n",
    "            'volume': volume\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def create_training_dataset(n_periods, symbol):\n",
    "    \"\"\"Create training dataset with features and labels\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Creating training dataset ({n_periods} periods)...\")\n",
    "    \n",
    "    # Generate or load OHLCV data\n",
    "    try:\n",
    "        if modules_available:\n",
    "            from arbi.ai.training_v2 import generate_synthetic_ohlcv_data\n",
    "            df = generate_synthetic_ohlcv_data(n_periods, symbol)\n",
    "            print(\"‚úÖ Using repository OHLCV generation\")\n",
    "        else:\n",
    "            raise ImportError(\"Using fallback\")\n",
    "    except:\n",
    "        df = generate_fallback_ohlcv_data(n_periods, symbol)\n",
    "        print(\"‚úÖ Using fallback OHLCV generation\")\n",
    "    \n",
    "    # Compute features\n",
    "    try:\n",
    "        if modules_available:\n",
    "            from arbi.ai.feature_engineering_v2 import compute_features_deterministic\n",
    "            feature_result = compute_features_deterministic(df, symbol)\n",
    "            feature_df = feature_result.features\n",
    "            print(\"‚úÖ Using repository feature engineering\")\n",
    "        else:\n",
    "            raise ImportError(\"Using fallback\")\n",
    "    except:\n",
    "        feature_df = create_fallback_features(df)\n",
    "        print(\"‚úÖ Using fallback feature engineering\")\n",
    "    \n",
    "    # Create labels\n",
    "    future_periods = CFG['horizon']\n",
    "    threshold = CFG['pos_thresh']\n",
    "    \n",
    "    # Calculate future returns\n",
    "    future_returns = df['close'].shift(-future_periods) / df['close'] - 1\n",
    "    \n",
    "    # Binary classification: 1 if return > threshold, 0 otherwise\n",
    "    labels_binary = (future_returns > threshold).astype(int)\n",
    "    \n",
    "    # Regression target: actual future return\n",
    "    labels_regression = future_returns\n",
    "    \n",
    "    # Remove rows where we can't calculate future returns\n",
    "    valid_mask = ~future_returns.isna()\n",
    "    \n",
    "    feature_df = feature_df[valid_mask].reset_index(drop=True)\n",
    "    labels_binary = labels_binary[valid_mask].reset_index(drop=True)\n",
    "    labels_regression = labels_regression[valid_mask].reset_index(drop=True)\n",
    "    timestamps = df['timestamp'][valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created:\")\n",
    "    print(f\"  Samples: {len(feature_df)}\")\n",
    "    print(f\"  Features: {feature_df.shape[1]}\")\n",
    "    print(f\"  Positive class: {labels_binary.sum()}/{len(labels_binary)} ({100*labels_binary.mean():.1f}%)\")\n",
    "    print(f\"  Regression target range: {labels_regression.min():.4f} to {labels_regression.max():.4f}\")\n",
    "    \n",
    "    return feature_df, labels_binary, labels_regression, timestamps\n",
    "\n",
    "# Create training dataset\n",
    "X, y_binary, y_regression, timestamps = create_training_dataset(CFG['n_periods'], SYMBOL)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
