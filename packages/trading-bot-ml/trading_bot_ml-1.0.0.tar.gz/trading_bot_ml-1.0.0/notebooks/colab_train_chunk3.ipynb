{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_splits(X, y, timestamps, train_size=0.6, val_size=0.2):\n",
    "    \"\"\"Create time-based train/val/test splits\"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    train_end = int(n_samples * train_size)\n",
    "    val_end = int(n_samples * (train_size + val_size))\n",
    "    \n",
    "    splits = {\n",
    "        'X_train': X.iloc[:train_end].copy(),\n",
    "        'y_train': y.iloc[:train_end].copy(),\n",
    "        'X_val': X.iloc[train_end:val_end].copy(),\n",
    "        'y_val': y.iloc[train_end:val_end].copy(),\n",
    "        'X_test': X.iloc[val_end:].copy(),\n",
    "        'y_test': y.iloc[val_end:].copy()\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Time-based splits:\")\n",
    "    print(f\"  Train: {len(splits['X_train'])} samples\")\n",
    "    print(f\"  Val:   {len(splits['X_val'])} samples\")\n",
    "    print(f\"  Test:  {len(splits['X_test'])} samples\")\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def train_lightgbm_model(X_train, y_train, X_val, y_val, task_type='binary'):\n",
    "    \"\"\"Train LightGBM model\"\"\"\n",
    "    \n",
    "    if task_type == 'binary':\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31 if CFG['fast_test'] else 63,\n",
    "            'learning_rate': 0.1 if CFG['fast_test'] else 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'min_data_in_leaf': 10 if CFG['fast_test'] else 20,\n",
    "            'verbose': -1,\n",
    "            'random_state': CFG['seed']\n",
    "        }\n",
    "        metric_name = 'auc'\n",
    "    else:\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31 if CFG['fast_test'] else 63,\n",
    "            'learning_rate': 0.1 if CFG['fast_test'] else 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'min_data_in_leaf': 10 if CFG['fast_test'] else 20,\n",
    "            'verbose': -1,\n",
    "            'random_state': CFG['seed']\n",
    "        }\n",
    "        metric_name = 'rmse'\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Train model\n",
    "    n_rounds = 100 if CFG['fast_test'] else 1000\n",
    "    early_stop = 20 if CFG['fast_test'] else 50\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=n_rounds,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(early_stop, verbose=False),\n",
    "            lgb.log_evaluation(50 if CFG['fast_test'] else 100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model, params\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, task_type='binary', model_name='Model'):\n",
    "    \"\"\"Evaluate trained model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if 'xgb' in str(type(model)).lower():\n",
    "            # XGBoost model\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            y_pred = model.predict(dtest)\n",
    "        else:\n",
    "            # LightGBM or other sklearn-compatible model\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if task_type == 'binary':\n",
    "            # Ensure probabilities are in [0,1]\n",
    "            if y_pred.max() > 1:\n",
    "                y_pred = 1 / (1 + np.exp(-y_pred))  # sigmoid\n",
    "            \n",
    "            auc = roc_auc_score(y_test, y_pred)\n",
    "            print(f\"📊 {model_name} - AUC: {auc:.4f}\")\n",
    "            return {'auc': auc, 'predictions': y_pred}\n",
    "        else:\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f\"📊 {model_name} - RMSE: {rmse:.6f}, R²: {r2:.4f}\")\n",
    "            return {'rmse': rmse, 'r2': r2, 'predictions': y_pred}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error evaluating {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create data splits\n",
    "print(\"🔄 Creating data splits...\")\n",
    "binary_splits = create_time_splits(X, y_binary, timestamps)\n",
    "regression_splits = create_time_splits(X, y_regression, timestamps)\n",
    "\n",
    "# Train LightGBM models\n",
    "print(\"\\n🚀 Training LightGBM Binary Classifier...\")\n",
    "lgbm_binary, lgbm_binary_params = train_lightgbm_model(\n",
    "    binary_splits['X_train'], binary_splits['y_train'],\n",
    "    binary_splits['X_val'], binary_splits['y_val'],\n",
    "    task_type='binary'\n",
    ")\n",
    "\n",
    "print(\"\\n🚀 Training LightGBM Regressor...\")\n",
    "lgbm_regressor, lgbm_reg_params = train_lightgbm_model(\n",
    "    regression_splits['X_train'], regression_splits['y_train'],\n",
    "    regression_splits['X_val'], regression_splits['y_val'],\n",
    "    task_type='regression'\n",
    ")\n",
    "\n",
    "# Train XGBoost models (if available)\n",
    "xgb_binary = None\n",
    "xgb_regressor = None\n",
    "xgb_binary_params = None\n",
    "xgb_reg_params = None\n",
    "\n",
    "if XGB_AVAILABLE and not CFG['fast_test']:\n",
    "    print(\"\\n🚀 Training XGBoost models...\")\n",
    "    \n",
    "    try:\n",
    "        # XGBoost Binary\n",
    "        xgb_binary_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'max_depth': 5,\n",
    "            'eta': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 10,\n",
    "            'random_state': CFG['seed'],\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        dtrain = xgb.DMatrix(binary_splits['X_train'], label=binary_splits['y_train'])\n",
    "        dval = xgb.DMatrix(binary_splits['X_val'], label=binary_splits['y_val'])\n",
    "        \n",
    "        xgb_binary = xgb.train(\n",
    "            xgb_binary_params,\n",
    "            dtrain,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "            early_stopping_rounds=30,\n",
    "            verbose_eval=50\n",
    "        )\n",
    "        print(\"✅ XGBoost Binary trained\")\n",
    "        \n",
    "        # XGBoost Regressor\n",
    "        xgb_reg_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'max_depth': 5,\n",
    "            'eta': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'min_child_weight': 10,\n",
    "            'random_state': CFG['seed'],\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        dtrain_reg = xgb.DMatrix(regression_splits['X_train'], label=regression_splits['y_train'])\n",
    "        dval_reg = xgb.DMatrix(regression_splits['X_val'], label=regression_splits['y_val'])\n",
    "        \n",
    "        xgb_regressor = xgb.train(\n",
    "            xgb_reg_params,\n",
    "            dtrain_reg,\n",
    "            num_boost_round=200,\n",
    "            evals=[(dtrain_reg, 'train'), (dval_reg, 'val')],\n",
    "            early_stopping_rounds=30,\n",
    "            verbose_eval=50\n",
    "        )\n",
    "        print(\"✅ XGBoost Regressor trained\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  XGBoost training failed: {e}\")\n",
    "        XGB_AVAILABLE = False\n",
    "\n",
    "print(\"\\n🎯 Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331b7f5",
   "metadata": {},
   "source": [
    "# 📊 Model Evaluation\n",
    "\n",
    "Evaluate all trained models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Evaluating models on test set...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Evaluate binary classification models\n",
    "print(\"\\n🔍 Binary Classification Results:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "lgbm_binary_results = evaluate_model(\n",
    "    lgbm_binary, binary_splits['X_test'], binary_splits['y_test'], \n",
    "    'binary', 'LightGBM Binary'\n",
    ")\n",
    "\n",
    "xgb_binary_results = None\n",
    "if xgb_binary is not None:\n",
    "    xgb_binary_results = evaluate_model(\n",
    "        xgb_binary, binary_splits['X_test'], binary_splits['y_test'], \n",
    "        'binary', 'XGBoost Binary'\n",
    "    )\n",
    "\n",
    "# Evaluate regression models\n",
    "print(\"\\n🔍 Regression Results:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "lgbm_reg_results = evaluate_model(\n",
    "    lgbm_regressor, regression_splits['X_test'], regression_splits['y_test'], \n",
    "    'regression', 'LightGBM Regression'\n",
    ")\n",
    "\n",
    "xgb_reg_results = None\n",
    "if xgb_regressor is not None:\n",
    "    xgb_reg_results = evaluate_model(\n",
    "        xgb_regressor, regression_splits['X_test'], regression_splits['y_test'], \n",
    "        'regression', 'XGBoost Regression'\n",
    "    )\n",
    "\n",
    "print(\"\\n✅ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9b241",
   "metadata": {},
   "source": [
    "# 💾 Save Model Artifacts\n",
    "\n",
    "Save trained models with metadata to repository and Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b06da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_commit_hash(repo_path):\n",
    "    \"\"\"Get current git commit hash\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['git', 'rev-parse', 'HEAD'],\n",
    "            cwd=repo_path,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()[:10]  # Short hash\n",
    "        return 'unknown'\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def calculate_dataset_hash(X, y):\n",
    "    \"\"\"Calculate hash of dataset for reproducibility\"\"\"\n",
    "    try:\n",
    "        data_str = f\"{X.shape}_{X.iloc[0].values.sum():.6f}_{y.sum()}\"\n",
    "        return hashlib.md5(data_str.encode()).hexdigest()[:10]\n",
    "    except:\n",
    "        return 'unknown'\n",
    "\n",
    "def save_model_artifacts(models_info, base_path, symbol, cfg):\n",
    "    \"\"\"Save model artifacts with metadata\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Create directory structure\n",
    "    symbol_path = Path(base_path) / symbol\n",
    "    timestamp_path = symbol_path / timestamp\n",
    "    \n",
    "    artifact_paths = []\n",
    "    \n",
    "    for model_info in models_info:\n",
    "        if model_info['model'] is None:\n",
    "            continue\n",
    "            \n",
    "        model_id = f\"{model_info['name'].lower().replace(' ', '_')}\"\n",
    "        model_path = timestamp_path / model_id\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_file = model_path / 'model.pkl'\n",
    "        joblib.dump(model_info['model'], model_file, compress=3)\n",
    "        \n",
    "        # Save scaler if available\n",
    "        scaler = StandardScaler().fit(X)  # Fit on full dataset for consistency\n",
    "        scaler_file = model_path / 'scaler.pkl'\n",
    "        joblib.dump(scaler, scaler_file, compress=3)\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'model_id': model_id,\n",
    "            'model_name': model_info['name'],\n",
    "            'model_type': model_info['type'],\n",
    "            'task_type': model_info['task'],\n",
    "            'symbol': symbol,\n",
    "            'timestamp': timestamp,\n",
    "            'commit_hash': get_git_commit_hash(REPO_PATH) if REPO_CLONED else 'unknown',\n",
    "            'dataset_hash': calculate_dataset_hash(X, y_binary),\n",
    "            'config': cfg,\n",
    "            'performance': model_info.get('results', {}),\n",
    "            'hyperparameters': model_info.get('params', {}),\n",
    "            'feature_names': list(X.columns),\n",
    "            'n_features': len(X.columns),\n",
    "            'n_samples': len(X),\n",
    "            'class_distribution': y_binary.value_counts().to_dict(),\n",
    "            'artifacts': {\n",
    "                'model': 'model.pkl',\n",
    "                'scaler': 'scaler.pkl',\n",
    "                'metadata': 'meta.json'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        meta_file = model_path / 'meta.json'\n",
    "        with open(meta_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        artifact_info = {\n",
    "            'model_id': model_id,\n",
    "            'path': str(model_path),\n",
    "            'files': ['model.pkl', 'scaler.pkl', 'meta.json'],\n",
    "            'size_mb': sum(f.stat().st_size for f in model_path.glob('*')) / (1024*1024)\n",
    "        }\n",
    "        artifact_paths.append(artifact_info)\n",
    "        \n",
    "        print(f\"✅ Saved {model_info['name']} to {model_path}\")\n",
    "    \n",
    "    return artifact_paths, timestamp\n",
    "\n",
    "# Prepare models for saving\n",
    "models_to_save = [\n",
    "    {\n",
    "        'name': 'LightGBM Binary',\n",
    "        'type': 'lightgbm',\n",
    "        'task': 'classification',\n",
    "        'model': lgbm_binary,\n",
    "        'params': lgbm_binary_params,\n",
    "        'results': lgbm_binary_results\n",
    "    },\n",
    "    {\n",
    "        'name': 'LightGBM Regression',\n",
    "        'type': 'lightgbm', \n",
    "        'task': 'regression',\n",
    "        'model': lgbm_regressor,\n",
    "        'params': lgbm_reg_params,\n",
    "        'results': lgbm_reg_results\n",
    "    }\n",
    "]\n",
    "\n",
    "if xgb_binary is not None:\n",
    "    models_to_save.append({\n",
    "        'name': 'XGBoost Binary',\n",
    "        'type': 'xgboost',\n",
    "        'task': 'classification', \n",
    "        'model': xgb_binary,\n",
    "        'params': xgb_binary_params,\n",
    "        'results': xgb_binary_results\n",
    "    })\n",
    "\n",
    "if xgb_regressor is not None:\n",
    "    models_to_save.append({\n",
    "        'name': 'XGBoost Regression',\n",
    "        'type': 'xgboost',\n",
    "        'task': 'regression',\n",
    "        'model': xgb_regressor, \n",
    "        'params': xgb_reg_params,\n",
    "        'results': xgb_reg_results\n",
    "    })\n",
    "\n",
    "# Save to repository\n",
    "print(\"💾 Saving model artifacts to repository...\")\n",
    "repo_artifacts, training_timestamp = save_model_artifacts(\n",
    "    models_to_save, MODEL_SAVE_REPO_PATH, SYMBOL, CFG\n",
    ")\n",
    "\n",
    "# Save to Google Drive (if mounted)\n",
    "drive_artifacts = []\n",
    "if DRIVE_MOUNTED:\n",
    "    print(\"\\n💾 Copying artifacts to Google Drive...\")\n",
    "    try:\n",
    "        drive_artifacts, _ = save_model_artifacts(\n",
    "            models_to_save, MODEL_SAVE_DRIVE_PATH, SYMBOL, CFG\n",
    "        )\n",
    "        print(\"✅ Artifacts copied to Google Drive\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Drive copy failed: {e}\")\n",
    "\n",
    "print(f\"\\n📦 Total artifacts saved: {len(repo_artifacts)} models\")\n",
    "total_size = sum(a['size_mb'] for a in repo_artifacts)\n",
    "print(f\"📦 Total size: {total_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
